<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="author" content="Luc Anselin" />


  <title>Cluster Analysis (3)</title>

  <script src="lab7c_files/header-attrs-2.3/header-attrs.js"></script>
  <link href="lab7c_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
  <script src="lab7c_files/highlightjs-9.12.0/highlight.js"></script>
  <title>GeoDa on Github</title>

  <style>
    * {
      margin: 0;
      padding: 0;
    }

    .shadowfilter {
      -webkit-filter: drop-shadow(12px 12px 7px rgba(0, 0, 0, 0.5));
      filter: url(shadow.svg#drop-shadow);
    }

    .intro1 {
      margin-left: -45px;
    }
  </style>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css"
    media="screen">
  <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
  <style>
    ul {
      padding-left: 30px;
    }

    figcaption {
      top: .70em;
      left: .35em;
      bottom: auto !important;
      right: auto !important;
    }
  </style>

  <style>
    h1 {
      text-align: center;
    }

    h3.subtitle {
      text-align: center;
    }

    h4.author {
      text-align: center;
    }

    h4.date {
      text-align: center;
    }

    p.caption {
      font-size: 12px;
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LC0QJ53WFS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-LC0QJ53WFS');
  </script>
  <!-- End Google tag -->

  <style type="text/css">
    code {
      white-space: pre;
    }
  </style>
  <script type="text/javascript">
    if (window.hljs) {
      hljs.configure({ languages: [] });
      hljs.initHighlightingOnLoad();
      if (document.readyState && document.readyState === "complete") {
        window.setTimeout(function () { hljs.initHighlighting(); }, 0);
      }
    }
  </script>








</head>

<body>


  <section class="main-content">


    <h1 class="title toc-ignore">Cluster Analysis (3)</h1>
    <h3 class="subtitle">Advanced Clustering Methods</h3>
    <h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
    <h4 class="date">07/30/2020 (latest update)</h4>


    <div id="TOC">
      <ul>
        <li><a href="#introduction">Introduction</a>
          <ul>
            <li><a href="#objectives">Objectives</a>
              <ul>
                <li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
              </ul>
            </li>
            <li><a href="#getting-started">Getting started</a></li>
          </ul>
        </li>
        <li><a href="#k-medians">K Medians</a>
          <ul>
            <li><a href="#principle">Principle</a></li>
            <li><a href="#implementation">Implementation</a>
              <ul>
                <li><a href="#variable-settings-panel">Variable Settings Panel</a></li>
                <li><a href="#cluster-results">Cluster results</a></li>
              </ul>
            </li>
            <li><a href="#options-and-sensitivity-analysis">Options and sensitivity analysis</a>
              <ul>
                <li><a href="#mad-standardization">MAD standardization</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#k-medoids">K Medoids</a>
          <ul>
            <li><a href="#principle-1">Principle</a>
              <ul>
                <li><a href="#the-pam-algorithm-for-k-medoids">The PAM algorithm for k-medoids</a></li>
                <li><a href="#improving-on-the-pam-algorithm">Improving on the PAM algorithm</a></li>
              </ul>
            </li>
            <li><a href="#implementation-1">Implementation</a>
              <ul>
                <li><a href="#variable-settings-panel-1">Variable Settings Panel</a></li>
                <li><a href="#cluster-results-1">Cluster results</a></li>
              </ul>
            </li>
            <li><a href="#options-and-sensitivity-analysis-1">Options and sensitivity analysis</a>
              <ul>
                <li><a href="#clara">CLARA</a></li>
                <li><a href="#clarans">CLARANS</a></li>
                <li><a href="#comparison-of-methods-and-initialization-settings">Comparison of methods and
                    initialization settings</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#spectral-clustering">Spectral Clustering</a>
          <ul>
            <li><a href="#principle-2">Principle</a>
              <ul>
                <li><a href="#clustering-as-a-graph-partitioning-problem">Clustering as a graph partitioning problem</a>
                </li>
              </ul>
            </li>
            <li><a href="#the-spectral-clustering-algorithm">The spectral clustering algorithm</a>
              <ul>
                <li><a href="#creating-an-adjacency-matrix">Creating an adjacency matrix</a></li>
                <li><a href="#clustering-on-the-eigenvectors-of-the-graph-laplacian">Clustering on the eigenvectors of
                    the graph Laplacian</a></li>
                <li><a href="#spectral-clustering-parameters">Spectral clustering parameters</a></li>
              </ul>
            </li>
            <li><a href="#implementation-2">Implementation</a>
              <ul>
                <li><a href="#variable-settings-panel-2">Variable Settings Panel</a></li>
                <li><a href="#cluster-results-2">Cluster results</a></li>
              </ul>
            </li>
            <li><a href="#options-and-sensitivity-analysis-2">Options and sensitivity analysis</a>
              <ul>
                <li><a href="#k-nearest-neighbors-affinity-matrix">K-nearest neighbors affinity matrix</a></li>
                <li><a href="#gaussian-kernel-affinity-matrix">Gaussian kernel affinity matrix</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#appendix">Appendix</a>
          <ul>
            <li><a href="#worked-example-for-k-medians">Worked example for k-medians</a></li>
            <li><a href="#worked-example-for-pam">Worked example for PAM</a>
              <ul>
                <li><a href="#build">BUILD</a></li>
                <li><a href="#swap">SWAP</a></li>
              </ul>
            </li>
            <li><a href="#the-graph-laplacian">The graph Laplacian</a></li>
          </ul>
        </li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <p><br></p>
    <div id="introduction" class="section level2 unnumbered" number="">
      <h2>Introduction</h2>
      <p>In this chapter, we consider some more advanced partitioning methods. First, we cover two
        variants of K-means, i.e., <em>K-medians</em> and <em>K-medoids</em>. These operate in the same
        manner as K-means, but differ in the way the central point of each cluster is defined and the
        manner in which the <em>nearest</em> points are assigned. In addition,
        we discuss <em>spectral clustering</em>, a graph partitioning method that can be interpreted as simultaneously
        implementing
        dimension reduction with cluster identification.</p>
      <p>As implemented in <code>GeoDa</code>, these methods share almost all the same options with the partitioning and
        hierarchical
        clustering methods discussed in the previous chapters. These common aspects will not be considered again. We
        refer to the previous chapters
        for details on the common options and sensitivity analyses.</p>
      <p>We continue to use the Guerry data set to illustrate k-medians and k-medoids, but introduce a new sample data
        set,
        <strong>spirals.csv</strong>, for the spectral clustering examples.
      </p>
      <div id="objectives" class="section level3 unnumbered" number="">
        <h3>Objectives</h3>
        <ul>
          <li>
            <p>Understand the difference between k-median and k-medoid clustering</p>
          </li>
          <li>
            <p>Carry out and interpret the results of k-median clustering</p>
          </li>
          <li>
            <p>Gain insight into the logic behind the PAM, CLARA and CLARANS algorithms</p>
          </li>
          <li>
            <p>Carry out and interpret the results of k-medoid clustering</p>
          </li>
          <li>
            <p>Understand the graph-theoretic principles underlying spectral clustering</p>
          </li>
          <li>
            <p>Carry out and interpret the results of spectral clustering</p>
          </li>
        </ul>
        <div id="geoda-functions-covered" class="section level4 unnumbered" number="">
          <h4>GeoDa functions covered</h4>
          <ul>
            <li>Clusters &gt; K Medians
              <ul>
                <li>select variables</li>
                <li>MAD standardization</li>
              </ul>
            </li>
            <li>Clusters &gt; K Medoids</li>
            <li>Clusters &gt; Spectral</li>
          </ul>
          <p><br></p>
        </div>
      </div>
      <div id="getting-started" class="section level3 unnumbered" number="">
        <h3>Getting started</h3>
        <p>The Guerry data set can be loaded in the same way as before.</p>
        <p>The <strong>spirals</strong> data set is specifically designed to illustrate some of the special
          characteristics
          of spectral clustering. It is one of the GeoDaCenter sample data sets.</p>
        <p>To activate this data set, you load the file <strong>spirals.csv</strong> and select <strong>x</strong> and
          <strong>y</strong> as the coordinates (the
          data set only has two variables), as in Figure <a href="#fig:spiralscsv">1</a>. This will ensure that the
          resulting layer is
          represented as a point map.</p>
        <div class="figure" style="text-align: center"><span id="fig:spiralscsv"></span>
          <img src="pics7c/00_spirals_csv.png" alt="Spirals convert csv file to point map" width="40%" />
          <p class="caption">
            Figure 1: Spirals convert csv file to point map
          </p>
        </div>
        <p>The result shows the 300 points, consisting of two distinct but interwoven spirals, as in Figure <a
            href="#fig:spiralmap">2</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:spiralmap"></span>
          <img src="pics7c/00_spirals_map.png" alt="Spirals themeless point map" width="60%" />
          <p class="caption">
            Figure 2: Spirals themeless point map
          </p>
        </div>
        <p>We will not be needing this data set until we cover spectral clustering. For k-medians and k-medoids, we
          use the Guerry data set.</p>
      </div>
    </div>
    <div id="k-medians" class="section level2 unnumbered" number="">
      <h2>K Medians</h2>
      <div id="principle" class="section level3 unnumbered" number="">
        <h3>Principle</h3>
        <p>K-medians is a variant of k-means clustering. As a partitioning method, it starts
          by randomly picking k starting points and assigning observations to the nearest initial
          point. After the assignment,
          the center for each cluster is re-calculated and the assignment process repeats itself.
          In this way, k-medians proceeds in exactly the same manner as k-means. It is in fact also
          an EM algorithm.</p>
        <p>In contrast to k-means, the central point is not the average
          (in multiattribute space), but instead the median of the cluster observations. The median center
          is computed separately for each dimension, so it is not necessarily an actual observation
          (similar to what is the case for the cluster average in k-means).</p>
        <p>The objective function for k-medians is to find the allocation <span class="math inline">\(C(i)\)</span> of
          observations <span class="math inline">\(i\)</span> to clusters
          <span class="math inline">\(h = 1, \dots k\)</span>, such that the sum of the Manhattan distances between the
          members of
          each cluster and the cluster median is minimized:
          <span class="math display">\[\mbox{argmin}_{C(i)} \sum_{h=1}^k \sum_{i \in h} || x_i - x_{h_{med}}
            ||_{L_1},\]</span>
          where the distance metric follows the <span class="math inline">\(L_1\)</span> norm, i.e., the Manhattan block
          distance.
        </p>
        <p>K-medians is often
          confused with k-medoids. However, there is an important difference in that in
          k-medoids, the central point has to be one of the observations <span class="citation">(Kaufman and Rousseeuw
            <a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>.
          We consider k-medoids in the next section.</p>
        <p>The Manhattan distance metric is used to assign observations to the nearest center.
          From a theoretical perspective, this is superior to using Euclidean distance since it
          is consistent with the notion of a median as the center <span class="citation">(Hoon, Imoto, and Miyano <a
              href="#ref-deHoonetal:17" role="doc-biblioref">2017</a>, 16)</span>.</p>
        <p>In all other respects, the implementation and interpretation is the same as for
          k-means. To illustrate the logic, a simple worked example is provided in the <a href="#appendix">Appendix</a>.
        </p>
        <p><code>GeoDa</code> employs the k-medians implementation that is part of the C clustering library
          of <span class="citation">Hoon, Imoto, and Miyano (<a href="#ref-deHoonetal:17"
              role="doc-biblioref">2017</a>)</span>.</p>
      </div>
      <div id="implementation" class="section level3 unnumbered" number="">
        <h3>Implementation</h3>
        <p>Just as the previous clustering techniques, k-medians is invoked from the <strong>Clusters</strong> toolbar.
          From the menu, it is selected as <strong>Clusters &gt; K Medians</strong>, the second item in the classic
          clustering subset, as shown
          in Figure <a href="#fig:kmedian">3</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedian"></span>
          <img src="pics7c/1_071_kmedians.png" alt="K Medians Option" width="10%" />
          <p class="caption">
            Figure 3: K Medians Option
          </p>
        </div>
        <p>This brings up the <strong>K Medians Clustering Settings</strong> dialog, with the <strong>Input</strong>
          options
          in the left-hand side panel, shown in Figure <a href="#fig:kmedianvars">4</a>.</p>
        <div id="variable-settings-panel" class="section level4 unnumbered" number="">
          <h4>Variable Settings Panel</h4>
          <p>The user interface is identical to that for k-means, to which we refer for details. The main difference
            is that the <strong>Distance Function</strong> is Manhattan distance. In the example in Figure <a
              href="#fig:kmedianvars">4</a>,
            we again select the same six variables as before, with the <strong>Number of Clusters</strong> set to 5 and
            all other
            options left to the default settings.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedianvars"></span>
            <img src="pics7c/1_072_kmedian_vars.png" alt="K Medians variable selection" width="35%" />
            <p class="caption">
              Figure 4: K Medians variable selection
            </p>
          </div>
          <p>Selecting <strong>Run</strong> brings up the cluster map and fills out the right-hand panel with some
            cluster characteristics, listed under <strong>Summary</strong>. The cluster categories are added to the
            Table
            using the variable name specified in the dialog (default is <strong>CL</strong>, in our example we use
            <strong>CLme1</strong>).</p>
        </div>
        <div id="cluster-results" class="section level4 unnumbered" number="">
          <h4>Cluster results</h4>
          <p>The cluster map is shown in Figure <a href="#fig:kmedianmap">5</a>. The three largest clusters
            (they are labeled in sequence of their size) are well-balanced, with 24, 20 and 20 observations.
            The two others are much smaller, at 12 and 9. Interesting is that the clusters are
            also geographically quite compact, except for cluster 4, which consists of four different spatial
            subgroups. Cluster 2, in the south of the country, is actually fully contiguous (without imposing
            any spatial constraints). This is not the case for k-means.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedianmap"></span>
            <img src="pics7c/1_073_kmedian_map.png" alt="K Medians cluster map (k=5)" width="60%" />
            <p class="caption">
              Figure 5: K Medians cluster map (k=5)
            </p>
          </div>
          <p>While the grouping may seem similar to what we obtained with other methods, this is in fact not the
            case. In Figure <a href="#fig:kmeanmed">6</a>, the cluster map for k-means and k-medians are shown next to
            each other,
            with the labels for k-medians adjusted so as the get similar colors for each category. This highlights
            some of the important differences between the two methods. First of all, the size of the different
            “matching”
            clusters is not the same, nor is their geographic configuration. Considering the clusters for k-medians
            (with
            their new labels), we see that the largest cluster, with 24 observations, corresponds most closely with
            cluster
            3 for k-means, which had 18 observations.</p>
          <p>The closest match between the two results is for cluster 2, with only one mismatch out of 9 observations,
            although that cluster is much larger for k-means, with 19 observations.
            The worst match is for cluster 5, where only three observations are shared by the two methods for that
            cluster (out of 12). For the others, there is about a 3/4 match. In other words, the two methods pick out
            different patterns of similarity in the data. There is no “best” method, since each uses a different
            objective function. It is up to the analyst to decide which of the objectives
            makes most sense, in light of the goals of a particular study.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanmed"></span>
            <img src="pics7c/4_kmeankmed.png" alt="K Means and K Medians compared (k=5)" width="90%" />
            <p class="caption">
              Figure 6: K Means and K Medians compared (k=5)
            </p>
          </div>
          <p>Further insight into the characteristics of the clusters obtained by the k-medians algorithm are found
            in the <strong>Summary</strong> panel on the right side of the settings dialog, shown in Figure <a
              href="#fig:kmediansummary">7</a>.</p>
          <p>The first set of items summarizes the settings for the analysis, such as the method
            used, the number of clusters and the various options for initialization, standardization, etc.
            Next follow the values for each of the variables associated with the <em>median</em> center of each cluster.
            These
            results are given in the <em>original scale</em> for the variables, whereas the other summary measures
            depend
            on the standardization used. Typically,
            the median center values are used to interpret the type of grouping that is obtained. This is not always
            easy, since one
            has to look for systematic combinations of variables with high or low values for the median so as to
            characterize the cluster.</p>
          <p>The third set of items contains the summary statistics, using the squared difference and mean as the
            criterion,
            similar to what is used for k-means. Note that this is only for a general comparison, since this is
            <em>not</em>
            the criterion used in the objective function. So, in a sense, it gives a general impression of how the
            k-medians
            results compare using the standard used for k-means. In our example, we obtain a ratio of between to total
            sum of squares of 0.447, compared to 0.497 for k-means (with the default settings). This does not mean
            that the k-medians result is <em>worse</em> than that for k-means, but it gives a sense of how it performs
            under a different criterion that what it is optimized for.</p>
          <p>The final set of summary characteristics are the proper ones for the objective of minimizing the
            within-cluster Manhattan
            distance relative to the cluster median. The total sum of the distances is 372.318. This is the sum of
            the distances between all observations and the overall median (using the z-standardized values for
            the variables). For k-medians, the objective is to decrease
            this value by grouping the observations into clusters with their own medians. The within-cluster total
            distance
            is listed for each cluster. In our results, there is quite a range in these values, going from 15.97 in the
            smallest cluster (with only 9 observations) to 70.49 in cluster 3 (with 20 observations). Clusters 1 and 2,
            that
            are larger or equal to the size of cluster 3, have a much better fit. This is also reflected in the average
            within-cluster
            distance results, with the smallest value of 1.77 for C5, followed by 2.61 for C1. Interestingly, the latter
            has about
            double the total distance compared to C4, but its average is better (2.61 compared to 2.84). The averages
            correct for
            the size of the cluster and are thus a good comparative measure of <em>fit</em>.</p>
          <p>The total of the within-cluster distances is 250.399, a decrease of 121.9 from the original total. As a
            fraction
            of the original total, the final result is 0.673. When comparing results for different values of k, we would
            look for
            a bend in the elbow plot as this ratio decreases with increasing values of k.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmediansummary"></span>
            <img src="pics7c/1_074_kmedians_sum.png" alt="K Medians cluster characteristics (k=5)" width="50%" />
            <p class="caption">
              Figure 7: K Medians cluster characteristics (k=5)
            </p>
          </div>
        </div>
      </div>
      <div id="options-and-sensitivity-analysis" class="section level3 unnumbered" number="">
        <h3>Options and sensitivity analysis</h3>
        <p>The variables settings panel contains all the same options as for k-means, except that
          initialization is always by randomization, since there is no k-means++ method for k-medians.
          One option that is particularly useful in the context of k-medians (and k-medoids) is the
          use of a different standardization.</p>
        <div id="mad-standardization" class="section level4 unnumbered" number="">
          <h4>MAD standardization</h4>
          <p>The default z-standardization uses the mean and the variance of the original variables. Both of these are
            sensitive to the influence of outliers. Since the use of Manhattan distance and the median center for
            clusters in k-medians already reduces the effect of such outliers, it makes sense to also use a
            standardization that is less sensitive to those. We considered range standardization in the discussion of
            k-means. Here, we look at
            the mean absolute deviation, or MAD. As usual, this is selected as one of the
            <strong>Transformation</strong> options, as shown
            in Figure <a href="#fig:kmedianmad">8</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedianmad"></span>
            <img src="pics7c/1_kmedians_mad.png" alt="MAD variable standardization" width="40%" />
            <p class="caption">
              Figure 8: MAD variable standardization
            </p>
          </div>
          <p>The resulting cluster map and summary characteristics are shown in
            Figures <a href="#fig:kmedianmadmap">9</a> and <a href="#fig:kmedianmadsummary">10</a>.</p>
          <p>The main effect seems to be on the largest cluster, which grows from 24 to 27 observations, mostly
            at the expense of what was the second largest cluster (which goes from 20 to 18 observations).
            As a result, none of the clusters are fully contiguous any more.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedianmadmap"></span>
            <img src="pics7c/1_kmedian_mad_map.png" alt="K Medians cluster map - MAD standardization (k=5)"
              width="60%" />
            <p class="caption">
              Figure 9: K Medians cluster map - MAD standardization (k=5)
            </p>
          </div>
          <p>The distance measures listed in the summary show a different starting point, with a total distance
            sum of 490.478, compared to 372.318 for z-standardization (recall that these measures are expressed
            in whatever units were used for the standardization). Therefore, the values for the within-cluster
            distance and their averages are not directly comparable to those using z-standardization. Only
            relative comparisons are warranted.</p>
          <p>In the end, the total within-clusters are reduced
            to 0.677 of the original total, a slightly worse result than for z-standardization. However, this does
            not necessarily mean that z-standardization is superior. The choice of a particular transformation should
            be made within the context of the substantive research question. When no strong guidelines exist, a
            sensitivity analysis comparing, for example, z-standardization, range standardization and MAD may be
            the best strategy.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedianmadsummary"></span>
            <img src="pics7c/1_kmedian_mad_summary.png"
              alt="K Medians cluster characteristics - MAD standardization (k=5)" width="50%" />
            <p class="caption">
              Figure 10: K Medians cluster characteristics - MAD standardization (k=5)
            </p>
          </div>
        </div>
      </div>
    </div>
    <div id="k-medoids" class="section level2 unnumbered" number="">
      <h2>K Medoids</h2>
      <div id="principle-1" class="section level3 unnumbered" number="">
        <h3>Principle</h3>
        <p>The objective of the k-medoids algorithm is to minimize the sum of the distances
          from the observations in each cluster to a <em>representative center</em> for that cluster.
          In contrast to k-means and k-medians, those centers do not need to be computed,
          since they are actual observations. As a consequence, k-medoids works with any
          dissimilarity matrix. If actual observations are available (as in the implementation
          in <code>GeoDa</code>), the Manhattan distance is the preferred metric, since
          it is less affected by outliers. In addition, since the objective function is
          based on the sum of distances instead of their squares, the influence of
          outliers is even smaller.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
        <p>The objective function can thus be expressed as finding the cluster assignments <span
            class="math inline">\(C(i)\)</span> such that:
          <span class="math display">\[\mbox{argmin}_{C(i)} \sum_{h=1}^k \sum_{i \in h} d_{i,h_c},\]</span>
          where <span class="math inline">\(h_c\)</span> is a representative center for cluster <span
            class="math inline">\(h\)</span> and <span class="math inline">\(d\)</span> is the distance metric used
          (from a dissimilarity matrix). As was the case for k-means (and k-medians), the problem is NP hard and
          an exact solution does not exist.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
        </p>
        <p>The main approach to the k-medoids problem is the so-called <em>partitioning around medoids</em> (PAM)
          algorithm
          of <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
              role="doc-biblioref">2005</a>)</span>.
          The logic underlying the PAM algorithm consists of two stages, <em>BUILD</em> and <em>SWAP</em>. In the first,
          a set of <span class="math inline">\(k\)</span> starting centers are selected
          from the <span class="math inline">\(n\)</span> observations. In some implementations, this is a random
          selection, but <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
              role="doc-biblioref">2005</a>)</span>, and,
          more recently <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19"
              role="doc-biblioref">2019</a>)</span> prefer
          a step-wise procedure that optimizes the initial set. The main part of the algorithm
          proceeds in a greedy iterative manner by swapping a current center with a candidate from the
          remaining non-centers, as long as the objective function can be improved. Detailed descriptions are given
          in <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
              role="doc-biblioref">2005</a>)</span>, Chapters 2 and 3, as well as in <span class="citation">Hastie,
            Tibshirani, and Friedman (<a href="#ref-Hastieetal:09" role="doc-biblioref">2009</a>)</span>, pp. 515-520,
          and <span class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12"
              role="doc-biblioref">2012</a>)</span>, pp. 454-457. A brief
          outline is presented next.</p>
        <div id="the-pam-algorithm-for-k-medoids" class="section level4 unnumbered" number="">
          <h4>The PAM algorithm for k-medoids</h4>
          <p>The <em>BUILD</em> phase of the algorithm consists of identifying <span class="math inline">\(k\)</span>
            observations
            out of the <span class="math inline">\(n\)</span> and assigning them to be cluster centers <span
              class="math inline">\(h\)</span>, with <span class="math inline">\(h = 1, \dots, k\)</span>.
            This can be accomplished by randomly selecting the starting points a number of times
            and picking the one with the best (lowest) value for the objective function, i.e., the
            lowest sum of distances from observations to their cluster centers.</p>
          <p>As a preferred option, <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
                role="doc-biblioref">2005</a>)</span> outline a step-wise approach that starts by picking the center,
            say <span class="math inline">\(h_1\)</span>,
            that minimizes the overall sum. This is readily accomplished by taking the observation that
            corresponds with the smallest row or column sum of the dissimilarity matrix. Next, each additional center
            (for <span class="math inline">\(h = 2, \dots, k\)</span>) is selected that maximizes the difference between
            the closest distance to existing
            centers and the new potential center for all points (in practice, for the points that are closer to the
            new center than to existing centers).</p>
          <p>For example, given the distance to <span class="math inline">\(h_1\)</span>, we compute for each of the
            remaining <span class="math inline">\(n - 1\)</span> points <span class="math inline">\(j\)</span>, its
            distance to all candidate centers <span class="math inline">\(i\)</span> (i.e., the same <span
              class="math inline">\(n-1\)</span> points). Consider
            a <span class="math inline">\((n-1) \times (n-1)\)</span> matrix where each observation is both row (<span
              class="math inline">\(i\)</span>) and column (<span class="math inline">\(j\)</span>). For each column
            <span class="math inline">\(j\)</span>,
            we compare the distance to the row-element <span class="math inline">\(i\)</span> with the distance to the
            nearest current center for <span class="math inline">\(j\)</span>. In the
            second iteration, this is simply the distance to <span class="math inline">\(h_1\)</span>, but at later
            iterations different <span class="math inline">\(j\)</span> will have different
            centers closest to them. If <span class="math inline">\(i\)</span> is closer to <span
              class="math inline">\(j\)</span> than its current closest center, then <span
              class="math inline">\(d_{j,h_1} - d_{ji} &gt; 0\)</span>. The
            maximum of this difference and 0 is entered in position <span class="math inline">\(i,j\)</span> of the
            matrix (in other words, negatives are not counted). The row <span class="math inline">\(i\)</span> with the
            largest row sum (i.e., the largest improvement in the objective function) is selected as the next center.
          </p>
          <p>At this point, the distance for each <span class="math inline">\(j\)</span> to its nearest center is
            updated and the process starts anew for <span class="math inline">\(n-2\)</span> observations.
            This continues until <span class="math inline">\(k\)</span> centers have been picked.</p>
          <p>In the <em>SWAP</em> phase,
            we consider all possible pairs that consist of a
            cluster center <span class="math inline">\(i\)</span> and one of the <span class="math inline">\(n -
              k\)</span> non-centers, <span class="math inline">\(r\)</span>, for a possible swap, for a total
            of <span class="math inline">\(k \times (n-k)\)</span> pairs <span class="math inline">\((i, r)\)</span>.
          </p>
          <p>We proceed by evaluating the change to the objective function that would follow from removing center <span
              class="math inline">\(i\)</span> and
            replacing it with <span class="math inline">\(r\)</span>, as it affects the allocation of each other point
            (non-center and non-candidate) to either
            the new center <span class="math inline">\(r\)</span> or one of the current centers <span
              class="math inline">\(g\)</span> (but not <span class="math inline">\(i\)</span>, since that is no longer
            a center). This
            contribution follows from the change in distance that may occur between <span
              class="math inline">\(j\)</span> and its new center. Those values
            are summed over all <span class="math inline">\(n - k - 1\)</span> points <span
              class="math inline">\(j\)</span>.
            We compute this sum for each pair <span class="math inline">\(i, r\)</span> and find the minimum over all
            pairs. If this minimum
            is negative (i.e., it decreases the total sum of distances), then <span class="math inline">\(i\)</span> and
            <span class="math inline">\(r\)</span> are swapped. This continues until
            there are no more improvements, i.e., the minimum is positive.</p>
          <p>We label the change in the objective from <span class="math inline">\(j\)</span> from a swap between <span
              class="math inline">\(i\)</span> and <span class="math inline">\(r\)</span> as <span
              class="math inline">\(C_{jir}\)</span>. The
            total improvement for a given pair <span class="math inline">\(i, r\)</span> is the sum over all <span
              class="math inline">\(j\)</span>, <span class="math inline">\(T_{ir} = \sum_j C_{jir}\)</span>. The pair
            <span class="math inline">\(i, r\)</span> is selected for
            a swap for which the minimum over all pairs of <span class="math inline">\(T_{ir}\)</span> is negative,
            i.e., <span class="math inline">\(\mbox{argmin}_{i,r} T_{ir} &lt; 0\)</span>.</p>
          <p>The computational burden associated with this algorithm is quite high, since at each iteration <span
              class="math inline">\(k \times (n - k)\)</span> pairs need to be evaluated. On the other hand, no
            calculations other than comparison and addition/subtraction
            are involved, and all the information is in the (constant) dissimilarity matrix.</p>
          <p>To compute the net change in the objective function due to <span class="math inline">\(j\)</span> that
            follows from a swap between <span class="math inline">\(i\)</span> and <span
              class="math inline">\(r\)</span>, we distinguish
            between two cases. In one, <span class="math inline">\(j\)</span> belongs to the cluster <span
              class="math inline">\(i\)</span>, such that <span class="math inline">\(d_{ji} &lt; d_{jg}\)</span> for
            all other
            centers <span class="math inline">\(g\)</span>. In the other case, <span class="math inline">\(j\)</span>
            belongs to a different cluster, say <span class="math inline">\(g\)</span>, and <span
              class="math inline">\(d_{jg} &lt; d_{ji}\)</span>. In both instances,
            we have to compare the distances from the nearest current center (<span class="math inline">\(i\)</span> or
            <span class="math inline">\(g\)</span>) to the distance to the candidate point, <span
              class="math inline">\(r\)</span>.
            Note that we don’t actually have to carry out the cluster assignments, since we compare the distance for all
            <span class="math inline">\(n - k - 1\)</span>
            points <span class="math inline">\(j\)</span> to the closest center (<span class="math inline">\(i\)</span>
            or <span class="math inline">\(g\)</span>) and the candidate center <span class="math inline">\(r\)</span>.
            All this information is contained
            in the elements of the dissimilarity matrix.</p>
          <p>Consider the first case, where <span class="math inline">\(j\)</span> is not part of the cluster <span
              class="math inline">\(i\)</span>, as in Figure <a href="#fig:pam2">11</a>. We see two scenarios for the
            configuration of the point <span class="math inline">\(j\)</span>, labeled <span
              class="math inline">\(j1\)</span> and <span class="math inline">\(j2\)</span>. These points are closer to
            <span class="math inline">\(g\)</span> than to <span class="math inline">\(i\)</span>, since they are
            <em>not</em> part
            of the cluster around <span class="math inline">\(i\)</span>. We now need to check whether <span
              class="math inline">\(j\)</span> is closer
            to <span class="math inline">\(r\)</span> than to its current cluster center <span
              class="math inline">\(g\)</span>. If <span class="math inline">\(d_{jg} \leq d_{jr}\)</span>, then nothing
            changes and <span class="math inline">\(C_{jir} = 0\)</span>. This is the case for point
            <span class="math inline">\(j1\)</span>. The dashed red line gives the distance to the current center <span
              class="math inline">\(g\)</span> and the dashed green line gives the distance to <span
              class="math inline">\(r\)</span>. Otherwise, if <span class="math inline">\(d_{jr} &lt; d_{jg}\)</span>,
            as is the case for point <span class="math inline">\(j2\)</span>,
            then <span class="math inline">\(j\)</span> is assigned to <span class="math inline">\(r\)</span> and <span
              class="math inline">\(C_{jir} = d_{jr} - d_{jg}\)</span>, a negative value, which decreases the overall
            cost. In the figure, we can compare the length of the dashed red line to the length of the solid green line,
            which designates a re-assignment to the candidate center <span class="math inline">\(r\)</span>.
          </p>
          <div class="figure" style="text-align: center"><span id="fig:pam2"></span>
            <img src="pics7c/7_pam2.png" alt="PAM SWAP - case 1" width="40%" />
            <p class="caption">
              Figure 11: PAM SWAP - case 1
            </p>
          </div>
          <p>When <span class="math inline">\(j\)</span> is part of cluster <span class="math inline">\(i\)</span>, then
            we need to assess whether <span class="math inline">\(j\)</span> would be assigned to <span
              class="math inline">\(r\)</span> or to the next closest center,
            say <span class="math inline">\(g\)</span>, since <span class="math inline">\(i\)</span> would no longer be
            part of the cluster centers. This is illustrated in Figure <a href="#fig:pam1">12</a>, which has three
            options for the location of <span class="math inline">\(j\)</span> relative to <span
              class="math inline">\(g\)</span> and <span class="math inline">\(r\)</span>. In the first case,
            illustrated by point <span class="math inline">\(j1\)</span>, <span class="math inline">\(j\)</span> is
            closer to <span class="math inline">\(g\)</span> than to <span class="math inline">\(r\)</span>. This is
            illustrated by the difference in length between the dashed green line (<span
              class="math inline">\(d_{j1r}\)</span>) and the solid green line (<span
              class="math inline">\(d_{j1g}\)</span>). More
            precisely, <span class="math inline">\(d_{jr} \geq d_{jg}\)</span> so that <span
              class="math inline">\(j\)</span> is now
            assigned to <span class="math inline">\(g\)</span>. The change in the objective is
            <span class="math inline">\(C_{jir} = d_{jg} - d_{ji}\)</span>. This value is positive, since <span
              class="math inline">\(j\)</span> was part of cluster <span class="math inline">\(i\)</span> and thus was
            closer to <span class="math inline">\(i\)</span> than to <span class="math inline">\(g\)</span>
            (compare the length of the red dashed line between <span class="math inline">\(j1\)</span> and <span
              class="math inline">\(i\)</span> and the length of the line connecting <span
              class="math inline">\(j1\)</span> to <span class="math inline">\(g\)</span>).
          </p>
          <p>If <span class="math inline">\(j\)</span> is closer to <span class="math inline">\(r\)</span>, i.e., <span
              class="math inline">\(d_{jr} &lt; d_{jg}\)</span>, then we can distinguish between two cases, one depicted
            by <span class="math inline">\(j2\)</span>, the other
            by <span class="math inline">\(j3\)</span>. In both instances, the result is that
            <span class="math inline">\(j\)</span> is
            assigned to <span class="math inline">\(r\)</span>, but the effect on the objective differs. In the Figure,
            for both <span class="math inline">\(j2\)</span> and <span class="math inline">\(j3\)</span> the dashed
            green line to <span class="math inline">\(g\)</span> is
            longer than the solid green line to <span class="math inline">\(r\)</span>. The change in the objective
            is the difference between the new distance and the old one (<span class="math inline">\(d_{ji}\)</span>), or
            <span class="math inline">\(C_{jir} = d_{jr} - d_{ji}\)</span>. This value could be either positive or
            negative, since what matters is that <span class="math inline">\(j\)</span> is closer to
            <span class="math inline">\(r\)</span> than to <span class="math inline">\(g\)</span>, irrespective of how
            close <span class="math inline">\(j\)</span> might have been to <span class="math inline">\(i\)</span>. For
            point <span class="math inline">\(j2\)</span>, the distance to <span class="math inline">\(i\)</span>
            (dashed red line)
            was smaller than the new distance to <span class="math inline">\(r\)</span> (solid green line), so <span
              class="math inline">\(d_{jr} - d_{ji} &gt; 0\)</span>. In the case of <span
              class="math inline">\(j3\)</span>, the opposite
            holds, and the length to <span class="math inline">\(i\)</span> (dashed red line) is larger than the
            distance to the new center (solid green line). In this
            case, the change to the objective is <span class="math inline">\(d_{jr} - d_{ji} &lt; 0\)</span>.
          </p>
          <div class="figure" style="text-align: center"><span id="fig:pam1"></span>
            <img src="pics7c/7_pam1.png" alt="PAM SWAP - case 2" width="40%" />
            <p class="caption">
              Figure 12: PAM SWAP - case 2
            </p>
          </div>
          <p>After the value for <span class="math inline">\(C_{jir}\)</span> is computed for all <span
              class="math inline">\(j\)</span>, the sum <span class="math inline">\(T_{ir}\)</span> is evaluated. This
            is repeated for every
            possible pair <span class="math inline">\(i,r\)</span> (i.e., <span class="math inline">\(k\)</span> centers
            to be replaced by <span class="math inline">\(n-k\)</span> candidate centers). If the minimum over all pairs
            is negative, then <span class="math inline">\(i\)</span> and <span class="math inline">\(r\)</span> for the
            selected pair are
            exchanged, and the process is repeated. If the minimum is positive, the iterations end.</p>
          <p>An illustrative worked example is given in the <a href="#appendix">Appendix</a>.</p>
        </div>
        <div id="improving-on-the-pam-algorithm" class="section level4 unnumbered" number="">
          <h4>Improving on the PAM algorithm</h4>
          <p>The complexity of each iteration in the original PAM algorithm is of the order <span
              class="math inline">\(k \times (n - k)^2\)</span>, which means
            it will not scale well to large data sets with potentially large values of <span
              class="math inline">\(k\)</span>. To address this issue,
            <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
                role="doc-biblioref">2005</a>)</span> proposed the algorithm CLARA, based on a sampling strategy.
          </p>
          <p>Instead of considering the
            full data set, a subsample is drawn. Then PAM is applied to find the
            best <span class="math inline">\(k\)</span> medoids in the sample. Next, the distance from all observations
            (not just those in the sample) to their closest medoid is computed
            to assess the overall quality of the clustering.</p>
          <p>The sampling process can be repeated for several more samples (keeping the best solution
            from the previous iteration as part of the sampled observations), and at the end the best solution is
            selected. While
            easy to implement, this approach does not guarantee that the best local optimum solution is found. In fact,
            if one of the
            best medoids is never sampled, it is impossible for it to become part of the final solution. Note that as
            the
            sample size is increased, the results will tend to be closer to those given by PAM.<a href="#fn4"
              class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
          <p>In practical applications, <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
                role="doc-biblioref">2005</a>)</span> suggest to use
            a sample size of 40 + 2k and to repeat the process 5 times.<a href="#fn5" class="footnote-ref"
              id="fnref5"><sup>5</sup></a></p>
          <p>In <span class="citation">Ng and Han (<a href="#ref-NgHan:02" role="doc-biblioref">2002</a>)</span>, a
            different sampling strategy is outlined that keeps the full set of observations
            under consideration. The problem is formulated as finding the best node in a graph that consists
            of all possible combinations of <span class="math inline">\(k\)</span> observations that could serve as the
            <span class="math inline">\(k\)</span> medoids. The nodes
            are connected by edges to the <span class="math inline">\(k \times (n - k)\)</span> nodes that differ in one
            medoid (i.e., for each
            edge, one of the <span class="math inline">\(k\)</span> medoid nodes is swapped with one of the <span
              class="math inline">\(n - k\)</span> candidates).</p>
          <p>The algorithm CLARANS starts an iteration by randomly picking a node (i.e., a set of <span
              class="math inline">\(k\)</span> candidate medoids). Then, it
            randomly picks a neighbor of this node in the graph. This is a set of <span class="math inline">\(k\)</span>
            medoids where one is swapped
            with the current set. If this leads to an improvement in the cost, then the new node becomes the new
            start of the next set of searches (still part of the same iteration). If not, another neighbor is picked and
            evaluated, up to <em>maxneighbor</em>
            times. This ends an iteration.</p>
          <p>At the end of the iteration the cost of the last solution is compared to the stored current <em>best</em>.
            If the
            new solution constitutes an improvement, it becomes the new <em>best</em>. This search process
            is carried out a total of <em>numlocal</em> iterations and at the end the best overall solution is kept.
            Because of the special
            nature of the graph, not that many steps are required to achieve a local minimum (technically, there are
            many
            paths that lead to the local minimum, even when starting at a random node).</p>
          <p>To make this concept more concrete, consider the toy example used in the Appendix, which has 7
            observations.
            To construct k=2 clusters, any pair of 2 observations from the 7 could be considered a potential medoid. All
            those pairs constitute the nodes in the graph. The total number of nodes is given by the binomial
            coefficient
            <span class="math inline">\({n}\choose{k}\)</span>. In our example, <span
              class="math inline">\({{7}\choose{2}} = 21\)</span>.
          </p>
          <p>Each of the 21 nodes in the graph has <span class="math inline">\(k \times (n - k) = 2 \times 5 =
              10\)</span> <em>neighbors</em> that differ only
            in one medoid connected with an edge.
            In our example, let’s say we pick (4,7) as a starting
            node, as we did in the worked example. It will be connected to all the nodes that differ by one medoid,
            i.e., either 4 or 7 is replaced (<em>swapped</em> in
            PAM terminology) by one of the <span class="math inline">\(n - k = 5\)</span> remaining nodes. Specifically,
            this includes the following
            10 neighbors: 1-7, 2-7, 3-7, 5-7, 6-7, and 4-1, 4-2, 4-3, 4-5 and 4-6. Rather than evaluating all 10
            potential swaps,
            as we did for PAM, only a maximum number (<em>maxneighbor</em>) are evaluated. At the end of those
            evaluations, the best
            solution is kept. Then the process is repeated, up to the specified total number of iterations, which <span
              class="citation">Ng and Han (<a href="#ref-NgHan:02" role="doc-biblioref">2002</a>)</span> call
            <em>numlocal</em>.
          </p>
          <p>Let’s say we set <em>maxneighbors</em> to 2. Consider the first step of the random evaluation in which we
            <em>randomly</em> pick the pair 4-5
            from the neighboring nodes. In other words, we replace 7 in the original set by 5.
            Using the values from the worked example in the Appendix, we have <span class="math inline">\(T_{45} =
              3\)</span>, a positive value, so this does not improve the
            objective function. We increase the iteration count (for <em>maxneighbors</em>) and pick a second random
            node, say 4-2 (i.e., now
            replacing 7 by 2).
            Now the value <span class="math inline">\(T_{42} = -3\)</span> so the objective is improved to 20-3 = 17.
            Since we have reached the end of <em>maxneighbors</em>,
            we store this value as <em>best</em> and now repeat the process, picking a different random starting point.
            We continue this
            until we have obtained <em>numlocal</em> local optima and keep the best overall solution.</p>
          <p>Based on their numerical experiments, <span class="citation">Ng and Han (<a href="#ref-NgHan:02"
                role="doc-biblioref">2002</a>)</span> suggest that no more than 2 iterations need to be pursued (i.e.,
            <em>numlocal</em> = 2),
            with some evidence that more operations are not cost-effective. They also suggest a sample size of 1.25% of
            <span class="math inline">\(k \times (n-k)\)</span>.<a href="#fn6" class="footnote-ref"
              id="fnref6"><sup>6</sup></a></p>
          <p>Both CLARA and CLARANS are large data methods, since for smaller data sizes (say &lt; 100), PAM will be
            feasible and obtain
            better solutions (since it implements an exhaustive evaluation).</p>
          <p>Further speedup of PAM, CLARA and CLARANS is outlined in <span class="citation">Schubert and Rousseeuw (<a
                href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span>, where some redundancies in the
            comparison
            of distances in the SWAP phase are removed. In essence, this exploits the fact that observations allocated
            to a medoid that will be swapped out, will move
            to either the second closest medoid or to the swap point. Observations that are not currently allocated to
            the medoid under
            consideration will either stay in their current cluster, or move to the swap point, depending on how the
            distance to
            their cluster center compares to the distance to the swap point. These ideas shorten the number of loops
            that need to be
            evaluated and allow the algorithms to scale to much larger problems <span class="citation">(details are in
              Schubert and Rousseeuw <a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>, 175)</span>. In
            addition, they provide
            and option to
            carry out the swaps for all current k medoids simultaneously, similar to the logic in k-means <span
              class="citation">(this is implemented
              in the FASTPAM2 algorithm, see Schubert and Rousseeuw <a href="#ref-SchubertRousseeuw:19"
                role="doc-biblioref">2019</a>, 178)</span>.</p>
          <p>A second improvement in the algorithm pertains to the BUILD phase. The original approach is replaced by a
            so-called
            <em>Linear Approximative BUILD</em> (LAB), which achieves linear runtime in <span
              class="math inline">\(n\)</span>. Instead of considering all candidate points, only
            a subsample from the data is used, repeated <span class="math inline">\(k\)</span> times (once for each
            medoid).
          </p>
          <p>The FastPAM2 algorithm tends to yield the best cluster results relative to the other methods, in terms of
            the smallest sum of distances to the respective
            medoids. However, especially for large n and large k, FastCLARANS yields much smaller
            compute times, although the quality of the clusters is not as good as for FastPAM2. FastCLARA is always much
            slower than the other two.
            In terms of the initialization methods, LAB tends to be much faster than BUILD, especially for larger n and
            k.</p>
          <p>The FastPAM2, FastCLARA and FastCLARANS algorithms
            from <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19"
                role="doc-biblioref">2019</a>)</span> were ported to C++ in <code>GeoDa</code> from the original Java
            code by the authors.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
        </div>
      </div>
      <div id="implementation-1" class="section level3 unnumbered" number="">
        <h3>Implementation</h3>
        <p>K-medoids is invoked from the <strong>Clusters</strong> toolbar, as the third item in the classic clustering
          subset, as shown
          in Figure <a href="#fig:kmedoid">13</a>.
          Alternatively, from the menu, it is selected as <strong>Clusters &gt; K Medoids</strong>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedoid"></span>
          <img src="pics7c/4_kmedoids.png" alt="K Medoids Option" width="10%" />
          <p class="caption">
            Figure 13: K Medoids Option
          </p>
        </div>
        <p>This brings up the usual variable settings panel.</p>
        <div id="variable-settings-panel-1" class="section level4 unnumbered" number="">
          <h4>Variable Settings Panel</h4>
          <p>The variable settings panel in Figure <a href="#fig:kmedoidvars">14</a> has the by now familiar layout for
            the
            input section. It is
            identical to that for k-medians, except for the <strong>Method</strong> selection and the associated
            options.
            In our example with the same six variables as before (z-standardized), we use the default method
            of <strong>FastPAM</strong>. The default <strong>Initialization Method</strong> is <strong>LAB</strong>,
            with <strong>BUILD</strong> available as an
            alternative. In practice, LAB is much faster than BUILD.</p>
          <p>There are no other options to be set, since the PAM algorithm proceeds with an exhaustive
            search in the SWAP phase, after an initial set of medoids is selected.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidvars"></span>
            <img src="pics7c/5_medoid_vars.png" alt="K Medoids variable selection" width="35%" />
            <p class="caption">
              Figure 14: K Medoids variable selection
            </p>
          </div>
          <p>Clicking on <strong>Run</strong> brings up the cluster map, as well as the usual cluster characteristics in
            the right-hand panel.
            In addition, the cluster classifications are saved to the table using the variable name specified in
            the dialog (here <strong>CLmd1</strong>).</p>
        </div>
        <div id="cluster-results-1" class="section level4 unnumbered" number="">
          <h4>Cluster results</h4>
          <p>The cluster map in Figure <a href="#fig:kmedoidmap">15</a> show results that are fairly similar to
            k-medians, although different
            in some important respects. In essence, the first cluster changes slightly, now with 26 members, while CL2
            and CL3 swap places.
            The new CL2 has 21 members (compared to 20 for CL2 in k-median) and CL3 has 18 (compared to 20 for CL2 in
            k-median).</p>
          <p>The two smallest clusters have basically the same size in both applications, but their match differs
            considerably. CL4 has
            no overlap between the two methods. On the other hand, CL5 is mostly the same between
            the two (7 out of 9 members in common). As we have pointed out before, this highlights how the various
            methods pick up
            different aspects of multi-attribute similarity. In many applications, k-medoids is preferred over k-means,
            since it tends to
            be less sensitive to outliers.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidmap"></span>
            <img src="pics7c/5_kmedoid_map.png" alt="K Medoids cluster map (k=5)" width="60%" />
            <p class="caption">
              Figure 15: K Medoids cluster map (k=5)
            </p>
          </div>
          <p>One additional useful characteristic of the k-medoids approach is that the cluster centers are actual
            observations.
            In Figure <a href="#fig:kmedoidcenters">16</a>, these are highlighted for our example. Note that these
            observations are <em>centers</em>
            in multi-attribute space, and clearly not in geographical space (we address this in the next chapter).</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidcenters"></span>
            <img src="pics7c/5_kmedoid_centers.png" alt="K Medoids cluster centers (k=5)" width="60%" />
            <p class="caption">
              Figure 16: K Medoids cluster centers (k=5)
            </p>
          </div>
          <p>A more meaningful substantive interpretation of the cluster centers can be obtained from the cluster
            characteristics in
            Figure <a href="#fig:kmedoidsummary">17</a>. Below the usual listing of methods and options, the
            <em>observation numbers</em> of the
            cluster medoids are given. These can be used to select the corresponding observations in the table.<a
              href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> As before, the values for the different
            variables in
            each cluster center are listed, but now these correspond to actual observations, so we can also look these
            up in the
            data table.</p>
          <p>While these values are expressed in the original units, the remainder of the summary characteristics are in
            the units
            used for the specified standardization, z-standardized in our example. First are the results for the within
            and between
            sum of squares. The summary ratio is 0.414, slightly worse than for k-median (but recall that this is not
            the proper
            objective function).</p>
          <p>The point of departure for the within-cluster distances is the total distance to the overall medoid for the
            data. This is the observation for which the sum of distances from all other observations is the smallest
            (i.e., the observation with the smallest row or column sum of the elements in the distance matrix).</p>
          <p>In our example, the within-cluster distances to each medoid and their averages show fairly tight clusters
            for
            CL1 and CL5, and less so for the other three. The original total distance decreases from 398.5 to 265.1, a
            ratio of 0.665, compared to 0.673 for k-median. This would suggest a slightly greater success at reducing
            the overall sum
            of distances (smaller values are better).</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidsummary"></span>
            <img src="pics7c/5_kmedoid_summary.png" alt="K Medoids cluster characteristics (k=5)" width="50%" />
            <p class="caption">
              Figure 17: K Medoids cluster characteristics (k=5)
            </p>
          </div>
        </div>
      </div>
      <div id="options-and-sensitivity-analysis-1" class="section level3 unnumbered" number="">
        <h3>Options and sensitivity analysis</h3>
        <p>The main option for k-medoids is the choice of the <strong>Method</strong>. As shown in
          Figure <a href="#fig:kmedoidmethods">18</a>, in addition to the default <strong>FastPAM</strong>, the
          algorithms <strong>FastCLARA</strong> and
          <strong>FastCLARANS</strong> are available as well. The latter two are large data methods and they will always
          perform (much) worse than FastPAM in small to medium-sized data sets. In our example, it would not
          be appropriate to use these methods, but we provide a brief illustration anyway to highlight the
          deterioration in the solutions found.
        </p>
        <p>In addition, there is a choice of <strong>Initialization Method</strong> between <strong>LAB</strong> and
          <strong>BUILD</strong>. In most
          circumstances, LAB is the preferred option, but BUILD is included for the sake of completeness and
          to allow for a full range of comparisons.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedoidmethods"></span>
          <img src="pics7c/5_method_options.png" alt="K Medoids method options" width="35%" />
          <p class="caption">
            Figure 18: K Medoids method options
          </p>
        </div>
        <div id="clara" class="section level4 unnumbered" number="">
          <h4>CLARA</h4>
          <p>The two main parameters that need to be specified for the CLARA method are the number of samples considered
            (by default set to 2) and the sample size. Since n &lt; 100 in our example, the latter is set to 40+2K = 50,
            shown in Figure <a href="#fig:kmedoidclara">19</a>. In addition, the option to include the best previous
            medoids in the
            sample is checked.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidclara"></span>
            <img src="pics7c/5_clara_options.png" alt="K Medoids CLARA parameters" width="35%" />
            <p class="caption">
              Figure 19: K Medoids CLARA parameters
            </p>
          </div>
          <p>The resulting cluster map is given in Figure <a href="#fig:kmedoidclaramap">20</a>, with associated cluster
            characteristics
            in Figure <a href="#fig:kmedoidclarasummary">21</a>.</p>
          <p>The main effect seems to be on CL4, which, even though it has the same number of members as for PAM, they
            are in
            totally different locations. CL1 increases its membership from 26 to 30. CL2 and CL3 shrink somewhat, mostly
            due to the (new) presence of the members of CL4. CL5 does not change.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidclaramap"></span>
            <img src="pics7c/5_clara_map.png" alt="K Medoids CLARA cluster map (k=5)" width="60%" />
            <p class="caption">
              Figure 20: K Medoids CLARA cluster map (k=5)
            </p>
          </div>
          <p>As the summary characteristics show, CL3 and CL4 now have different medoids.
            Overall, there is a slight deterioration of the quality of the clusters, with the total within-cluster
            distance now
            268.9 (compared to 265.1), for a ratio of 0.675 (compared to 0.665).</p>
          <p>In this example, the use of a sample instead of the full data set does not lead to
            a meaningful deterioration of the results. This is not totally surprising, since the sample size of 50 is
            almost 59% of
            the total sample size. For larger data sets with k=5, the sample size will remain at 50, thus constituting a
            smaller and smaller
            share of the actual observations as the sample size increases.<a href="#fn9" class="footnote-ref"
              id="fnref9"><sup>9</sup></a></p>
          <p>To illustrate the effect of sample size, we can set its value to 30. This results in a much larger overall
            within sum of distances
            of 273.9, with a ratio of 0.687 (details not shown). On the other hand, if we set the sample size to 85,
            then we obtain the exact
            same results as for PAM.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidclarasummary"></span>
            <img src="pics7c/5_clara_summary.png" alt="K Medoids CLARA cluster characteristics (k=5)" width="50%" />
            <p class="caption">
              Figure 21: K Medoids CLARA cluster characteristics (k=5)
            </p>
          </div>
        </div>
        <div id="clarans" class="section level4 unnumbered" number="">
          <h4>CLARANS</h4>
          <p>For CLARANS, the two relevant parameters pertain to the number of iterations and the sample rate, as shown
            in Figure <a href="#fig:kmedoidclarans">22</a>. The former corresponds to the <em>numlocal</em> parameter in
            <span class="citation">Ng and Han (<a href="#ref-NgHan:02" role="doc-biblioref">2002</a>)</span>,
            i.e., the number of times a local optimum is computed (default = 2). The sample rate pertains to the maximum
            number
            of <em>neighbors</em> that will be randomly sampled in each iteration (<em>maxneighbors</em> in the paper).
            This is
            expressed as a fraction of <span class="math inline">\(k \times (n - k)\)</span>. We use the value of 0.025
            recommended by <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19"
                role="doc-biblioref">2019</a>)</span>, which
            yields a maximum neighbors of 10 (0.025 x 400) for each iteration in our example. Unlike PAM and CLARA,
            there is no
            initialization option.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidclarans"></span>
            <img src="pics7c/5_clarans_options.png" alt="K Medoids CLARANS parameters" width="35%" />
            <p class="caption">
              Figure 22: K Medoids CLARANS parameters
            </p>
          </div>
          <p>The results are presented in Figures <a href="#fig:kmedoidclaransmap">23</a> and <a
              href="#fig:kmedoidclaranssummary">24</a>.
            The cluster map is quite different from the result for PAM. All but the original CL5 are considerably
            affected.
            CL1 from PAM moves southward and grows to 32 members. All the other clusters lose members. CL2 from PAM
            disintegrates
            and drops to 16 members. CL3 shifts in location, but stays roughly the same size. CL4 moves north.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidclaransmap"></span>
            <img src="pics7c/5_clarans_map.png" alt="K Medoids CLARANS cluster map (k=5)" width="60%" />
            <p class="caption">
              Figure 23: K Medoids CLARANS cluster map (k=5)
            </p>
          </div>
          <p>The cluster medoids listed in Figure <a href="#fig:kmedoidclaranssummary">24</a> show all different cluster
            centers under CLARANS, except for CL5 (which
            is essentially unchanged). The total within sum of distances is 301.177, quite a bit larger than 265.147 for
            PAM. Correspondingly, the
            ratio of within to total is much higher as well, at 0.756.</p>
          <p>CLARANS is a large data method and is optimized for speed (especially with large n and large k). It should
            not be used in smaller samples, where the exhaustive search carried out by PAM can be computed in a
            reasonable time.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmedoidclaranssummary"></span>
            <img src="pics7c/5_clarans_summary.png" alt="K Medoids CLARANS cluster characteristics (k=5)" width="50%" />
            <p class="caption">
              Figure 24: K Medoids CLARANS cluster characteristics (k=5)
            </p>
          </div>
        </div>
        <div id="comparison-of-methods-and-initialization-settings" class="section level4 unnumbered" number="">
          <h4>Comparison of methods and initialization settings</h4>
          <p>In order to provide a better idea of the various trade-offs involved in the selection of algorithms and
            initialization
            settings, Figures <a href="#fig:sdoh">25</a> and <a href="#fig:natregimes">26</a> list the results of a
            number of experiments, for different
            values of k and n.</p>
          <p>Figure <a href="#fig:sdoh">25</a> pertains to a sample of 791 Chicago census tracts and a clustering of
            socio-economic characteristics
            that were reduced to 5 principal components.<a href="#fn10" class="footnote-ref"
              id="fnref10"><sup>10</sup></a> For this number of
            observations, computation time is irrelevant, since all results are obtained almost instantaneously (in less
            than a second).</p>
          <p>The number of clusters is considered for k = 5, 10, 30, 77 (the number of community areas in the city), and
            150. Recall
            that the value of k is a critical component of the sample size for CLARA (80 + 2k) and CLARANS (a percentage
            of <span class="math inline">\(k \times (n-k)\)</span>).
            In all cases, PAM with LAB obtains the best local minimum (for k=5, it is tied with PAM-BUILD). Also,
            CLARANS consistently
            gives the worst result. The gap with the best local optimum grows as k gets larger, from about 6% larger for
            k=5 to more than
            20% for k=150. The results for CLARA are always in the middle, with LAB superior for k=5 and 10, and BUILD
            superior in the
            other instances.</p>
          <div class="figure" style="text-align: center"><span id="fig:sdoh"></span>
            <img src="pics7c/6_sdoh.png" alt="Local Optima - Chicago Census Tracts (n=791)" width="60%" />
            <p class="caption">
              Figure 25: Local Optima - Chicago Census Tracts (n=791)
            </p>
          </div>
          <p>Figure <a href="#fig:natregimes">26</a> uses the <strong>natregimes</strong> sample data set with 3085
            observations for U.S. counties. The number
            of variables to be clustered was increased to 20, consisting of the variables RD, PS, UE, DV and MA in all
            four years.</p>
          <p>In this instance, compute time does matter, and significantly so starting with k=30. CLARANS is always the
            fastest, from
            instantaneous at k=30 to 1 second for k=300 and 2 seconds for k=500. PAM is also reasonable, but quite a bit
            slower, with
            the LAB option always faster than BUILD. For k=30, the difference between the two is minimal (2 seconds vs 3
            seconds), but
            for k=300 PAM-LAB is about twice as fast (12 seconds vs 26 seconds), and for k=500 almost four times as fast
            (14 seconds vs
            41 seconds). The compute time for CLARA is problematic, especially when using BUILD and for k=300 and 500
            (for smaller k, it
            is on a par with PAM). For k=500, CLARA-BUILD takes some 6 minutes, whereas CLARA-LAB only take somewhat
            over a minute.</p>
          <p>The best local optimum is again obtained for PAM, with BUILD slightly better for k=10 and 30, whereas for
            the other cases
            the LAB option is superior. In contrast to the Chicago example, CLARANS is not always worst and is better
            than CLARA for
            k = 5, 10 and 30, but not for k = 300 and 500.</p>
          <div class="figure" style="text-align: center"><span id="fig:natregimes"></span>
            <img src="pics7c/6_natregimes.png" alt="Local Optima - U.S. Counties (n=3085)" width="60%" />
            <p class="caption">
              Figure 26: Local Optima - U.S. Counties (n=3085)
            </p>
          </div>
          <p>Overall, it seems that the FastPAM approach with the default setting of LAB performs very reliably and
            with decent (to superior) computation times. However, it remains a good idea to carry out further
            sensitivity analysis. Also, the algorithms only obtain <em>local</em> optima, and there is no guarantee of
            a global optimum. Therefore, if there is the opportunity to explore different options, they should be
            investigated.</p>
        </div>
      </div>
    </div>
    <div id="spectral-clustering" class="section level2 unnumbered" number="">
      <h2>Spectral Clustering</h2>
      <div id="principle-2" class="section level3 unnumbered" number="">
        <h3>Principle</h3>
        <p>Clustering methods like k-means, k-medians or k-medoids are designed to discover convex
          clusters in the multidimensional data cloud. However, several interesting cluster shapes are
          not convex, such as the classic textbook spirals or moons example, or the famous <em>Swiss roll</em> and
          similar lower
          dimensional shapes embedded in a higher dimensional space. These problems are characterized
          by a property that projections of the data points onto the original orthogonal coordinate axes
          (e.g., the x, y, z, etc. axes) do not create good separations. <em>Spectral clustering</em> approaches
          this issue by reprojecting the observations onto a new axes system and carrying out the
          clustering on the projected data points. Technically, this will boil down to the use
          of eigenvalues and eigenvectors, hence the designation as <em>spectral</em> clustering (recall the
          spectral decomposition of a matrix discussed in the chapter on principal components).</p>
        <p>To illustrate the problem, consider the result of k-means and k-medoids (for k=2) applied to the famous
          <em>spirals</em> data set from
          Figure <a href="#fig:spiralmap">2</a>. As Figure <a href="#fig:spiralskmean">27</a> shows, these
          methods tend to yield convex clusters, which fail to detect the nonlinear arrangement of the data. The k-means
          clusters are arranged above and below a diagonal, whereas the k-medoids result shows more of a left-right
          pattern.</p>
        <div class="figure" style="text-align: center"><span id="fig:spiralskmean"></span>
          <img src="pics7c/8_kmean_kmedoid_spiral.png" alt="K-means and K-medoids on spirals data set (k=2)"
            width="90%" />
          <p class="caption">
            Figure 27: K-means and K-medoids on spirals data set (k=2)
          </p>
        </div>
        <p>In constrast, as shown in Figure <a href="#fig:spiralsspectral">28</a>, a spectral clustering algorithm
          applied to this data set perfectly extracts the two underlying
          patterns (initialized with a knn parameter of 3, see below for further details and illustration).</p>
        <div class="figure" style="text-align: center"><span id="fig:spiralsspectral"></span>
          <img src="pics7c/8_spectral_spiral.png" alt="Spectral clustering on spirals data set (k=2)" width="60%" />
          <p class="caption">
            Figure 28: Spectral clustering on spirals data set (k=2)
          </p>
        </div>
        <p>The mathematics underlying spectral clustering view it as a problem of graph partitioning, i.e.,
          separating a graph into subgraphs that are internally well connected, but only weakly connected
          with the other subgraphs. We briefly discuss this idea first, followed by an overview of the
          main steps in a spectral clustering algorithm, including a review of some
          important parameters that need to be tuned in practice.</p>
        <p>An exhaustive overview of the various mathematical properties associated with spectral clustering
          is contained in the <em>tutorial</em> by <span class="citation">von Luxburg (<a href="#ref-vonLuxburg:07"
              role="doc-biblioref">2007</a>)</span>, to which we refer for technical details. An intuitive
          description is also given in <span class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12"
              role="doc-biblioref">2012</a>)</span>, pp. 519-522.</p>
        <div id="clustering-as-a-graph-partitioning-problem" class="section level4 unnumbered" number="">
          <h4>Clustering as a graph partitioning problem</h4>
          <p>So far, the clustering methods we have discussed were based on a <em>dissimilarity</em> matrix and had the
            objective of minimizing within-cluster dissimilarity. In spectral clustering, the focus is on the
            complement, i.e., a <em>similarity</em> matrix, and the goal is to maximize the internal similarity within a
            cluster. Of course, any dissimilarity matrix can be turned into a similarity matrix using a number
            of different methods, such as the use of a distance decay function (inverse distance, negative exponential)
            or
            by simply taking the difference from the maximum (e.g., <span class="math inline">\(d_{max} -
              d_{ij}\)</span>).</p>
          <p>A similarity matrix <span class="math inline">\(S\)</span> consisting of elements <span
              class="math inline">\(s_{ij}\)</span> can be viewed as the basis for the <em>adjacency matrix</em>
            of a weighted undirected graph <span class="math inline">\(G = (V,E)\)</span>. In this graph, the vertices
            <span class="math inline">\(V\)</span> are the observations and the edges <span
              class="math inline">\(E\)</span>
            give the strength of the similarity between <span class="math inline">\(i\)</span> and <span
              class="math inline">\(j\)</span>, <span class="math inline">\(s_{ij}\)</span>. This is identical to the
            interpretation of a
            spatial weights matrix that we have seen before. In fact, the standard notation for the adjacency matrix is
            to use <span class="math inline">\(W\)</span>, just as we did for spatial weights.</p>
          <p>Note that, in practice, the adjacency matrix is typically not the same as the full similarity matrix, but
            follows from a transformation of the latter to a <em>sparse</em> form (see below for specifics).</p>
          <p>The goal of graph partitioning is to delineate subgraphs that are internally strongly connected, but only
            weakly connected with the other subgraphs. In the ideal case, the subgraphs are so-called connected
            components,
            in that their elements are all internally connected, but there are no connections to the other subgraphs. In
            practice, this will rarely be the case. The objective thus becomes one of finding a set of <em>cuts</em> in
            the graph that
            create a partioning of <span class="math inline">\(k\)</span> subsets to maximize internal connectivity and
            minimize in-between connectivity. A naive application
            of this principle would lead to the least connected vertices to become singletons (similar to what we found
            in single and average linkage hierarchical clustering) and all the rest to form one large cluster. Better
            suited partitioning methods include a weighting of the cluster <em>size</em> so as to end up with
            well-balanced
            subset.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></p>
          <p>A very important concept in this regard is the <em>graph Laplacian</em> associated with the adjacency
            matrix <span class="math inline">\(W\)</span>. We
            discuss this further in the <a href="#appendix">Appendix</a>.</p>
        </div>
      </div>
      <div id="the-spectral-clustering-algorithm" class="section level3 unnumbered" number="">
        <h3>The spectral clustering algorithm</h3>
        <p>In general terms, a spectral clustering algorithm consists of four phases:</p>
        <ul>
          <li>
            <p>turning the similarity matrix into an adjacency matrix</p>
          </li>
          <li>
            <p>computing the <span class="math inline">\(k\)</span> smallest eigenvalues and eigenvectors of the graph
              Laplacian
              (alternatively, the <span class="math inline">\(k\)</span> largest eigenvalues and eigenvectors of the
              affinity matrix can be calculated)</p>
          </li>
          <li>
            <p>using the (rescaled) resulting eigenvectors to carry out k-means clustering</p>
          </li>
          <li>
            <p>associating the resulting clusters back to the original observations</p>
          </li>
        </ul>
        <p>The most important step is the construction of the adjacency matrix, which we consider first.</p>
        <div id="creating-an-adjacency-matrix" class="section level4 unnumbered" number="">
          <h4>Creating an adjacency matrix</h4>
          <p>The first phase consists of selecting a criterion to turn the <em>dense</em> similarity matrix into a
            <em>sparse</em> adjacency matrix, sometimes also referred to as the <em>affinity matrix</em>. The logic is
            very similar to that of creating spatial weights by means of
            a distance criterion.
          </p>
          <p>For example, we could use a distance band to select <em>neighbors</em> that are within a critical
            distance <span class="math inline">\(\delta\)</span>. In the spectral clustering literature, this is
            referred to as an epsilon (<span class="math inline">\(\epsilon\)</span>) criterion.
            This approach shares the same issues as in the spatial case when the observations are distributed with
            very different densities. In order to avoid isolates, a max-min nearest neighbor distance needs to be
            selected, which can result in a very unbalanced adjacency matrix. An adjacency matrix derived from the
            <span class="math inline">\(\epsilon\)</span> criterion is typically used in unweighted form.
          </p>
          <p>A preferred approach is to use k nearest neighbors, although this is not a symmetric property.
            Consequently,
            the resulting adjacency matrix is for a directed graph, since <span class="math inline">\(i\)</span> being
            one of the k nearest neighbors of <span class="math inline">\(j\)</span> does not
            guarantee that <span class="math inline">\(j\)</span> is one of the k nearest neighbors for <span
              class="math inline">\(i\)</span>. We can illustrate this with our toy example, using two
            nearest neighbors for observations 5 and 6 in Figure <a href="#fig:knn65">29</a>. In the left hand panel,
            the two neighbors of observation
            5 are shown as 4 and 7. In the right hand panel, 5 and 7 are neighbors of 6. So, while 5 is a neighbor of 6,
            the reverse is
            not true, creating an asymmetry in the affinity matrix (the same is true for 2 and 3: 3 is a neighbor of 2,
            but 2 is not
            a neighbor of 3).</p>
          <div class="figure" style="text-align: center"><span id="fig:knn65"></span>
            <img src="pics7c/8_nbrs5_6.png" alt="Asymmetry of nearest neighbors" width="60%" />
            <p class="caption">
              Figure 29: Asymmetry of nearest neighbors
            </p>
          </div>
          <p>Since the eigenvalue computations require a symmetric matrix, there are two approaches to remedy the
            asymmetry.
            In one, the affinity matrix is made symmetric as <span class="math inline">\((1/2) (W + W&#39;)\)</span>. In
            other words, if <span class="math inline">\(w_{ij} = 1\)</span>, but <span class="math inline">\(w_{ji} =
              0\)</span>
            (or the reverse), a
            new set of weights is created with <span class="math inline">\(w_{ij} = w_{ji} = 1/2\)</span>. This is
            illustrated in the left-hand panel of
            Figure <a href="#fig:mutualknn">30</a>, where the associated symmetric connectivity graph is shown.</p>
          <p>Instead of a pure k-nearest neighbor criterion, so-called <em>mutual</em> k nearest
            neighbors can be defined, which consists of those neighbors among the k-nearest neighbor set that <span
              class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>
            have in common. More precisely, only those connectivities are kept for which <span
              class="math inline">\(w_{ij} = w_{ji} = 1\)</span>. The
            corresponding connectivity graph for our example is shown in the right hand panel of Figure <a
              href="#fig:mutualknn">30</a>. The links
            between 2 and 3 as well as between 5 and 6 have been removed.</p>
          <p>Once the affinity matrix has been turned into a symmetric form, the resulting adjacencies are weighted
            with the original <span class="math inline">\(s_{ij}\)</span> values.</p>
          <div class="figure" style="text-align: center"><span id="fig:mutualknn"></span>
            <img src="pics7c/8_mutualandfullknn.png" alt="Symmetric k-nearest neighbor matrices" width="60%" />
            <p class="caption">
              Figure 30: Symmetric k-nearest neighbor matrices
            </p>
          </div>
          <p>The knn adjacency matrix can join points in disconnected parts of the graph, whereas the mutual
            k nearest neighbors will be sparser and tends to connect observations in
            regions with constant density. Each has pros and cons, depending on the underlying structure of the data.
          </p>
          <p>A final approach is to compute a similarity matrix that has a built-in distance decay. The most common
            method is to use a Gaussian density (or <em>kernel</em>) applied to the Euclidean distance between
            observations:
            <span class="math display">\[s_{ij} = \exp [- (x_i - x_j)^2 / 2\sigma^2],\]</span>
            where the standard deviation <span class="math inline">\(\sigma\)</span> plays the role of a bandwidth. With
            the right choice of <span class="math inline">\(\sigma\)</span>, we
            can make the corresponding adjacency matrix more or less sparse.
          </p>
          <p>Note that, in fact, the Gaussian transformation translates a
            dissimilarity matrix (Euclidean distances) into a similarity matrix. The new similarity matrix plays the
            role
            of the adjacency matrix in the remainder of the algorithm.</p>
        </div>
        <div id="clustering-on-the-eigenvectors-of-the-graph-laplacian" class="section level4 unnumbered" number="">
          <h4>Clustering on the eigenvectors of the graph Laplacian</h4>
          <p>With the adjacency matrix in place, the <span class="math inline">\(k\)</span> smallest eigenvalues and
            associated eigenvectors
            of the normalized graph Laplacian can be computed. Since we only need a few eigenvalues, specialized
            algorithms are used that only extract the smallest or largest eigenvalues/eigenvectors.<a href="#fn12"
              class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
          <p>When a symmetric normalized graph Laplacian is used, the <span class="math inline">\(n \times k\)</span>
            matrix of eigenvectors, say <span class="math inline">\(U\)</span>,
            is row-standardized such that the norm of each row equals 1. The new matrix <span
              class="math inline">\(T\)</span> has elements:<a href="#fn13" class="footnote-ref"
              id="fnref13"><sup>13</sup></a>
            <span class="math display">\[t_{ij} = \frac{u_{ij}}{(\sum_{h=1}^k u_{ih}^2)^{1/2}}.\]</span>
          </p>
          <p>The new “observations” consist of the values of <span class="math inline">\(t_{ij}\)</span> for each <span
              class="math inline">\(i\)</span>. These values are used in a
            standard k-means clustering algorithm to yield <span class="math inline">\(k\)</span> clusters. Finally, the
            labels for the clusters
            are associated with the original observations and several cluster characteristics can be computed.</p>
          <p>In <code>GeoDa</code>, the symmetric normalized adjacency matrix is used, porting to C++ the algorithm of
            <span class="citation">Ng, Jordan, and Weiss (<a href="#ref-Ngetal:02" role="doc-biblioref">2002</a>)</span>
            as implemented in Python in <code>scikit-learn</code>.
          </p>
        </div>
        <div id="spectral-clustering-parameters" class="section level4 unnumbered" number="">
          <h4>Spectral clustering parameters</h4>
          <p>In practice, the results of spectral clustering tend to be highly sensitive to the choice of the
            parameters that are used to define the adjacency matrix. For example, when using k-nearest neighbors,
            the choice of the number of neighbors is an important decision. In the literature,
            a value of <span class="math inline">\(k\)</span> (neighbors, not clusters) of the order of <span
              class="math inline">\(\log(n)\)</span> is suggested for large <span class="math inline">\(n\)</span> <span
              class="citation">(von Luxburg <a href="#ref-vonLuxburg:07" role="doc-biblioref">2007</a>)</span>. In
            practice,
            both <span class="math inline">\(\ln(n)\)</span> as well as <span
              class="math inline">\(\log_{10}(n)\)</span> are used. <code>GeoDa</code> provides both as options.<a
              href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
          <p>Simlarly, the bandwidth of the Gaussian transformation is determined by the value for the standard
            deviation, <span class="math inline">\(\sigma\)</span>.
            One suggestion for the value of <span class="math inline">\(\sigma\)</span> is to
            take the mean distance to the k nearest neighbor, or <span class="math inline">\(\sigma \sim \log(n) +
              1\)</span> <span class="citation">(von Luxburg <a href="#ref-vonLuxburg:07"
                role="doc-biblioref">2007</a>)</span>.
            Again, either <span class="math inline">\(\ln(n) + 1\)</span> or <span class="math inline">\(\log_{10}(n) +
              1\)</span> can be implemented. In addition, the default
            value used in the <code>scikit-learn</code> implementation suggests <span class="math inline">\(\sigma =
              \sqrt{1/p}\)</span>, where <span class="math inline">\(p\)</span> is the
            number of variables (features) used.
            <code>GeoDa</code> includes all three options.
          </p>
          <p>In practice, these parameters are best set by trial and error, and a careful sensitivity
            analysis is in order.</p>
        </div>
      </div>
      <div id="implementation-2" class="section level3 unnumbered" number="">
        <h3>Implementation</h3>
        <p>Spectral clustering is invoked from the <strong>Clusters</strong> toolbar, as the next to last item in the
          classic clustering subset, as shown
          in Figure <a href="#fig:spectral">31</a>.
          Alternatively, from the menu, it is selected as <strong>Clusters &gt; Spectral</strong>.</p>
        <div class="figure" style="text-align: center"><span id="fig:spectral"></span>
          <img src="pics7c/8_spectral.png" alt="Spectral Clustering Option" width="10%" />
          <p class="caption">
            Figure 31: Spectral Clustering Option
          </p>
        </div>
        <p>We use the <strong>spirals</strong> data set, with an associated variable settings panel that only
          contains the variables <strong>x</strong> and <strong>y</strong>, as shown in Figure <a
            href="#fig:spectralvars">32</a>.</p>
        <div id="variable-settings-panel-2" class="section level4 unnumbered" number="">
          <h4>Variable Settings Panel</h4>
          <p>The variable settings panel has the same general layout as for the other clustering methods. One
            distinction
            is that there are two sets of parameters. One set pertains to the construction of the
            <strong>Affinity</strong> (or adjacency) matrix,
            the others are relevant for the k-means algorithm that is applied to the transformed eigenvectors. The
            k-means options
            are the standard ones.</p>
          <p>The <strong>Affinity</strong> option provides three alternatives, <strong>K-NN</strong>, <strong>Mutual
              K-NN</strong> and <strong>Gaussian</strong>, with specific parameters for
            each. For now, we set the number of clusters to 2 and keep all options to the default setting. This includes
            <strong>K-NN</strong> with 3 neighbors
            for the affinity matrix, and all the default settings for k-means. The value of 3 for knn corresponds to
            <span class="math inline">\(\log_{10}(300) = 2.48\)</span>, rounded up to the
            next integer.</p>
          <p>We save the cluster in the field <strong>CLs1</strong>.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralvars"></span>
            <img src="pics7c/00_spectral_variables.png" alt="Spectral clustering variable selection" width="35%" />
            <p class="caption">
              Figure 32: Spectral clustering variable selection
            </p>
          </div>
        </div>
        <div id="cluster-results-2" class="section level4 unnumbered" number="">
          <h4>Cluster results</h4>
          <p>The cluster map in Figure <a href="#fig:spectralmap">33</a> shows a perfect separation of the two spirals,
            with 150 observations
            each. As mentioned earlier, neither k-means nor k-medoids is able to extract these highly nonlinear and
            non-convex clusters.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralmap"></span>
            <img src="pics7c/00_spectral_default_map.png" alt="Spectral cluster map (k=2)" width="60%" />
            <p class="caption">
              Figure 33: Spectral cluster map (k=2)
            </p>
          </div>
          <p>The cluster characteristics in Figure <a href="#fig:spectralsummary">34</a> list the parameter settings
            first, followed by the values
            for the cluster centers (the mean) for the two (standardized) coordinates and the decomposition of the sum
            of squares. The
            ratio of between sum of squares to total sum of squares is a dismal 0.04. This is not surprising, since this
            criterion provides a
            measure of the degree of compactness for the cluster, which a non-convex cluster like the spirals example
            does not meet.</p>
          <p>In this example, it is easy to visually assess the extent to which the nonlinearity is captured. However,
            in the typical high-dimensional
            application, this will be much more of a challenge, since the usual measures of compactness may not be
            informative. A careful
            inspection of the distribution of the different variables across the observations in each cluster is
            therefore in order.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralsummary"></span>
            <img src="pics7c/00_spectral_default_results.png" alt="Spectral cluster characteristics (k=2)"
              width="50%" />
            <p class="caption">
              Figure 34: Spectral cluster characteristics (k=2)
            </p>
          </div>
        </div>
      </div>
      <div id="options-and-sensitivity-analysis-2" class="section level3 unnumbered" number="">
        <h3>Options and sensitivity analysis</h3>
        <p>The results of spectral clustering are extremely sensitive to the parameters chosen to create
          the affinity matrix. Suggestions for default values are only suggestions, and the particular values
          many sometimes be totally unsuitable. Experimentation is therefor a necessity. There are two
          classes of parameters. One set pertains to the number of nearest neighbors for knn or mutual knn.
          The other set relates to the bandwidth of the Gaussian kernel, determined by the standard deviation
          <strong>sigma</strong>.
        </p>
        <div id="k-nearest-neighbors-affinity-matrix" class="section level4 unnumbered" number="">
          <h4>K-nearest neighbors affinity matrix</h4>
          <p>The two default values for the number of nearest neighbors are contained in a drop-down list, as shown in
            Figure <a href="#fig:spectralknnparms">35</a>. In our example, with n=300, <span
              class="math inline">\(\log_{10}(n) = 2.48\)</span>, which rounds up to 3,
            and <span class="math inline">\(\ln(n) = 5.70\)</span>, which rounds up to 6. These are the two default
            values provided. Any other value can
            be entered manually in the dialog as well.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralknnparms"></span>
            <img src="pics7c/00_affinity_knn.png" alt="KNN affinity parameter values" width="25%" />
            <p class="caption">
              Figure 35: KNN affinity parameter values
            </p>
          </div>
          <p>The cluster map with knn = 6 is shown in Figure <a href="#fig:spectralknn6">36</a>. Unlike what we found
            for the
            default option, this value is not able to yield a clean separation of the spirals. In addition, the
            resulting clusters are highly unbalanced, with respectively 241 and 59 members.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralknn6"></span>
            <img src="pics7c/00_spectral_knn_6.png" alt="KNN affinity k=6 cluster map" width="60%" />
            <p class="caption">
              Figure 36: KNN affinity k=6 cluster map
            </p>
          </div>
          <p>The options for a mutual knn affinity matrix have the same entries, as in Figure <a
              href="#fig:spectralmutualknnparms">37</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralmutualknnparms"></span>
            <img src="pics7c/00_affinity_mutualknn.png" alt="Mutual KNN affinity parameter values" width="25%" />
            <p class="caption">
              Figure 37: Mutual KNN affinity parameter values
            </p>
          </div>
          <p>Here again, neither of the options yields a satisfactory solution, as illustrated in Figure <a
              href="#fig:spectralmutual">38</a>,
            for knn=3 in the left panel and knn=6 in the right panel. The first solution has members in both spirals,
            whereas the second
            solution does not cross over, but it only picks up part of the separate spiral.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralmutual"></span>
            <img src="pics7c/00_mutualknn.png" alt="Mutual KNN affinity cluster maps" width="90%" />
            <p class="caption">
              Figure 38: Mutual KNN affinity cluster maps
            </p>
          </div>
        </div>
        <div id="gaussian-kernel-affinity-matrix" class="section level4 unnumbered" number="">
          <h4>Gaussian kernel affinity matrix</h4>
          <p>The built-in options for <strong>sigma</strong>, the standard deviation of the Gaussian kernel are listed
            in Figure <a href="#fig:spectralgaussianparms">39</a>.
            The smallest value of 0.707107 corresponds to <span class="math inline">\(\sqrt{1/p}\)</span>, where <span
              class="math inline">\(p\)</span>, the number of variables, equals 2 in our example.
            The other two values are <span class="math inline">\(\log_{10}(n) + 1\)</span> and <span
              class="math inline">\(\ln(n) + 1\)</span>, yielding respectively 3.477121 and 6.703782 for n=300. In
            addition,
            any other value can be entered in the dialog.</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralgaussianparms"></span>
            <img src="pics7c/00_affinity_sigma.png" alt="Gaussian kernel affinity parameter values" width="25%" />
            <p class="caption">
              Figure 39: Gaussian kernel affinity parameter values
            </p>
          </div>
          <p>None of the default values yield particularly good results, as illustrated in Figure <a
              href="#fig:spectralgaussian2">40</a>.
            In the left hand panel, the clusters are shown with <span class="math inline">\(\sigma = 0.707107\)</span>.
            The result totally fails to extract the shape of
            the separate spirals and looks similar to the results for k-mean and k-median in Figure <a
              href="#fig:spiralskmean">27</a>. The result
            for <span class="math inline">\(\sigma = 6.703782\)</span> is almost identical, with the roles of cluster 1
            and 2 switched. Both are perfectly balanced
            clusters (the result for <span class="math inline">\(\sigma = 3.477121\)</span> are similar).</p>
          <div class="figure" style="text-align: center"><span id="fig:spectralgaussian2"></span>
            <img src="pics7c/00_gaussian.png" alt="Gaussian kernel affinity cluster maps" width="90%" />
            <p class="caption">
              Figure 40: Gaussian kernel affinity cluster maps
            </p>
          </div>
          <p>In order to find a solution that provides the same separation as in
            Figure <a href="#fig:spectralmap">33</a>, we need to experiment with different values for <span
              class="math inline">\(\sigma\)</span>. As it turns
            out, we obtain the same result as for knn with 3 neighbors for <span class="math inline">\(\sigma =
              0.08\)</span> or <span class="math inline">\(0.07\)</span>, neither of which
            are even close to the default values. This illustrates how in an actual example, where the results cannot
            be readily visualized in two dimensions, it may be very difficult to find the parameter values that
            discover the true underlying patterns.</p>
        </div>
      </div>
    </div>
    <div id="appendix" class="section level2 unnumbered" number="">
      <h2>Appendix</h2>
      <div id="worked-example-for-k-medians" class="section level3 unnumbered" number="">
        <h3>Worked example for k-medians</h3>
        <p>We use the same toy example as in the previous chapters to illustrate the workings of the
          k-median algorithm. For easy reference, the coordinates of the seven points are listed again
          in the second and third columns of Figure <a href="#fig:kmedianex1">41</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedianex1"></span>
          <img src="pics7c/2_exampledata.png" alt="Worked example - basic data" width="35%" />
          <p class="caption">
            Figure 41: Worked example - basic data
          </p>
        </div>
        <p>The corresponding dissimilarity matrix is based on the Manhattan distance metric, i.e., the sum
          of the absolute difference in the x and y coordinates between the points. The result is given in
          Figure <a href="#fig:manhattand">42</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:manhattand"></span>
          <img src="pics7c/2_manhattan_distance.png" alt="Manhattan distance matrix" width="80%" />
          <p class="caption">
            Figure 42: Manhattan distance matrix
          </p>
        </div>
        <p>The median center of the data cloud consists of the median of the x and y coordinates. In our case,
          this is the point (6,6), as shown in Figure <a href="#fig:kmedianex1">41</a>. This happens to coincide with
          one of the
          observations (4), but in general this would not be the case. The distances from each observation to the
          median center are listed in the fourth column of <a href="#fig:kmedianex1">41</a>. The sum of these distances
          (24) can
          be used to gauge the improvement that follows from each subsequent cluster allocation.</p>
        <p>As in the k-means algorithm, the first step consists of randomly selecting k starting centers. In our
          example, with k=2, we select observations 4 and 7, as we did for k-means.
          Figure <a href="#fig:kmedian1">43</a> gives the Manhattan distance between each observation and each of the
          centers.
          Observations closest to the respective center are grouped into the two initial clusters. In our example,
          these first two clusters consist of 1, 2, 3, and 5 assigned to center 4, and 6 assigned to center 7.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedian1"></span>
          <img src="pics7c/2_kmedian1.png" alt="K-median - step 1" width="25%" />
          <p class="caption">
            Figure 43: K-median - step 1
          </p>
        </div>
        <p>Next, we calculate the new cluster medians and compute the total within cluster distance as the sum of
          the distances from each allocated observation to the median center. This is illustrated in
          Figure <a href="#fig:kmedian1a">44</a>. For the first cluster, the median is (4,5) and the total within
          cluster
          distance is 14. In the second cluster, the median is (8.5,7) and the total is 3 (in the case of
          an even number of observations in the cluster, the midpoint
          between the two values closest to the median is chosen, hence 8.5 and 7). The value of the objective
          function after this first step is thus 14 + 3 = 17.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedian1a"></span>
          <img src="pics7c/2_kmedian_alloc1.png" alt="K-median - step 1 distance to median" width="70%" />
          <p class="caption">
            Figure 44: K-median - step 1 distance to median
          </p>
        </div>
        <p>We repeat the procedure, calculate the distance from each observation to both cluster centers and
          assign the observation to the closest center. This results in observation 5 moving from cluster 1
          to cluster 2, as illustrated in Figure <a href="#fig:kmedian2">45</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedian2"></span>
          <img src="pics7c/2_kmedian2.png" alt="K-median - step 2" width="25%" />
          <p class="caption">
            Figure 45: K-median - step 2
          </p>
        </div>
        <p>The new allocation results in (4,3.5) as the median center for the first cluster, and (8,6) as the
          center for the second cluster, as shown in
          Figure <a href="#fig:kmedian2a">46</a>. The total distance decreases to 10 + 4 = 14.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedian2a"></span>
          <img src="pics7c/2_kmedian_alloc2.png" alt="K-median - step 2 distance to median" width="70%" />
          <p class="caption">
            Figure 46: K-median - step 2 distance to median
          </p>
        </div>
        <p>We compute the distances to the two new centers in Figure <a href="#fig:kmedian3">47</a> and assign the
          observations to the closest center. Now, observation 4 moves from the first cluster to the second cluster.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedian3"></span>
          <img src="pics7c/2_kmedian3.png" alt="K-median - step 3" width="25%" />
          <p class="caption">
            Figure 47: K-median - step 3
          </p>
        </div>
        <p>The updated median centers are (4,3) and (7.5,6) as shown in Figure <a href="#fig:kmedian3a">48</a>. The
          total
          distance is 5 + 6 = 11.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedian3a"></span>
          <img src="pics7c/2_kmedian_alloc3.png" alt="K-median - step 3 distance to median" width="70%" />
          <p class="caption">
            Figure 48: K-median - step 3 distance to median
          </p>
        </div>
        <p>Finally, computing the distances to the new centers does not induce any more changes, and the
          algorithm concludes (Figure <a href="#fig:kmedianf">49</a>).</p>
        <div class="figure" style="text-align: center"><span id="fig:kmedianf"></span>
          <img src="pics7c/2_kmedian_final.png" alt="K-median - final allocation" width="25%" />
          <p class="caption">
            Figure 49: K-median - final allocation
          </p>
        </div>
        <p>The first cluster consists of 1, 2, and 3 with a median center of (4,3), the second cluster consists
          of 4, 5, 6, and 7 with a median center of (7.5,6). Neither of these are actual observations. The clustering
          resulted in a reduction of the total sum of distances to the center from 24 to 11.</p>
        <p>The ratio 11/24 = 0.458 gives a
          measure of the relative improvement of the objective function. It should not be confused with the ratio of
          within sum of squares to total sum of squares, since the latter uses a different measure of fit (squared
          differences
          instead of absolute differences) and a different reference point (the mean of the cluster rather than the
          median).
          So, the two measures are not comparable.</p>
      </div>
      <div id="worked-example-for-pam" class="section level3 unnumbered" number="">
        <h3>Worked example for PAM</h3>
        <p>We continue to use the same toy example, with the point coordinates as in Figure <a
            href="#fig:kmedianex1">41</a>
          and associated Manhattan distance matrix in Figure <a href="#fig:manhattand">42</a>.</p>
        <div id="build" class="section level4 unnumbered" number="">
          <h4>BUILD</h4>
          <p>In the BUILD phase, since <span class="math inline">\(k=2\)</span>, we need find two starting centers. The
            first center is the observation that minimizes the row or
            column sum of the Manhattan distance matrix. In Figure <a href="#fig:kmedianex1">41</a>, we saw that the
            median center
            coincided with observation 4 (6,6), with an associated sum of distances to the center of 24. That is our
            point
            of departure. To find the second center, we need for each candidate (i.e., the 6 remaining observations) the
            distance to observation 4, the current <em>closest</em> center (there is only one, so that is
            straightforward in our
            example). The associated values are listed below the observation ID in the second row of Figure <a
              href="#fig:build">50</a>.</p>
          <p>Next, we evaluate for each row-column combination <span class="math inline">\(i,j\)</span> the expression
            <span class="math inline">\(\mbox{max}(d_{j4} - d_{ij},0)\)</span>, and enter that
            in the corresponding element of the matrix. For example, for column 1 and row 2, that value is <span
              class="math inline">\(7 - d_{2,1} = 7 - 3 = 4\)</span>.
            For column 1 and row 5, the corresponding value is <span class="math inline">\(7 - 8 = -1\)</span>, which
            results in a zero entry.</p>
          <p>With all the values entered in the matrix, we compute the row sums. There is a tie between observations 3
            and 7. For
            consistency with our other examples, we pick 7 as the second starting point.</p>
          <div class="figure" style="text-align: center"><span id="fig:build"></span>
            <img src="pics7c/3_build.png" alt="PAM - finding the second initial center" width="70%" />
            <p class="caption">
              Figure 50: PAM - finding the second initial center
            </p>
          </div>
        </div>
        <div id="swap" class="section level4 unnumbered" number="">
          <h4>SWAP</h4>
          <p>The initial stage is the same as for the k-median example, given in Figure <a href="#fig:kmedian1">43</a>.
            The total
            distance to each cluster center equals 20, with cluster 1 contributing 17 and cluster 2, 3.</p>
          <p>The first step in the swap procedure consists of evaluating whether center 4 or center 7 can be replaced
            by any of the current non-centers, i.e., 1, 2, 3, 5, or 6. The comparison involves three distances for each
            point: the distance
            to the closest center, the distance to the second closest center, and the distance to the candidate center.
            In our example, this is greatly simplified, since there are only two centers, with distances <span
              class="math inline">\(d_{j4}\)</span> and
            <span class="math inline">\(d_{j7}\)</span> for each non-candidate and non-center point <span
              class="math inline">\(j\)</span>. In addition, we need the distance to the candidate
            center <span class="math inline">\(d_{jr}\)</span>, where each non-center point is in turn considered as a
            candidate (<span class="math inline">\(r\)</span> in our notation).
          </p>
          <p>All the evaluations for the first step are included in Figure <a href="#fig:swap1">51</a>. There are five
            main panels,
            one for each current non-center point. The rows in each panel are the non-center, non-candidate points.
            For example, in the top panel, 1 is considered a candidate, so the rows pertain to 2, 3, 5, and 6.
            Columns 3-4 give the distance to, respectively, center 4 (<span class="math inline">\(d_{j4}\)</span>),
            center 7 (<span class="math inline">\(d_{j7}\)</span>) and candidate center 1 (<span
              class="math inline">\(d_{j1}\)</span>).</p>
          <p>Columns 5 and 6 give the contribution of each row to the objective with point 1 replacing, respectively 4
            (<span class="math inline">\(C_{j41}\)</span>) and
            7 (<span class="math inline">\(C_{j71}\)</span>). First consider a replacement of 4 by 1. For point 2, the
            distances to 4 and 7 are 6 and 9, so point 2 is closest to center 4. As a result 2 will be allocated to
            either the new candidate center 1 or the current center 7. It is closest to the new center (3 relative to
            9). The decrease in the objective from assigning 2 to 1 rather than 4 is 3 - 6 = -3, the entry in the
            column <span class="math inline">\(C_{j41}\)</span>.</p>
          <p>Now consider the contribution of 2 to the replacement of 7 by 1. Since 2 is closer to 4, we have the
            situation that a point is <em>not</em> closest to the center that is to be replaced. So, now we need to
            check whether it would stay with its current center (4) or move to the candidate. We already know that 2 is
            closer to 1 than to 4, so the gain from the swap is again -3, entered
            under <span class="math inline">\(C_{j71}\)</span>. We proceed in the same way for each of the other
            non-center and non-candidate points and find the sum of
            the contributions, listed in the row labeled <span class="math inline">\(T\)</span>. For a replacement of 4
            by 1, the sum of -3, 1, 1, and 0 gives -1 as the
            value of <span class="math inline">\(T_{41}\)</span>. Similarly, the value of <span
              class="math inline">\(T_{47}\)</span> is the sum of -3, 0, 0, and 1, or -2.</p>
          <p>The remaining panels show the results when each of the other current non-centers is evaluated
            as a center candidate. The minimum value over all pairs <span class="math inline">\(i,r\)</span> is obtained
            for <span class="math inline">\(T_{43} = -5\)</span>. This suggests that center 4 should
            be replaced by point 3 (there is actually a tie with <span class="math inline">\(T_{73}\)</span>, so
            in each case 3 should enter the center set; we take it to replace 4). The improvement in the overall
            objective function from this step
            is -5.</p>
          <div class="figure" style="text-align: center"><span id="fig:swap1"></span>
            <img src="pics7c/3_step1.png" alt="PAM - swap step 1" width="50%" />
            <p class="caption">
              Figure 51: PAM - swap step 1
            </p>
          </div>
          <p>This process is repeated in Figure <a href="#fig:swap2">52</a>, but now using <span
              class="math inline">\(d_{j3}\)</span> and <span class="math inline">\(d_{j7}\)</span> as reference
            distances. The smallest value for <span class="math inline">\(T\)</span> is found for <span
              class="math inline">\(T_{75} = -2\)</span>, which is also the improvement
            to the objective function (note that the improvement is smaller than for the first step, something
            we would expect from a gradient descent method). This suggests that 7 should be replaced by 5.</p>
          <div class="figure" style="text-align: center"><span id="fig:swap2"></span>
            <img src="pics7c/3_step2.png" alt="PAM - swap step 2" width="50%" />
            <p class="caption">
              Figure 52: PAM - swap step 2
            </p>
          </div>
          <p>In the next step, we repeat the calculations, using <span class="math inline">\(d_{j3}\)</span> and <span
              class="math inline">\(d_{j5}\)</span> as the distances. The smallest
            value for <span class="math inline">\(T\)</span> is found for <span class="math inline">\(T_{32} =
              -1\)</span>, suggesting that 3 should be replaced by 2. The improvement in
            the objective is -1 (again, smaller than in the previous step).</p>
          <div class="figure" style="text-align: center"><span id="fig:swap3"></span>
            <img src="pics7c/3_step3.png" alt="PAM - swap step 3" width="50%" />
            <p class="caption">
              Figure 53: PAM - swap step 3
            </p>
          </div>
          <p>In the last step, we compute everything again for <span class="math inline">\(d_{j2}\)</span> and <span
              class="math inline">\(d_{j5}\)</span>. At this stage, none of the <span class="math inline">\(T\)</span>
            yield
            a negative value, so the algorithm has reached a local optimum and stops.</p>
          <div class="figure" style="text-align: center"><span id="fig:swap4"></span>
            <img src="pics7c/3_step4.png" alt="PAM - swap step 4" width="50%" />
            <p class="caption">
              Figure 54: PAM - swap step 4
            </p>
          </div>
          <p>The final result consists of a cluster of three elements, centered on 2, and a cluster of four elements,
            centered on 5. As Figure <a href="#fig:swapfinal">55</a> shows, both clusters contribute 6 to the total sum
            of deviations,
            for a final value of 12. This also turns out to be 20 - 5 - 2 - 1, or the total effect of each swap on the
            objective function.</p>
          <div class="figure" style="text-align: center"><span id="fig:swapfinal"></span>
            <img src="pics7c/3_final.png" alt="PAM - final solution" width="25%" />
            <p class="caption">
              Figure 55: PAM - final solution
            </p>
          </div>
        </div>
      </div>
      <div id="the-graph-laplacian" class="section level3 unnumbered" number="">
        <h3>The graph Laplacian</h3>
        <p>A useful property of the adjancency matrix <span class="math inline">\(W\)</span> associated with a graph is
          the <em>degree</em> of a vertex <span class="math inline">\(i\)</span>:
          <span class="math display">\[d_i = \sum_j w_{ij},\]</span>
          the row-sum of the similarity weights.
          Previously, we used an identical concept to characterize the connectivity structure of a spatial weights, and
          referred to it as neighbor cardinality.
        </p>
        <p>The <em>graph Laplacian</em> is the following matrix:
          <span class="math display">\[L = D - W,\]</span>
          where <span class="math inline">\(D\)</span> is a diagonal matrix containing the degree of each vertex. The
          graph Laplacian has the
          property that all its eigenvalues are real and non-negative, and, most importantly, that its smallest
          eigenvalue is zero.
        </p>
        <p>In the (ideal) case where the adjacency matrix can be organized into separate partitions (unconnected to
          each other), it takes on a block-diagonal form, with each block containing the partioning sub-matrix for
          the matching group. The corresponding graph Laplacian will similarly have a block-diagonal structure.
          Since each of these sub-blocks is itself a graph Laplacian (for the subnetwork corresponding to the
          partition), its smallest eigenvalue is zero as well. An important result is then that if the graph
          is partitioned into <span class="math inline">\(k\)</span> disconnected blocks, the graph Laplacian will have
          <span class="math inline">\(k\)</span> zero eigenvalues. This
          forms the basis for the logic of using the <span class="math inline">\(k\)</span> smallest eigenvalues of
          <span class="math inline">\(L\)</span> to find the corresponding
          clusters.</p>
        <p>While it can be used to proceed with spectral clustering, the unnormalized Laplacian <span
            class="math inline">\(L\)</span> has some undesirable properties.
          Instead, the preferred approach is to use a so-called <em>normalized</em> Laplacian. There are two ways to
          normalize the adjacency
          matrix and thus the associated Laplacian. One is to <em>row-standardize</em> the adjacency matrix, or <span
            class="math inline">\(D^{-1}W\)</span>. This is exactly
          the same idea as row-standardizing a spatial weights matrix. When applied to the Laplacian, this yields:
          <span class="math display">\[L_{rw} = D^{-1}L = D^{-1}D - D^{-1}W = I - D^{-1}W.\]</span>
          This is referred to as a <em>random walk</em> normalized graph Laplacian, since the row elements can be viewed
          as transition probabilities from state <span class="math inline">\(i\)</span> to each of the other states
          <span class="math inline">\(j\)</span>. As we saw with spatial weights, the
          resulting normalized matrix is no longer symmetric, although its eigenvalues remain real, with the smallest
          eigenvalue
          being zero. The associated eigenvector is <span class="math inline">\(\iota\)</span>, a vector of ones.
        </p>
        <p>A second transformation pre- and post-multiplies the Laplacian by <span
            class="math inline">\(D^{-1/2}\)</span>, the inverse of the square root of the degree.
          This yields a <em>symmetric</em> normalized Laplacian as:
          <span class="math display">\[L_{sym} = D^{-1/2}LD^{-1/2} = D^{-1/2}DD^{-1/2} - D^{-1/2}WD^{-1/2} = I -
            D^{-1/2}WD^{-1/2}.\]</span>
          Again, the smallest eigenvalue is zero, but the associated eigenvector is <span
            class="math inline">\(D^{1/2}\iota\)</span>.
        </p>
        <p>Spectral clustering algorithms differ by whether the unnormalized or normalized Laplacian is used to compute
          the <span class="math inline">\(k\)</span> smallest eigenvalues and associated eigenvectors and whether the
          Laplacian or the adjacency matrix
          is the basis for the calculation.</p>
        <p>Specifically, as an alternative to using the smallest eigenvalues and associated eigenvectors of the
          normalized Laplacian,
          the <em>largest</em> eigenvalues/eigenvectors of the normalized adjacency (or affinity) matrix can be
          computed. The standard eigenvalue
          expression is the following equality:
          <span class="math display">\[Lu = \lambda u,\]</span>
          where <span class="math inline">\(u\)</span> is the eigenvector associated with eigenvalue <span
            class="math inline">\(\lambda\)</span>. If we subtract both sides from <span
            class="math inline">\(Iu\)</span>, we get:
          <span class="math display">\[(I - L)u = (1 - \lambda)u.\]</span>
          In other words, if the smallest eigenvalue of <span class="math inline">\(L\)</span> is 0, then the largest
          eigenvalue of <span class="math inline">\(I - L\)</span> is 1. Moreover:
          <span class="math display">\[I - L = I - [I - D^{-1/2}WD^{-1/2}] = D^{-1/2}WD^{-1/2},\]</span>
          so that the search for the smallest eigenvalue of <span class="math inline">\(L\)</span> is equivalent to the
          search for the <em>largest</em> eigenvalue
          of <span class="math inline">\(D^{-1/2}WD^{-1/2}\)</span>, the normalized adjacency matrix.
        </p>
        <p><br></p>
      </div>
    </div>
    <div id="references" class="section level2 unnumbered" number="">
      <h2>References</h2>
      <div id="refs" class="references hanging-indent">
        <div id="ref-Anselin:19b">
          <p>Anselin, Luc. 2019. “Quantile Local Spatial Autocorrelation.” <em>Letters in Spatial and Resource
              Sciences</em> 12 (2): 155–66.</p>
        </div>
        <div id="ref-Hanetal:12">
          <p>Han, Jiawei, Micheline Kamber, and Jian Pei. 2012. <em>Data Mining (Third Edition)</em>. Amsterdam:
            MorganKaufman.</p>
        </div>
        <div id="ref-Hastieetal:09">
          <p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning (2nd
              Edition)</em>. New York, NY: Springer.</p>
        </div>
        <div id="ref-deHoonetal:17">
          <p>Hoon, Michiel de, Seiya Imoto, and Satoru Miyano. 2017. “The C Clustering Library.” Tokyo, Japan: The
            University of Tokyo, Institute of Medical Science, Human Genome Center.</p>
        </div>
        <div id="ref-KaufmanRousseeuw:05">
          <p>Kaufman, L., and P. Rousseeuw. 2005. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>.
            New York, NY: John Wiley.</p>
        </div>
        <div id="ref-Ngetal:02">
          <p>Ng, Andrew Y., Michael I. Jordan, and Yair Weiss. 2002. “On Spectral Clustering: Analysis and an
            Algorithm.” In <em>Advances in Neural Information Processing Systems 14</em>, edited by T. G. Dietterich, S.
            Becker, and Z. Ghahramani, 849–56. Cambridge, MA: MIT Press.</p>
        </div>
        <div id="ref-NgHan:02">
          <p>Ng, Raymond R., and Jiawei Han. 2002. “CLARANS: A Method for Clustering Objects for Spatial Data Mining.”
            <em>IEEE Transactions on Knowledge and Data Engineering</em> 14: 1003–16.</p>
        </div>
        <div id="ref-SchubertRousseeuw:19">
          <p>Schubert, Erich, and Peter J. Rousseeuw. 2019. “Faster k-Medoids Clustering: Improving the PAM, CLARA, and
            CLARANS Algorithms.” In <em>Similarity Search and Applications, SISAP 2019</em>, edited by Giuseppe Amato,
            Claudio Gennaro, Vincent Oria, and Miloš Radovanović, 171–87. Cham, Switzerland: Springer Nature.</p>
        </div>
        <div id="ref-ShiMalik:00">
          <p>Shi, Jianbo, and Jitendra Malik. 2000. “Normalized Cuts and Image Segmentation.” <em>IEEE Transactions on
              Pattern Analysis and Machine Intelligence</em> 22 (8): 888–905.</p>
        </div>
        <div id="ref-vonLuxburg:07">
          <p>von Luxburg, Ulrike. 2007. “A Tutorial on Spectral Clustering.” <em>Statistical Computing</em> 27 (4):
            395–416.</p>
        </div>
      </div>
    </div>
    <div class="footnotes">
      <hr />
      <ol>
        <li id="fn1">
          <p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu"
              class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn2">
          <p>Strictly speaking, k-medoids
            minimizes the average distance to the representative center, but the sum
            is easier for computational reasons.<a href="#fnref2" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn3">
          <p>As <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
                role="doc-biblioref">2005</a>)</span> show in Chapter 2, the problem is identical
            to some facility location problems for which integer programming branch and bound solutions have
            been suggested. But these tend to be limited so smaller sized problems.<a href="#fnref3"
              class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn4">
          <p>Clearly, with a sample
            size of 100%, CLARA becomes the same as PAM.<a href="#fnref4" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn5">
          <p>More precisely, the first sample consists of 40 + 2k
            random points. From the second sample on, the best k medoids found in a previous iteration are included, so
            that
            there are 40 + k additional random points. Also, in <span class="citation">Schubert and Rousseeuw (<a
                href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span>, they suggested to use 80 + 4k
            and 10 repetitions for larger data sets. In the implementation in <code>GeoDa</code>, the latter is used for
            data
            sets larger than 100.<a href="#fnref5" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn6">
          <p><span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19"
                role="doc-biblioref">2019</a>)</span> also consider 2.5% in larger data sets with 4 iterations instead
            of 2.<a href="#fnref6" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn7">
          <p>The Java
            code is contained in the open source <code>ELKI</code> software, available from <a
              href="https://elki-project.github.io" class="uri">https://elki-project.github.io</a>.<a href="#fnref7"
              class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn8">
          <p>This is the approach
            used to obtain the selections in Figure <a href="#fig:kmedoidcenters">16</a>.<a href="#fnref8"
              class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn9">
          <p>As mentioned, for sample sizes &gt; 100, <code>GeoDa</code> uses a sample
            size of 80 + 4k.<a href="#fnref9" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn10">
          <p>The data set is available as one of the GeoDa Center sample data. Details about
            the computation of the principal components and other aspects of the data are given in <span
              class="citation">Anselin (<a href="#ref-Anselin:19b" role="doc-biblioref">2019</a>)</span>.<a
              href="#fnref10" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn11">
          <p>For details, see <span class="citation">Shi and Malik (<a href="#ref-ShiMalik:00"
                role="doc-biblioref">2000</a>)</span>, as well as the discussion of a “graph cut point of view” in <span
              class="citation">von Luxburg (<a href="#ref-vonLuxburg:07" role="doc-biblioref">2007</a>)</span>.<a
              href="#fnref11" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn12">
          <p><code>GeoDa</code> uses
            the <a href="https://spectralib.org">Spectra</a> C++ library for large scale eigenvalue problems. Note that
            the particular
            routines implemented in this library extract the <em>largest</em> eigenvalues/eigenvectors from the
            symmetric normalized
            affinity matrix, not the smallest from the graph Laplacian. The latter is the textbook explanation, but not
            always
            the most efficient implementation in practice. The equivalence between the two approaches is detailed in the
            <a href="#appendix">Appendix</a>.<a href="#fnref12" class="footnote-back">↩︎</a>
          </p>
        </li>
        <li id="fn13">
          <p>Alternative normalizations are used as well.
            For example, in the implementation in <a
              href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html">scikit-learn</a>,
            the eigenvectors are rescaled by the inverse square root of the eigenvalues.<a href="#fnref13"
              class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn14">
          <p>In <code>GeoDa</code>, the values for the number of nearest
            neighbors are <em>rounded up</em> to the nearest integer.<a href="#fnref14" class="footnote-back">↩︎</a></p>
        </li>
      </ol>
    </div>


  </section>


  <!-- code folding -->


  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>

</html>