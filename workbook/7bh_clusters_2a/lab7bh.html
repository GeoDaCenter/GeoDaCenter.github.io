<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Luc Anselin" />


<title>Cluster Analysis (2)</title>

<script src="lab7bh_files/header-attrs-2.3/header-attrs.js"></script>
<link href="lab7bh_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lab7bh_files/highlightjs-9.12.0/highlight.js"></script>
    <title>GeoDa on Github</title>

    <style>
    	*{margin:0;padding:0;}
	    .shadowfilter {
	       -webkit-filter: drop-shadow(12px 12px 7px rgba(0,0,0,0.5));
	        filter: url(shadow.svg#drop-shadow);
	     }
	     .intro1 { margin-left: -45px;}
    </style>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
    <style>
    ul {padding-left:30px;}
	figcaption {
	  top: .70em;
   	  left: .35em;
 	  bottom: auto!important;
	  right: auto!important;
	}
    </style>

        <style>
    h1 {
        text-align: center;
    }
    h3.subtitle {
        text-align: center;
    }
    h4.author {
        text-align: center;
    }
    h4.date {
        text-align: center;
    }
    p.caption {
        font-size : 12px;
    }
    </style>

<!-- Google Analytics -->
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-72724100-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<!-- End Google Analytics -->
<!-- Google Tag Manager -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-53RVF8"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-53RVF8');</script>
<!-- End Google Tag Manager -->

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








</head>

<body>


    <section class="page-header">
      <h1 class="project-name">GeoDa</h1>
      <h2 class="project-tagline">An Introduction to Spatial Data Analysis</h2>
      <a href="https://geodacenter.github.io/index.html" class="btn">Homepage</a>
      <a href="https://geodacenter.github.io/download.html" class="btn">Download</a>
      <a href="https://github.com/GeoDaCenter/geoda/" class="btn">View on GitHub</a>
      <a href="https://spatial.uchicago.edu/sample-data"  target="_blank" class="btn">Data</a>
       <a href="https://geodacenter.github.io/documentation.html" class="btn">Documentation</a>
       <a href="https://geodacenter.github.io/support.html" class="btn">Support</a>
       <a href="https://geodacenter.github.io/index-cn.html" class="btn">中文</a>
    </section>

    <section class="main-content">


<h1 class="title toc-ignore">Cluster Analysis (2)</h1>
<h3 class="subtitle">Hierarchical Clustering Methods</h3>
<h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
<h4 class="date">07/18/2020 (latest update)</h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#objectives">Objectives</a>
<ul>
<li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
</ul></li>
</ul></li>
<li><a href="#agglomerative-clustering">Agglomerative Clustering</a>
<ul>
<li><a href="#dissimilarity">Dissimilarity</a></li>
<li><a href="#illustration-single-linkage">Illustration: single linkage</a></li>
<li><a href="#dendrogram">Dendrogram</a></li>
<li><a href="#complete-linkage">Complete linkage</a></li>
<li><a href="#average-linkage">Average linkage</a></li>
<li><a href="#wards-method">Ward’s method</a></li>
</ul></li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#variable-settings-panel">Variable Settings Panel</a></li>
<li><a href="#cluster-results">Cluster results</a>
<ul>
<li><a href="#dendrogram-1">Dendrogram</a></li>
<li><a href="#cluster-map">Cluster map</a></li>
<li><a href="#cluster-summary">Cluster summary</a></li>
</ul></li>
</ul></li>
<li><a href="#linkage-method">Linkage method</a>
<ul>
<li><a href="#single-linkage">Single linkage</a></li>
<li><a href="#complete-linkage-1">Complete linkage</a></li>
<li><a href="#average-linkage-1">Average linkage</a></li>
</ul></li>
<li><a href="#options-and-sensitivity-analysis">Options and sensitivity analysis</a>
<ul>
<li><a href="#distance-metric">Distance metric</a></li>
</ul></li>
<li><a href="#appendix">Appendix</a>
<ul>
<li><a href="#single-linkage-worked-example">Single linkage worked example</a></li>
<li><a href="#complete-linkage-worked-example">Complete linkage worked example</a></li>
<li><a href="#average-linkage-worked-example">Average linkage worked example</a></li>
<li><a href="#wards-method-worked-example">Ward’s method worked example</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p><br></p>
<div id="introduction" class="section level2 unnumbered" number="">
<h2>Introduction</h2>
<p>In this second chapter on classical clustering methods, we cover hierarchical clustering. In contrast to partioning methods,
where the number of clusters (k) needs to be specified a priori, hierarchical clustering methods build up the clusters step by step.
The size of the cluster is determined from the results at each step that are visually represented in a tree structure,
the so-called <em>dendrogram</em>.</p>
<p>The successive steps can be approached in a top-down fashion or in a bottom-up fashion. The latter is referred to as <em>agglomerative</em> hierarchical
clustering, the former as <em>divisive</em> clustering. In this chapter, we will limit our attention to <em>agglomerative</em> methods. A form of divisive
clustering is implemented in some of the spatially constrained clustering techniques, which are discussed in a later chapter.</p>
<p>In agglomerative clustering, we start with each observation being assigned to its own cluster (i.e., <em>n</em> clusters
of one observation each).
Next, the two observations are found that are <em>closest</em> using a specified distance criterion,
and they are combined into a cluster. This process repeats itself, by using a <em>representative distance</em> for each grouping once multiple observations are combined. At the end of this process,
there is a single cluster that contains all the observations. The result of the successive groupings of the
observations is
visualized in a <em>dendrogram</em>. A <em>cut point</em> can be applied at any position in the tree to extract the full description of a cluster for all values of <span class="math inline">\(k\)</span>
between <span class="math inline">\(n\)</span> and 1.</p>
<p>Agglomerative clustering methods differ with respect to the way in which distances between observations
and clusters are computed. There are at least seven different ways to implement
this. Here, we will focus on the four most commonly used methods: <em>single linkage</em>, <em>complete linkage</em>,
<em>average linkage</em>, and <em>Ward’s method</em> (a special form of centroid linkage).</p>
<p>Hierarchical clustering techniques are covered in detail in Chapter 4 of <span class="citation">Everitt et al. (<a href="#ref-Everittetal:11" role="doc-biblioref">2011</a>)</span> and in
Chapter 5 of <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>. A discussion of fast modern algorithms is contained
in <span class="citation">Müllner (<a href="#ref-Mullner:11" role="doc-biblioref">2011</a>)</span> and <span class="citation">Müllner (<a href="#ref-Mullner:13" role="doc-biblioref">2013</a>)</span>.</p>
<p>To illustrate these methods, we will continue to use the Guerry data set on
moral statistics in 1830 France, which comes pre-installed with GeoDa.</p>
<div id="objectives" class="section level3 unnumbered" number="">
<h3>Objectives</h3>
<ul>
<li><p>Understand the principles behind agglomerative clustering</p></li>
<li><p>Distinguish between different linkage functions</p></li>
<li><p>Understand the implications of using different linkage functions</p></li>
<li><p>Interpret a dendrogram for hierarchical clustering</p></li>
<li><p>Carry out sensitivity analysis</p></li>
</ul>
<div id="geoda-functions-covered" class="section level4 unnumbered" number="">
<h4>GeoDa functions covered</h4>
<ul>
<li>Clusters &gt; Hierarchical
<ul>
<li>select cut point</li>
<li>select linkage function</li>
<li>select standardization option</li>
<li>select distance metric</li>
<li>cluster characteristics</li>
<li>mapping the clusters</li>
<li>saving the cluster classification</li>
</ul></li>
</ul>
<p><br></p>
</div>
</div>
</div>
<div id="agglomerative-clustering" class="section level2 unnumbered" number="">
<h2>Agglomerative Clustering</h2>
<div id="dissimilarity" class="section level3 unnumbered" number="">
<h3>Dissimilarity</h3>
<p>An agglomerative clustering algorithm starts with each observation serving
as its own cluster, i.e., beginning with <span class="math inline">\(n\)</span> clusters of size 1. Next, the algorithm moves through a sequence
of steps, where each time the number of clusters is decreased by one, either by creating
a new cluster from two observations, or by assigning an observation to an existing cluster, or
by merging two clusters. Such algorithms are sometimes referred to as SAHN, which stands
for sequential, agglomerative, hierarchic and non-overlapping <span class="citation">(Müllner <a href="#ref-Mullner:11" role="doc-biblioref">2011</a>)</span>.</p>
<p>The smallest within-group sum of squares is obtained in the initial
stage, where each observation is its own cluster. As soon as two observations are grouped, the within sum of
squares increases. Hence, each time a new merger is carried out, the overall objective of minimizing
the within sum of squares deteriorates. At the final stage, when all observations are joined into a
single cluster, the total within sum of squares equals the total sum of squares. So, whereas partioning
cluster methods aim to improve the objective function at each step, agglomerative hierarchical
clustering moves in the opposite direction.</p>
<p>The key element in this process is how the dissimilarity (or distance) between two clusters is computed, the so-called
<em>linkage</em>. In this chapter, we consider four common types. We discuss single linkage here to illustrate
the main concepts and cover the other
methods separately below.</p>
<p>For <em>single linkage</em>, the relevant dissimilarity is between the two points in each cluster that are
closest together. More precisely, the dissimilarity between clusters <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is:
<span class="math display">\[d_{AB} = \mbox{min}_{i \in A,j \in B} d_{ij},\]</span>
i.e., the <em>nearest neighbor</em> distance between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>A second important concept is how the distances between other points (or clusters) and a newly
merged cluster are computed, the so-called <em>updating formula</em>. With some clever algebra, it can
be shown that these calculations can be based on the dissimilarity matrix from the previous step and do not
require going back to the original <span class="math inline">\(n \times n\)</span> dissimilarity matrix.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Note that at each step, the
dimension of the relevant dissimilarity matrix decreases by one, which allows for very memory-efficient
algorithms.</p>
<p>In the single linkage case, the updating formula takes a simple form. The dissimilarity between a point (or cluster) <span class="math inline">\(P\)</span>
and a cluster <span class="math inline">\(C\)</span> that was obtained by merging <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is the smallest of the dissimilarities
between <span class="math inline">\(P\)</span> and <span class="math inline">\(A\)</span> and <span class="math inline">\(P\)</span> and <span class="math inline">\(B\)</span>:
<span class="math display">\[d_{PC} = \mbox{min}(d_{PA},d_{PB}),\]</span>
using the definition of dissimilarity between two objects (<span class="math inline">\(d_{AB}\)</span>) given above.</p>
<p>The minimum condition can also be obtained as the result of an algebraic
expression:
<span class="math display">\[d_{PC} = (1/2) (d_{PA} + d_{PB}) - (1/2)| d_{PA} - d_{PB} |,\]</span>
in the same notation as before.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>The updating formula only affects the row/column in the dissimilarity matrix that pertains to
the newly merged cluster. The other elements of the dissimilarity matrix remain unchanged.</p>
<p>Single linkage clusters tend to result in a few clusters consisting of long drawn out chains of observations as well as several singletons (observations
that form their own cluster).
This is due to the fact that sometimes disparate clusters are joined when they have two close points, but otherwise
are far apart. Single linkage is sometimes used to detect outliers, i.e., observations that remain singletons and
thus are far apart from all others.</p>
</div>
<div id="illustration-single-linkage" class="section level3 unnumbered" number="">
<h3>Illustration: single linkage</h3>
<p>To illustrate the logic of agglomerative hierarchical clustering algorithms, we use the example of
<em>single linkage</em> applied to the same seven points as we used for k-means. For ease of reference,
the points are shown in Figure <a href="#fig:slinkexample">1</a>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The point IDs (not shown) are ordered with increasing values of X first, then Y, starting
with observation 1 in the lower left corner. Full details are given in the <a href="#appendix">Appendix</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:slinkexample"></span>
<img src="pics7b/01_points.png" alt="Single link hierarchical clustering toy example" width="35%" />
<p class="caption">
Figure 1: Single link hierarchical clustering toy example
</p>
</div>
<p>The basis for any agglomerative clustering method is a <span class="math inline">\(n \times n\)</span> symmetric dissimilarity matrix. Except for
Ward’s method,
this is the only information needed.
In our example, we use a dissimilarity matrix based on the Euclidean distances between the points, as shown in Figure <a href="#fig:slinkdist">2</a></p>
<div class="figure" style="text-align: center"><span id="fig:slinkdist"></span>
<img src="pics7b/01_distance_matrix.png" alt="Dissimilarity matrix" width="80%" />
<p class="caption">
Figure 2: Dissimilarity matrix
</p>
</div>
<p>The first step is to identify the pair of observations that have the smallest nearest neighbor distance.
From the distance matrix and also from Figure <a href="#fig:slinknn">3</a> this is the pair 4-5 (<span class="math inline">\(d_{4,5}=1.0\)</span>). This forms the
first cluster, highlighted in dark blue in the figure. The five other observations remain in their own
cluster. In other words, at this stage, we still have six clusters.</p>
<div class="figure" style="text-align: center"><span id="fig:slinknn"></span>
<img src="pics7b/02_points1.png" alt="Nearest neighbors" width="35%" />
<p class="caption">
Figure 3: Nearest neighbors
</p>
</div>
<p>The algorithm then moves sequentially to identify the nearest neighbor at each step, merge the
relevant observations/clusters and so decrease the number of clusters by one. The steps are illustrated in the
panels of Figure <a href="#fig:slinksteps">4</a>, going from left to right and starting at the top. In the second step, another
observation (7) is added to the 4,5 cluster. Next, a new cluster emerges consisting of 1 and 2 (the two green
points in the lower left). Finally, first 3 and then
6 are added to the large blue cluster. Details are given in the <a href="#appendix">Appendix</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:slinksteps"></span>
<img src="pics7b/02_allsteps.png" alt="Single linkage iterations" width="70%" />
<p class="caption">
Figure 4: Single linkage iterations
</p>
</div>
</div>
<div id="dendrogram" class="section level3 unnumbered" number="">
<h3>Dendrogram</h3>
<p>The agglomerative <em>nesting</em> is visually represented in a tree structure, the so-called <em>dendrogram</em>. For each step,
the graph shows which observations/clusters are combined. In addition, the different horizontal
extents (i.e., how far each cluster combination is from the right side of the graph) give
a sense of the degree of change in
the objective function achieved by each merger.</p>
<p>The implementation of the dendrogram in <code>GeoDa</code> is currently somewhat limited, but it accomplishes the
main goal. In Figure <a href="#fig:slinkdendro">5</a>, the result is shown for single linkage in our
toy example. The graph shows how the cluster starts by combining two observations (4 and 5), to which then
a third (7) is added. These first two steps are highlighted in the dendrogram and the corresponding
observations are selected in the Table.</p>
<p>Next, following the tree structure reveals how two more observations (1 and 2) are combined into a separate cluster, and two
observations (3 and 6) are added to the original cluster of 4,5 and 7. The fact that the last three operations all
line up (same distance from the right side) follows from a three-way <em>tie</em> in the inter-group distances (see the <a href="#appendix">Appendix</a>).
As a result, the change in the objective function (more precisely, a deterioration) that follows from adding the points
to a cluster is the same in each case.</p>
<p>The dashed vertical line represents a <em>cut</em> line. It corresponds with a particular value of k for which the
make up of the clusters and their characteristics can be further investigated
(see the section on <a href="#cluster-results">Cluster results</a> for details on the implementation in <code>GeoDa</code>).</p>
<div class="figure" style="text-align: center"><span id="fig:slinkdendro"></span>
<img src="pics7b/01_dendrogram.png" alt="Single linkage dendrogram" width="60%" />
<p class="caption">
Figure 5: Single linkage dendrogram
</p>
</div>
</div>
<div id="complete-linkage" class="section level3 unnumbered" number="">
<h3>Complete linkage</h3>
<p><em>Complete linkage</em> is the opposite of single linkage in that the dissimilarity between two
clusters is defined as the furthest neighbors, i.e., the pair of points, one from
each cluster, that are separated by the greatest dissimilarity. For the dissimilarity
between clusters <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, this boils down to:
<span class="math display">\[d_{AB} = \mbox{max}_{i \in A,j \in B} d_{ij}.\]</span></p>
<p>With this measure of dissimilarity in hand, we still continue to combine the two observations/clusters
that are closest at each step. The only difference with single linkage is how the respective
distances are calculated.</p>
<p>The updating formula similarly is the opposite of the one for single linkage.
The dissimilarity between a point (or cluster) <span class="math inline">\(P\)</span>
and a cluster <span class="math inline">\(C\)</span> that was obtained by merging <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is the largest of the dissimilarities
between <span class="math inline">\(P\)</span> and <span class="math inline">\(A\)</span> and <span class="math inline">\(P\)</span> and <span class="math inline">\(B\)</span>:
<span class="math display">\[d_{PC} = \mbox{max}(d_{PA},d_{PB}).\]</span></p>
<p>The algebraic counterpart of the updating formula is:
<span class="math display">\[d_{PC} = (1/2) (d_{PA} + d_{PB}) + (1/2)| d_{PA} - d_{PB} |,\]</span>
using the same logic as in the single linkage case.</p>
<p>A detailed worked example for complete linkage using our toy data is given in the <a href="#appendix">Appendix</a>.
A summary is provided in the dendrogram in Figure <a href="#fig:compdendro">6</a>. This shows the first
cluster as observations 4,5. The next steps are to combine 1 and 2 (at the bottom) and 6 and 7
(at the top). In the last two steps, 3 is added to 1,2 and 4,5 and 6,7 are merged (see the
<a href="#appendix">Appendix</a> for details).</p>
<div class="figure" style="text-align: center"><span id="fig:compdendro"></span>
<img src="pics7b/11_completelink_dendrogram.png" alt="Complete linkage dendrogram" width="60%" />
<p class="caption">
Figure 6: Complete linkage dendrogram
</p>
</div>
<p>In contrast to single linkage, complete linkage tends to result in a large number of well-balanced compact clusters.
Instead of merging fairly disparate clusters that have (only) two close points, it can have the
opposite effect of keeping similar observations in separate clusters.</p>
</div>
<div id="average-linkage" class="section level3 unnumbered" number="">
<h3>Average linkage</h3>
<p>In <em>average linkage</em>, the dissimilarity between two clusters is the average of all pairwise
dissimilarities between observations <span class="math inline">\(i\)</span> in cluster <span class="math inline">\(A\)</span> and <span class="math inline">\(j\)</span> in cluster <span class="math inline">\(B\)</span>. There are
<span class="math inline">\(n_A.n_B\)</span> such pairs (only counting each pair once), with <span class="math inline">\(n\)</span> as the number of observations
in each cluster. Consequently, the dissimilarity
between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is:
<span class="math display">\[d_{AB} = \frac{\sum_{i \in A} \sum_{j \in B} d_{ij}}{n_A.n_B}.\]</span>
Note that when merging two single observations, <span class="math inline">\(d_{AB}\)</span> is simply the dissimilarity between the two,
since <span class="math inline">\(n_A = n_B = 1\)</span> and the denominator in the expression is 1.</p>
<p>The updating formula to compute the dissimilarity between a point (or cluster) <span class="math inline">\(P\)</span> and the new
cluster <span class="math inline">\(C\)</span> formed by merging <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is the weighted average of the dissimilarities
<span class="math inline">\(d_{PA}\)</span> and <span class="math inline">\(d_{PB}\)</span>:
<span class="math display">\[d_{PC} = \frac{n_A}{n_A + n_B} d_{PA} + \frac{n_B}{n_A + n_B} d_{PB}.\]</span>
As before, the other distances are not affected.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>A detailed worked example for complete linkage using our toy data is given in the <a href="#appendix">Appendix</a>.
A summary is provided in the dendrogram in Figure <a href="#fig:avgdendro">7</a>. This shows the first
cluster again as observations 4,5. As in the case of complete linkage, the next steps are to combine 1 and 2 (at the bottom) and 6 and 7
(at the top). In the last two steps, first 4,5 and 6,7 are merged and then 3 is added to 1,2, in the reverse
order of what happened for complete linkage (see the
<a href="#appendix">Appendix</a> for details).</p>
<div class="figure" style="text-align: center"><span id="fig:avgdendro"></span>
<img src="pics7b/44_avg_dendrogram.png" alt="Average linkage dendrogram" width="60%" />
<p class="caption">
Figure 7: Average linkage dendrogram
</p>
</div>
<p>Average linkage can be viewed as a compromise between the nearest neighbor logic of single linkage
and the furthest neighbor logic of complete linkage. In our example, the end results are almost the same
as for complete linkage. However, in general, average linkage tends to produce a few large clusters
and many singletons, similar to the results for single linkage (see also the results for the
Guerry data shown below).</p>
</div>
<div id="wards-method" class="section level3 unnumbered" number="">
<h3>Ward’s method</h3>
<p>The three linkage methods discussed so far only make use of a dissimilarity matrix. How this matrix is
obtained does not matter. As a result, dissimilarity may be defined using Euclidean or Manhattan distance,
dissimilarity among categories, or even directly from interview or survey data.</p>
<p>In contrast, the method developed by <span class="citation">Ward (<a href="#ref-Ward:63" role="doc-biblioref">1963</a>)</span> is based on
a sum of squared errors rationale that only works for Euclidean distance between observations. In addition,
the sum of squared errors requires the consideration of the so-called <em>centroid</em> of each cluster, i.e., the
mean vector of the observations belonging to the cluster. Therefore, the input into Ward’s method is a
<span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(X\)</span> of actual observations on <span class="math inline">\(p\)</span> variables (as before, this is typically standardized
in some fashion).</p>
<p>Ward’s method is based on the objective of minimizing the deterioration in the overall within sum of squares.
The latter is the sum of squared deviations between the observations in a cluster and the centroid (mean),
as we have seen before in k-means clustering:
<span class="math display">\[WSS = \sum_{i \in C} (x_i - \bar{x}_C)^2,\]</span>
with <span class="math inline">\(\bar{x}_C\)</span> as the centroid of cluster <span class="math inline">\(C\)</span>.</p>
<p>Since any merger of two existing clusters (including individual observations) results in a worsening of the overall
WSS, Ward’s method is designed to minimize this deterioration. More specifically, it is designed to minimize the
difference between the new (larger) WSS in the merged cluster and the sum of the WSS of the components that were merged.
This turns out to boil down to minimizing the distance between cluster centers.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> This is reminiscent of the equivalence between Euclidean distances and sum of squared errors we saw in the
discussion of k-means. As in k-means, we work with the square of the Euclidean distance:
<span class="math display">\[d_{AB}^2 = \frac{2n_A n_B}{n_A + n_B} ||\bar{x}_A - \bar{x}_B ||^2,\]</span>
where <span class="math inline">\(||\bar{x}_A - \bar{x}_B ||\)</span> is the Euclidean distance between the two cluster centers (squared in the distance
squared expression).<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>The hierarchical nesting steps proceed in the same way as for the other methods, by merging the two observations/clusters that
are the closest, but now based on a more complex distance
metric.</p>
<p>The update equation to compute the (squared) distance from an observation (or cluster) <span class="math inline">\(P\)</span> to a new cluster <span class="math inline">\(C\)</span> obtained
from the merger of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is more complex than for the other linkage options:
<span class="math display">\[d^2_{PC} = \frac{n_A + n_P}{n_C + n_P} d^2_{PA} + \frac{n_B + n_P}{n_C + n_P} d^2_{PB} - \frac{n_P}{n_C + n_P} d^2_{AB},\]</span>
in the same notation as before. However, it can still readily be obtained from the information contained
in the dissimilarity matrix from the previous step, and it does not involve the actual computation of centroids.</p>
<p>To see why this is the case, consider the usual first step when two single observations are merged.
The distance squared between them is simply the Euclidean distance squared between their values, not involving
any centroids. The updated squared distances between other points and the two merged points only involve the point-to-point
squared distances <span class="math inline">\(d^2_{PA}\)</span>, <span class="math inline">\(d^2_{PB}\)</span> and <span class="math inline">\(d^2_{AB}\)</span>, no centroids. From then on, any update uses the results from the
previous distance matrix in the update equation.</p>
<p>A detailed worked example for Ward’s method using our toy data is given in the <a href="#appendix">Appendix</a>.
A summary is provided in the dendrogram in Figure <a href="#fig:warddendro">8</a>. This shows the first
cluster again as observations 4,5. As in the case of complete linkage, the next steps are to combine 1 and 2 (at the bottom of the graph) and 6 and 7
(at the top of the graph). In the last two steps, as in complete linkage, 3 is first added to 1,2 and then 4,5 and 6,7 are merged (see the
<a href="#appendix">Appendix</a> for details).</p>
<div class="figure" style="text-align: center"><span id="fig:warddendro"></span>
<img src="pics7b/55_ward_dendrogram.png" alt="Ward linkage dendrogram" width="60%" />
<p class="caption">
Figure 8: Ward linkage dendrogram
</p>
</div>
<p>Ward’s method tends to result in nicely balanced clusters. It is therefore set as the default
method in <code>GeoDa</code>.</p>
</div>
</div>
<div id="implementation" class="section level2 unnumbered" number="">
<h2>Implementation</h2>
<p>Hierarchical clustering is invoked in <code>GeoDa</code> from the same toolbar icon as the other clustering methods. It is the last item in the
classic methods subset, as shown in Figure <a href="#fig:hierarchicalmenu">9</a>. It can also be selected from the menu as <strong>Clusters &gt; Hierarchical</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:hierarchicalmenu"></span>
<img src="pics7b/4_035_hierarchical.png" alt="Hierarchical clustering option" width="10%" />
<p class="caption">
Figure 9: Hierarchical clustering option
</p>
</div>
<div id="variable-settings-panel" class="section level3 unnumbered" number="">
<h3>Variable Settings Panel</h3>
<p>As before, the variables to be clustered are selected in the <strong>Variables Settings</strong> panel. We
continue with the same six variables as for k-means, shown in Figure <a href="#fig:hiervars">10</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:hiervars"></span>
<img src="pics7b/5_048_hiervars.png" alt="Hierarchical clustering variable selection" width="35%" />
<p class="caption">
Figure 10: Hierarchical clustering variable selection
</p>
</div>
<p>The panel also allows one to set the usual options, such as the <strong>Transformation</strong> (default value
is a standardized z-value), the linkage <strong>Method</strong> (default is <strong>Ward’s linkage</strong>), and the
<strong>Distance Function</strong> (default is <strong>Euclidean</strong>). In the same way as for k-means, the cluster
classification is saved in the data table under the variable name specified in
<strong>Save Cluster in Field</strong>.</p>
</div>
<div id="cluster-results" class="section level3 unnumbered" number="">
<h3>Cluster results</h3>
<p>The actual computation of the clusters proceeds in two steps. In the first step,
a click on <strong>Run</strong> yields a dendrogram in the panel on the right. A cut point can be selected
interactively, of by setting a value the number of clusters in the panel. After this,
<strong>Save/Show Map</strong> creates the cluster map, computes the summary characteristics, and saves the
cluster classification in the data table.</p>
<div id="dendrogram-1" class="section level4 unnumbered" number="">
<h4>Dendrogram</h4>
<p>With all options set to the default, the resulting dendrogram is as in Figure <a href="#fig:dendrogram5">11</a>.
The dashed red line corresponds to a cut point that yields five clusters, to keep the results comparable
with what we obtained for k-means. The
dendrogram shows how individual observations are combined into groups of two, and subsequently
into larger and larger groups, by merging pairs of clusters. The colors on the right hand
side match the colors of the observations in the cluster map (see next). A selection rectangle allows
one to select specific groups of observations in the dendrogram. Through linking, the matching
observations are identified in the cluster map (and in any graph or map currently open).</p>
<div class="figure" style="text-align: center"><span id="fig:dendrogram5"></span>
<img src="pics7b/5_049_dendrogram5.png" alt="Dendrogram (k=5)" width="40%" />
<p class="caption">
Figure 11: Dendrogram (k=5)
</p>
</div>
<p>The dashed line (cut point) can be moved interactively. For example, in Figure <a href="#fig:dendrogram8">12</a>,
we <em>grabbed</em> the line at the top (it can equally be grabbed at the bottom), and moved
it to the right to yield eight clusters. The corresponding colors are shown on the
right hand bar.</p>
<div class="figure" style="text-align: center"><span id="fig:dendrogram8"></span>
<img src="pics7b/5_050_dendrogram8.png" alt="Dendrogram (k=8)" width="40%" />
<p class="caption">
Figure 12: Dendrogram (k=8)
</p>
</div>
</div>
<div id="cluster-map" class="section level4 unnumbered" number="">
<h4>Cluster map</h4>
<p>As mentioned before, once the dendrogram cut point is specified, clicking on <strong>Save/Show Map</strong>
will generate the cluster map, shown in Figure <a href="#fig:hiermapW5">13</a>. Note how the colors for
the map categories match the colors in the dendrogram. Also, the number of observations
in each class also are the same between the groupings in the dendrogram and the cluster map.</p>
<p>The default Ward’s method yields fairly balanced clusters with two groups with 13 observations, one
with 25, and two more with 17. The results show some similarity with the outcome of k-means for k=5,
although there are some important differences as well.</p>
<div class="figure" style="text-align: center"><span id="fig:hiermapW5"></span>
<img src="pics7b/5_051_hierclusmapW5.png" alt="Hierarchical cluster map (Ward, k=5)" width="60%" />
<p class="caption">
Figure 13: Hierarchical cluster map (Ward, k=5)
</p>
</div>
</div>
<div id="cluster-summary" class="section level4 unnumbered" number="">
<h4>Cluster summary</h4>
<p>Similarly, once <strong>Save/Show Map</strong> has been selected, the cluster descriptive statistics
become available from the <strong>Summary</strong> button in the dialog. The same characteristics are reported
as for k-means. In comparison to our k-means solution, this set of clusters is slightly
inferior in terms of the ratio of between to total sum of squares, achieving 0.482044 (compared
to 0.497). Clusters 4 and 5 achieve much better (smaller) within sum of squares than in the
k-means solution, but the other values are worse.</p>
<p>Setting the number of clusters at five is by no means necessarily the best solution. In a
real application of hierarchical clustering, one would experiment with different cut points and evaluate the solutions
relative to the k-means solution.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="pics7b/5_052_hierW5summary.png" alt="Hierarchical cluster characteristics (Ward, k=5" width="45%" />
<p class="caption">
Figure 14: Hierarchical cluster characteristics (Ward, k=5
</p>
</div>
<p>The two clustering approaches can also be used in conjunction with each other. For example, one could
explore the dendrogram to find a good cut-point, and then use this value for k in a k-means
or other partitioning method.</p>
</div>
</div>
</div>
<div id="linkage-method" class="section level2 unnumbered" number="">
<h2>Linkage method</h2>
<p>The main option of interest in hierarchical clustering is the linkage <strong>Method</strong>.
So far, we have used the default setting for <strong>Ward’s-linkage</strong>. We now consider each of the other linkage options in turn and illustrate the associated dendrogram, cluster map and cluster
characteristics.</p>
<div id="single-linkage" class="section level3 unnumbered" number="">
<h3>Single linkage</h3>
<p>The linkage options are chosen from the <strong>Method</strong> item in the dialog. For example,
in Figure <a href="#fig:singlelinkagemethod">15</a>, we select <strong>Single-linkage</strong>. The other
options are chosen in the same way.</p>
<div class="figure" style="text-align: center"><span id="fig:singlelinkagemethod"></span>
<img src="pics7b/5_053_single_linkage.png" alt="Single linkage" width="30%" />
<p class="caption">
Figure 15: Single linkage
</p>
</div>
<p>The cluster results for single linkage are typically characterized by one or a few very
large clusters and several singletons (one observation per cluster). This characteristic
is confirmed in our example,
with the dendrogram in Figure <a href="#fig:dendrosingle5">16</a>, and the
corresponding cluster map in Figure <a href="#fig:single5map">17</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:dendrosingle5"></span>
<img src="pics7b/5_054_dendrogram_single.png" alt="Dendrogram single linkage (k=5)" width="40%" />
<p class="caption">
Figure 16: Dendrogram single linkage (k=5)
</p>
</div>
<p>Four <em>clusters</em> consist of a
single observation, with the main cluster collecting the 81 other observations. This
situation is not remedied by moving the cut point such that more clusters result, since
almost all of the additional clusters are singletons as well.</p>
<div class="figure" style="text-align: center"><span id="fig:single5map"></span>
<img src="pics7b/5_055_single_map.png" alt="Hierarchical cluster map (single linkage, k=5)" width="60%" />
<p class="caption">
Figure 17: Hierarchical cluster map (single linkage, k=5)
</p>
</div>
<p>The characteristics of the single linkage hierarchical cluster are similarly dismal. Since
four <em>clusters</em> are singletons, their within cluster sum of squares is <strong>0</strong>. Hence,
the total within-cluster sum of squares equals the sum of squares for cluster 5.
The resulting ratio of between to total sum of squares is only 0.214771.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="pics7b/5_056_single_summary5.png" alt="Hierarchical cluster characteristics (single linkage, k=5)" width="45%" />
<p class="caption">
Figure 18: Hierarchical cluster characteristics (single linkage, k=5)
</p>
</div>
<p>In practice, in most situations, single linkage will not be a good choice, unless the
objective is to identify a lot of singletons and characterize these as <em>outliers</em>.</p>
</div>
<div id="complete-linkage-1" class="section level3 unnumbered" number="">
<h3>Complete linkage</h3>
<p>The complete linkage method yields clusters that are similar in balance to Ward’s method.
For example, in Figure <a href="#fig:dendrocomplete5">19</a>, the dendrogram is shown for our example,
using a cut point with five clusters. The clusters are fairly balanced, unlike what we
just saw for single linkage.</p>
<div class="figure" style="text-align: center"><span id="fig:dendrocomplete5"></span>
<img src="pics7b/5_057_dendro_complete5.png" alt="Dendrogram complete linkage (k=5)" width="40%" />
<p class="caption">
Figure 19: Dendrogram complete linkage (k=5)
</p>
</div>
<p>The corresponding cluster map is given as
Figure <a href="#fig:completemap">20</a>. The map is similar in structure to that obtained with Ward’s
method (Figure <a href="#fig:hiermapW5">13</a>), but note that the largest category (at 39) is much larger
than the largest for Ward (25).</p>
<div class="figure" style="text-align: center"><span id="fig:completemap"></span>
<img src="pics7b/5_058_complete_map.png" alt="Hierarchical cluster map (complete linkage, k=5)" width="60%" />
<p class="caption">
Figure 20: Hierarchical cluster map (complete linkage, k=5)
</p>
</div>
<p>In terms of the cluster characteristics, shown in Figure <a href="#fig:completesummary">21</a>, we note
a slight deterioration relative to Ward’s results, with the ratio of between to total sum
of squares at 0.423101 (but much better than single linkage).</p>
<div class="figure" style="text-align: center"><span id="fig:completesummary"></span>
<img src="pics7b/5_059_complete_summary.png" alt="Hierarchical cluster characteristics (complete linkage, k=5)" width="45%" />
<p class="caption">
Figure 21: Hierarchical cluster characteristics (complete linkage, k=5)
</p>
</div>
</div>
<div id="average-linkage-1" class="section level3 unnumbered" number="">
<h3>Average linkage</h3>
<p>Finally, the average linkage criterion suffers from some of the same problems as single
linkage, although it yields slightly better results. The dendrogram in Figures <a href="#fig:dendroavg5">22</a>
indicates a similar unbalanced structure, although only with two singletons. The other small
clusters consist of two and five observations.</p>
<div class="figure" style="text-align: center"><span id="fig:dendroavg5"></span>
<img src="pics7b/5_060_dendro_average.png" alt="Dendrogram average linkage (k=5)" width="40%" />
<p class="caption">
Figure 22: Dendrogram average linkage (k=5)
</p>
</div>
<p>The cluster map
is given as Figure <a href="#fig:avgmap">23</a>. It reinforces that the average linkage criterion is not conducive
to discovering compact clusters, but rather lumps most of the observations into one large cluster,
with a few outliers.</p>
<div class="figure" style="text-align: center"><span id="fig:avgmap"></span>
<img src="pics7b/5_061_average_map.png" alt="Hierarchical cluster map (average linkage, k=5)" width="60%" />
<p class="caption">
Figure 23: Hierarchical cluster map (average linkage, k=5)
</p>
</div>
<p>As given in Figure <a href="#fig:avgsummary">24</a>, the summary characteristics are slighly better
than in the single linkage case, with only two singletons. However, the overall ratio of
between to total sum of squares is still much worse than for the other two methods,
at 0.296838.</p>
<div class="figure" style="text-align: center"><span id="fig:avgsummary"></span>
<img src="pics7b/5_062_average_summary.png" alt="Hierarchical cluster characteristics (average linkage, k=5)" width="45%" />
<p class="caption">
Figure 24: Hierarchical cluster characteristics (average linkage, k=5)
</p>
</div>
</div>
</div>
<div id="options-and-sensitivity-analysis" class="section level2 unnumbered" number="">
<h2>Options and sensitivity analysis</h2>
<p>The options are essentially the same for all clustering methods. We can
change the <strong>Transformation</strong> and select a different <strong>Distance Function</strong>. We briefly consider
the latter. However, this option is not appropriate for Ward’s method, which only
applies to Euclidean distances.</p>
<p>The option to include a minimum bound is not appropriate for agglomerative clustering,
since it would preclude any individual observations to be merged. None would satisfy
the minimum bound, unless it was set at a level that was meaningless as a constraint.</p>
<p>Conditional plots, aggregation and dissolution of the cluster results operate in exactly the same way
as for k-means clustering and is not covered here.</p>
<div id="distance-metric" class="section level3 unnumbered" number="">
<h3>Distance metric</h3>
<p>The default metric behind the definition of dissimilarity is the
Euclidean distance between observations. In some contexts, it may be preferable to use absolute or Manhattan
block distance, which penalizes larger distances less. This option can be selected
through the <strong>Distance Function</strong> item in the dialog, as in Figure <a href="#fig:kmeansdist">25</a>,
where we choose it for a <strong>Complete Linkage</strong> clustering (the next best method after
Ward’s).</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansdist"></span>
<img src="pics7b/6_065_hier_manhattan.png" alt="Manhattan distance metric" width="30%" />
<p class="caption">
Figure 25: Manhattan distance metric
</p>
</div>
<p>The cluster map for a cut point of k=5 is shown in Figure <a href="#fig:mandistmap">26</a>. Compared to the
map for Euclidean distance in Figure <a href="#fig:completemap">20</a>, some of the configurations are
quite different. Two of the clusters are much smaller, but the largest ones are well balanced.</p>
<div class="figure" style="text-align: center"><span id="fig:mandistmap"></span>
<img src="pics7b/6_066_h_man_map.png" alt="Manhattan distance cluster map" width="60%" />
<p class="caption">
Figure 26: Manhattan distance cluster map
</p>
</div>
<p>The summary
characteristics given in Figure <a href="#fig:mandistsummary">27</a>. Relative to the Euclidean distance results, some the within
sum or squares are much larger, although the largest is not as large as in the Euclidean case, and the smallest is smaller.
Also, the ratio of between to total sum of squares
is somewhat worse for Manhattan distance, at 0.412 (compared to 0.423). However, this is not a totally fair comparison, since the
criterion for grouping is not based on a sum of squared deviations.</p>
<div class="figure" style="text-align: center"><span id="fig:mandistsummary"></span>
<img src="pics7b/6_067_h_man_summary.png" alt="Manhattan distance cluster characteristics" width="45%" />
<p class="caption">
Figure 27: Manhattan distance cluster characteristics
</p>
</div>
<p>As in the discussion of k-means, it cannot be emphasized enough that the interpretation and choice of the proper cluster is both an art and a
science. Considerable experimentation is needed to get good insight into the characteristics of the data and what
groupings make the most sense.</p>
</div>
</div>
<div id="appendix" class="section level2 unnumbered" number="">
<h2>Appendix</h2>
<div id="single-linkage-worked-example" class="section level3 unnumbered" number="">
<h3>Single linkage worked example</h3>
<p>We now illustrate each step in detail as the <em>single linkage</em> hierarchical cluster algorithm processes
our toy example. For ease of reference, the point coordinates (the same as for the k-means example) are listed again
in Figure <a href="#fig:slinkex1">28</a>. The associated dissimilarity matrix, based on the Euclidean distance
between the points (not the squared distance) is given in Figure <a href="#fig:slinkdist">2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:slinkex1"></span>
<img src="pics7b/01_coordinates.png" alt="Worked example - basic data" width="35%" />
<p class="caption">
Figure 28: Worked example - basic data
</p>
</div>
<p>The first step consists of identifying the two observations that are the <em>closest</em>. From the dissimilarity
matrix, we find the shortest dissimilarity to be a value of 1.0 between 4 and 5, as highlighted in
Figure <a href="#fig:slinkstep1">29</a>. As a result, 4 and 5 form the first cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:slinkstep1"></span>
<img src="pics7b/01_slink_1.png" alt="Single Linkage - Step 1" width="80%" />
<p class="caption">
Figure 29: Single Linkage - Step 1
</p>
</div>
<p>We update the dissimilarity matrix using the smallest dissimilarity between each observation and either 4 or 5 as
the entry for the combined unit 4,5. More precisely, the dissimilarity used between the cluster and the
other observations
varies depending on whether 4 or 5 is closest to the other observations. For example, in
Figure <a href="#fig:slinkstep2">30</a>, we list the dissimilarity between 4,5 and 1 as 5.0, wich is the smallest
of 1-4 (5.0) and 1-5 (5.83). The dissimilarities between the pairs of observations that do not involve 4,5 are not affected.</p>
<p>We update the other entries for 4,5 in the same way and again locate the smallest
dissimilarity in the matrix. This time, it is a dissimilarity of 2.0 between 4,5 and 7 (more precisely, between
5 and 7). Consequently, observation 7 is added to the 4,5 cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:slinkstep2"></span>
<img src="pics7b/01_slink_2.png" alt="Single Linkage - Step 2" width="72%" />
<p class="caption">
Figure 30: Single Linkage - Step 2
</p>
</div>
<p>The dissimilarities between 4,5,7 and the other points are updated in Figure <a href="#fig:slinkstep3">31</a>.
But now we encounter a problem. There is a three-way tie in terms of the smallest value:
1-2, 4,5,7-3 and 4,5,7-6 all have a dissimilarity of 2.24, but only one can be picked to update
the clusters. Ties can be handled by choosing one grouping
at random. The algorithm behind the
<strong>fastcluster</strong> implementation that is used by <code>GeoDa</code> selects the pair 1-2.</p>
<div class="figure" style="text-align: center"><span id="fig:slinkstep3"></span>
<img src="pics7b/01_slink_3.png" alt="Single Linkage - Step 3" width="62%" />
<p class="caption">
Figure 31: Single Linkage - Step 3
</p>
</div>
<p>With the distances updated, we - not unsurprisingly - again find 2.24 as the shortest dissimilarity,
tied for two pairs (in Figure <a href="#fig:slinkstep4">32</a>). This time the algorithm adds 3 to the
existing cluster 4,5,7.</p>
<div class="figure" style="text-align: center"><span id="fig:slinkstep4"></span>
<img src="pics7b/01_slink_4.png" alt="Single Linkage - Step 4" width="52%" />
<p class="caption">
Figure 32: Single Linkage - Step 4
</p>
</div>
<p>Finally, observation 6 is added to cluster 4,5,7,3 again for a dissimilarity of 2.24
(in Figure <a href="#fig:slinkstep5">33</a>)</p>
<div class="figure" style="text-align: center"><span id="fig:slinkstep5"></span>
<img src="pics7b/01_slink_5.png" alt="Single Linkage - Step 5" width="42%" />
<p class="caption">
Figure 33: Single Linkage - Step 5
</p>
</div>
<p>The end result is to merge the two clusters 1-2 and 4,5,7,3,6 into a single one,
which ends the iterations (Figure <a href="#fig:slinkstep6">34</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:slinkstep6"></span>
<img src="pics7b/01_slink_6.png" alt="Single Linkage - Step 6" width="32%" />
<p class="caption">
Figure 34: Single Linkage - Step 6
</p>
</div>
<p>In <code>GeoDa</code> the consecutive grouping can be found from the categories assigned
for each value of k and saved to the data table, as illustrated in
Figure <a href="#fig:slinkcats">35</a> (the cluster labels are arbitrary).<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> The
corresponding dendrogram is given in Figure <a href="#fig:slinkdendro">5</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:slinkcats"></span>
<img src="pics7b/01_stepsintable.png" alt="Single Linkage cluster categories saved in data table" width="50%" />
<p class="caption">
Figure 35: Single Linkage cluster categories saved in data table
</p>
</div>
</div>
<div id="complete-linkage-worked-example" class="section level3 unnumbered" number="">
<h3>Complete linkage worked example</h3>
<p>The first step in the complete linkage algorithm is the same as in single linkage, since the dissimilarity
between two single observations is unaffected by the criterion (the simple dissimilarity is also the minimum
and the maximum). As before (Figure <a href="#fig:slinkstep1">29</a>), observations 4 and 5 form the first cluster.</p>
<p>Next, we need to update the dissimilarities between the new cluster (4,5) and the other observations,
by taking the largest dissimilarity between each observation and either 4 or 5. For example, for observation 1,
we see from the dissimilarity matrix in Figure <a href="#fig:slinkdist">2</a> that the respective values are
5.00 (1-4) and 5.83 (1-5). In contrast to the single linkage updating formula, we now assign 5.83 as
the distance between 1 and the new cluster 4,5. We proceed in the same way for the other observations.
The result is the updated dissimilarity matrix given in Figure <a href="#fig:compstep2">36</a>. We encounter
a tie situation for the smallest value (2.24) and pick 1,2 as the new cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:compstep2"></span>
<img src="pics7b/22_complete2.png" alt="Complete Linkage - Step 2" width="72%" />
<p class="caption">
Figure 36: Complete Linkage - Step 2
</p>
</div>
<p>The updated dissimilarity matrix is shown in Figure <a href="#fig:compstep3">37</a>. Again, we take the largest dissimilarity
between an observation (or cluster) and either 1 or 2. The smallest overall dissimilarity is still 2.24, now between
6 and 7, which form the next cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:compstep3"></span>
<img src="pics7b/22_complete3.png" alt="Complete Linkage - Step 3" width="62%" />
<p class="caption">
Figure 37: Complete Linkage - Step 3
</p>
</div>
<p>The updated dissimilarity matrix is as in Figure <a href="#fig:compstep4">38</a>. Now, the smallest dissimilarity is 3.00 (again a tie).
We add 3 to 1,2 as the new cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:compstep4"></span>
<img src="pics7b/22_complete4.png" alt="Complete Linkage - Step 4" width="52%" />
<p class="caption">
Figure 38: Complete Linkage - Step 4
</p>
</div>
<p>This yields the updated dissimilarities as in Figure <a href="#fig:compstep5">39</a>. The smallest dissimilarity
is still 3.0, between 4,5 and 6,7. Those two clusters are grouped to yield the final result.</p>
<div class="figure" style="text-align: center"><span id="fig:compstep5"></span>
<img src="pics7b/22_complete5.png" alt="Complete Linkage - Step 5" width="42%" />
<p class="caption">
Figure 39: Complete Linkage - Step 5
</p>
</div>
<p>In the end, we have two clusters, one consisting of 1,2,3, the other of 4,5,6,7, as shown
in Figure <a href="#fig:compstep6">40</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:compstep6"></span>
<img src="pics7b/22_complete6.png" alt="Complete Linkage - Step 6" width="32%" />
<p class="caption">
Figure 40: Complete Linkage - Step 6
</p>
</div>
<p>In the same way as for single linkage, the consecutive grouping can be found in <code>GeoDa</code> from the categories assigned
for each value of k and saved to the data table, as illustrated in
Figure <a href="#fig:compcats">41</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:compcats"></span>
<img src="pics7b/33_complete_stepsintable.png" alt="Complete linkage cluster categories saved in data table" width="50%" />
<p class="caption">
Figure 41: Complete linkage cluster categories saved in data table
</p>
</div>
</div>
<div id="average-linkage-worked-example" class="section level3 unnumbered" number="">
<h3>Average linkage worked example</h3>
<p>Average linkage begins like the other methods, by selecting 4,5 as the first cluster.
As it turns out, in our example the updating formula for new clusters boils down to a simple
average of the dissimilarities to both points/clusters, since all but the last merger is between
balanced entities (e.g., 1 with 1 or 2 with 2). As a result, the weights attached to the
respective distances, <span class="math inline">\(n_a / (n_a + n_b) = n_b / (n_a + n_b) = 1/2\)</span>.</p>
<p>For example, for observation 1, the new dissimilarity to 4,5 is (1/2) <span class="math inline">\(\times\)</span> 5.00 + (1/2) <span class="math inline">\(\times\)</span> 5.83 = 5.42 (due to some
rounding) as shown in Figure <a href="#fig:avgstep2">42</a>. The other dissimilarities from/to 4,5 are updated in the same way.
The smallest dissimilarity (a tie) in the updated table is 2.24 between 1 and 2, which are grouped into a new cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:avgstep2"></span>
<img src="pics7b/44_avg2.png" alt="Average Linkage - Step 2" width="72%" />
<p class="caption">
Figure 42: Average Linkage - Step 2
</p>
</div>
<p>The new dissimilarities are shown in Figure <a href="#fig:avgstep3">43</a>, obtained as the average of the dissimilarities to 1 and 2
from the other points/clusters. In the new matrix, the smallest dissimilarity is again 2.24, between 6 and 7. They form the next cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:avgstep3"></span>
<img src="pics7b/44_avg3.png" alt="Average Linkage - Step 3" width="62%" />
<p class="caption">
Figure 43: Average Linkage - Step 3
</p>
</div>
<p>The updated dissimilarity matrix is given in Figure <a href="#fig:avgstep4">44</a>. The smallest dissimilarity is between
clusters 4,5 and 6,7, which are combined in the next step.</p>
<div class="figure" style="text-align: center"><span id="fig:avgstep4"></span>
<img src="pics7b/44_avg4.png" alt="Average Linkage - Step 4" width="52%" />
<p class="caption">
Figure 44: Average Linkage - Step 4
</p>
</div>
<p>In the updated dissimilarity matrix of Figure <a href="#fig:avgstep5">45</a>, the minimum dissimilarity is
between 3 and cluster 1,2. This yields the next to final grouping of 1,2,3 and 4,5,6,7.</p>
<div class="figure" style="text-align: center"><span id="fig:avgstep5"></span>
<img src="pics7b/44_avg5.png" alt="Average Linkage - Step 5" width="42%" />
<p class="caption">
Figure 45: Average Linkage - Step 5
</p>
</div>
<p>Only at this final stage is the computation of the averages somewhat more complex. Since the
distance of 4,5,6,7 needs to be computed with respect to the merger of 1,2 and 3, the respective
weights are 2/3 and 1/3. This yields the distance given in Figure <a href="#fig:avgstep6">46</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:avgstep6"></span>
<img src="pics7b/44_avg6.png" alt="Average Linkage - Step 6" width="32%" />
<p class="caption">
Figure 46: Average Linkage - Step 6
</p>
</div>
<p>The consecutive steps in the nesting process can be found from the categories assigned
for each value of k and saved to the <code>GeoDa</code> data table, as illustrated in
Figure <a href="#fig:avgcats">47</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:avgcats"></span>
<img src="pics7b/44_avg_stepsintable.png" alt="Average linkage cluster categories saved in data table" width="50%" />
<p class="caption">
Figure 47: Average linkage cluster categories saved in data table
</p>
</div>
</div>
<div id="wards-method-worked-example" class="section level3 unnumbered" number="">
<h3>Ward’s method worked example</h3>
<p>Ward’s method has the most complex updating formula of the four methods, involving not only the sum of the squared dissimilarity
between every point and the components of the new cluster (<span class="math inline">\(d_{PA}^2\)</span> and <span class="math inline">\(d_{PB}^2\)</span>), with weights <span class="math inline">\((n_a + n_p)/(n_c + n_p)\)</span>
and <span class="math inline">\((n_b + n_p)/(n_c + n_p)\)</span>, but also subtracts the squared dissimilarity between the merging elements
(<span class="math inline">\(d_{AB}^2\)</span>), with weight <span class="math inline">\(n_p / (n_c + n_p)\)</span>. However, as is the case for the other methods, the updated dissimilarities can readily be computed
from the matrix at the previous iteration.</p>
<p>The point of departure for Ward’s method is the matrix of <em>squared</em> Euclidian distances between the observations,
as shown for our example in Figure <a href="#fig:wardstep1">48</a>. As before, the smallest value is between 4 and 5, with <span class="math inline">\(d_{4,5}^2 = 1.00\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:wardstep1"></span>
<img src="pics7b/55_ward1.png" alt="Ward Linkage - Step 1" width="80%" />
<p class="caption">
Figure 48: Ward Linkage - Step 1
</p>
</div>
<p>Figure <a href="#fig:wardstep2">49</a> shows the first updated squared distance matrix. Since the new squared distances pertain
to single observations relative to a merged cluster of two single observations, <span class="math inline">\(n_a = n_b = n_p = 1\)</span> and <span class="math inline">\(n_c = 2\)</span>.
Therefore, the weights are 2/3 for <span class="math inline">\(d^2_{PA}\)</span> and <span class="math inline">\(d^2_{PB}\)</span> and 1/3 for <span class="math inline">\(d^2_{AB}\)</span>. For example, for the squared distance
between 1 and 4,5, the updating formula becomes <span class="math inline">\((2/3) \times 25.00 + (2/3) \times 34.00 - (1/3) \times 1.0 = 39.00\)</span>.
The other distances are computed in the same way. The squared distances that do not involve 4 or 5 are left unchanged.</p>
<p>The smallest squared distance in the matrix is between 1 and 2 with a value of 5.00. As before, there is a tie with
6 and 7, but the algorithm randomly picks 1,2. Hence, 1 and 2 are merged in the next step.</p>
<div class="figure" style="text-align: center"><span id="fig:wardstep2"></span>
<img src="pics7b/55_ward2.png" alt="Ward Linkage - Step 2" width="72%" />
<p class="caption">
Figure 49: Ward Linkage - Step 2
</p>
</div>
<p>The updated squared distances are shown in Figure <a href="#fig:wardstep3">50</a>. The values for the squared distances
between 1,2 and 3, 6 and 7 use the same weights as in the previous step. For the squared distance between
1,2 and 4,5, <span class="math inline">\(n_p = 2\)</span> and <span class="math inline">\(n_c = 2\)</span>, so that the weights become 3/4, 3/4 and 1/2. The smallest squared distance
in the matrix is 5.00, between 6 and 7. Those two observations form the next cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:wardstep3"></span>
<img src="pics7b/55_ward3.png" alt="Ward Linkage - Step 3" width="62%" />
<p class="caption">
Figure 50: Ward Linkage - Step 3
</p>
</div>
<p>The weights for the updating formula used in Figure <a href="#fig:wardstep4">51</a> are the same as in the previous step.
Now, there are two distances involving pairs of observations and one between a point and a pair. The smallest
value in the updated matrix is 9.67, between 1,2 and 3. Subsequently, in the next step, 3 is added to the cluster consisting
of 1 and 2.</p>
<div class="figure" style="text-align: center"><span id="fig:wardstep4"></span>
<img src="pics7b/55_ward4.png" alt="Ward Linkage - Step 4" width="52%" />
<p class="caption">
Figure 51: Ward Linkage - Step 4
</p>
</div>
<p>The updates in Figure <a href="#fig:wardstep5">52</a> involve squared distances between clusters of two observations (<span class="math inline">\(n_p = 2\)</span>) and
the new merger of two (<span class="math inline">\(n_a = 2\)</span>) and one observation (<span class="math inline">\(n_b = 1\)</span>), which gives <span class="math inline">\(n_c = 3\)</span>. The weights used in the update
of 4,5 to 1,2,3 and 6,7 to 1,2,3 are therefore 4/5 for <span class="math inline">\(d^2_{PA}\)</span>, 3/5 for <span class="math inline">\(d^2_{PB}\)</span> and 2/5 for <span class="math inline">\(d^2_{AB}\)</span>.</p>
<p>The smallest value in the matrix is 10.00 between 4,5, and 6,7, resulting in the merger of those two clusters
in the next to final step.</p>
<div class="figure" style="text-align: center"><span id="fig:wardstep5"></span>
<img src="pics7b/55_ward5.png" alt="Ward Linkage - Step 5" width="42%" />
<p class="caption">
Figure 52: Ward Linkage - Step 5
</p>
</div>
<p>In the final matrix, the weights are 5/7 for <span class="math inline">\(d^2_{PA}\)</span> and <span class="math inline">\(d^2_{PB}\)</span>, and 3/7 for <span class="math inline">\(d^2_{AB}\)</span>, yielding
the squared distance of 93.90 between the two remaining clusters, as shown in Figure <a href="#fig:wardstep6">53</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:wardstep6"></span>
<img src="pics7b/55_ward6.png" alt="Ward Linkage - Step 6" width="32%" />
<p class="caption">
Figure 53: Ward Linkage - Step 6
</p>
</div>
<p>As before, the consecutive steps in the nesting process are given in <code>GeoDa</code> by the categories assigned
for each value of k and saved to the data table, as illustrated in
Figure <a href="#fig:wardcats">54</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:wardcats"></span>
<img src="pics7b/55_ward_stepsintable.png" alt="Ward linkage cluster categories saved in data table" width="50%" />
<p class="caption">
Figure 54: Ward linkage cluster categories saved in data table
</p>
</div>
<p><br></p>
</div>
</div>
<div id="references" class="section level2 unnumbered" number="">
<h2>References</h2>
<div id="refs" class="references hanging-indent">
<div id="ref-Everittetal:11">
<p>Everitt, Brian S., Sabine Landau, Morven Leese, and Daniel Stahl. 2011. <em>Cluster Analysis, 5th Edition</em>. New York, NY: John Wiley.</p>
</div>
<div id="ref-KaufmanRousseeuw:05">
<p>Kaufman, L., and P. Rousseeuw. 2005. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. New York, NY: John Wiley.</p>
</div>
<div id="ref-Mullner:11">
<p>Müllner, Daniel. 2011. “Modern Hierarchical, Agglomerative Clustering Algorithms.” <em>ArXiv:1109.2378[stat.ML]</em>. <a href="http://arxiv.org/abs/1109.2378">http://arxiv.org/abs/1109.2378</a>.</p>
</div>
<div id="ref-Mullner:13">
<p>———. 2013. “fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for R and Python.” <em>Journal of Statistical Software</em> 53 (9).</p>
</div>
<div id="ref-Ward:63">
<p>Ward, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective Function.” <em>Journal of the American Statistical Association</em> 58: 236–44.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu" class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Detailed proofs for all the properties
are contained in Chapter 5 of <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>To see that this holds, consider the situation when <span class="math inline">\(d_{PA} &lt; d_{PB}\)</span>, i.e., <span class="math inline">\(A\)</span> is the
nearest neighbor to <span class="math inline">\(P\)</span>. As a result, the absolute value of <span class="math inline">\(d_{PA} - d_{PB}\)</span> is <span class="math inline">\(d_{PB} - d_{PA}\)</span>. Then the expression becomes <span class="math inline">\((1/2) d_{PA} + (1/2) d_{PB} - (1/2) d_{PB} + (1/2) d_{PA} = d_{PA}\)</span>, the desired result.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Unlike what was the case for k-means, the center point
does not play a role in single linkage clustering. Therefore, it is not shown in the Figure.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>By convention, the diagonal dissimilarity for the
newly merged cluster is set to zero.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>See <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>, Chapter 5, for detailed proofs.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>The factor 2 is included to make sure the expression works when two single observations are merged. In such an
instance, their centroid is their actual value and <span class="math inline">\(n_A + n_B = 2\)</span>. It does not matter in terms of the algorithm steps.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>The data input to <code>GeoDa</code> is a csv file with the point IDs and X, Y coordinates.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<footer class="site-footer">
  <span class="site-footer-owner"><a href="https://github.com/lixun910/geoda">GeoDa</a> is maintained by <a href="#">lixun910</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
</footer>

</section>


<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
