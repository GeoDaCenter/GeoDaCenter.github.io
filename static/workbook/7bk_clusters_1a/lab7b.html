<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="author" content="Luc Anselin" />


  <title>Cluster Analysis (1)</title>

  <script src="lab7b_files/header-attrs-2.3/header-attrs.js"></script>
  <link href="lab7b_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
  <script src="lab7b_files/highlightjs-9.12.0/highlight.js"></script>
  <title>GeoDa on Github</title>

  <style>
    * {
      margin: 0;
      padding: 0;
    }

    .shadowfilter {
      -webkit-filter: drop-shadow(12px 12px 7px rgba(0, 0, 0, 0.5));
      filter: url(shadow.svg#drop-shadow);
    }

    .intro1 {
      margin-left: -45px;
    }
  </style>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css"
    media="screen">
  <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
  <style>
    ul {
      padding-left: 30px;
    }

    figcaption {
      top: .70em;
      left: .35em;
      bottom: auto !important;
      right: auto !important;
    }
  </style>

  <style>
    h1 {
      text-align: center;
    }

    h3.subtitle {
      text-align: center;
    }

    h4.author {
      text-align: center;
    }

    h4.date {
      text-align: center;
    }

    p.caption {
      font-size: 12px;
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LC0QJ53WFS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-LC0QJ53WFS');
  </script>
  <!-- End Google tag -->

  <style type="text/css">
    code {
      white-space: pre;
    }
  </style>
  <script type="text/javascript">
    if (window.hljs) {
      hljs.configure({ languages: [] });
      hljs.initHighlightingOnLoad();
      if (document.readyState && document.readyState === "complete") {
        window.setTimeout(function () { hljs.initHighlighting(); }, 0);
      }
    }
  </script>








</head>

<body>


  <section class="main-content">


    <h1 class="title toc-ignore">Cluster Analysis (1)</h1>
    <h3 class="subtitle">K-Means Clustering</h3>
    <h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
    <h4 class="date">08/12/2020 (latest update)</h4>


    <div id="TOC">
      <ul>
        <li><a href="#introduction">Introduction</a>
          <ul>
            <li><a href="#objectives">Objectives</a>
              <ul>
                <li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#k-means">K Means</a>
          <ul>
            <li><a href="#principle">Principle</a>
              <ul>
                <li><a href="#partitioning-methods">Partitioning methods</a></li>
                <li><a href="#k-means-objective-function">K-means objective function</a></li>
                <li><a href="#iterative-relocation">Iterative relocation</a></li>
                <li><a href="#the-choice-of-k">The choice of K</a></li>
                <li><a href="#k-means-algorithms">K-means algorithms</a></li>
              </ul>
            </li>
            <li><a href="#implementation">Implementation</a>
              <ul>
                <li><a href="#variable-settings-panel">Variable Settings panel</a></li>
                <li><a href="#cluster-results">Cluster results</a></li>
                <li><a href="#adjusting-cluster-labels">Adjusting cluster labels</a></li>
                <li><a href="#cluster-variables">Cluster variables</a></li>
                <li><a href="#saving-the-cluster-results">Saving the cluster results</a></li>
              </ul>
            </li>
            <li><a href="#options-and-sensitivity-analysis">Options and sensitivity analysis</a>
              <ul>
                <li><a href="#changing-the-number-of-k-means-initial-runs">Changing the number of k-means++ initial
                    runs</a></li>
                <li><a href="#random-initialization">Random initialization</a></li>
                <li><a href="#selecting-a-different-standardization">Selecting a different standardization</a></li>
                <li><a href="#setting-a-minimum-bound">Setting a minimum bound</a></li>
                <li><a href="#elbow-plot">Elbow plot</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#cluster-categories-as-variables">Cluster Categories as Variables</a>
          <ul>
            <li><a href="#conditional-plots">Conditional plots</a></li>
            <li><a href="#aggregation-by-cluster">Aggregation by cluster</a></li>
          </ul>
        </li>
        <li><a href="#clustering-with-dimension-reduction">Clustering with Dimension Reduction</a>
          <ul>
            <li><a href="#pca">PCA</a></li>
            <li><a href="#mds">MDS</a></li>
          </ul>
        </li>
        <li><a href="#appendix">Appendix</a>
          <ul>
            <li><a href="#equivalence-of-euclidean-distances-and-sum-of-squared-errors-sse">Equivalence of Euclidean
                distances and Sum of Squared Errors (SSE)</a></li>
            <li><a href="#k-means-worked-example">K-means worked example</a></li>
          </ul>
        </li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <p><br></p>
    <div id="introduction" class="section level2 unnumbered" number="">
      <h2>Introduction</h2>
      <p>We now move from reducing the dimensionality of the variables to reducing the number of observations, or
        <em>clustering</em>. In general terms, clustering methods group <em>n</em> observations into <em>k</em>
        <strong>clusters</strong> such that
        the intra-cluster similarity is maximized, and the
        between-cluster similarity is minimized. Equivalently, we can think of it as minimizing intra-cluster
        dissimilarity and maximizing between-cluster dissimilarity.</p>
      <p>In other words, the goal of clustering methods is to achieve compact groups of <em>similar observations</em>
        that are separated as much as possible from the other
        groups.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
      <p>There are a very large number of clustering techniques and algorithms. They are standard tools of so-called
        <em>unsupervised learning</em> and constitute a core element in
        any machine learning toolbox. Classic texts include <span class="citation">Hartigan (<a href="#ref-Hartigan:75"
            role="doc-biblioref">1975</a>)</span>, <span class="citation">Jain and Dubes (<a href="#ref-JainDubes:88"
            role="doc-biblioref">1988</a>)</span>,
        <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05"
            role="doc-biblioref">2005</a>)</span>, and <span class="citation">Everitt et al. (<a
            href="#ref-Everittetal:11" role="doc-biblioref">2011</a>)</span>. A fairly recent overview of methods can be
        found in <span class="citation">Jain (<a href="#ref-Jain:10" role="doc-biblioref">2010</a>)</span>, and an
        evaluation of k-means on a number
        of benchmark data sets is contained in <span class="citation">Fränti and Sieranoja (<a
            href="#ref-FrantiSieranoja:18" role="doc-biblioref">2018</a>)</span>. In addition, excellent treatments of
        some of the more technical aspects are contained in Chapter 14 of <span class="citation">Hastie, Tibshirani, and
          Friedman (<a href="#ref-Hastieetal:09" role="doc-biblioref">2009</a>)</span>, Chapters 10 and 11 of <span
          class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12" role="doc-biblioref">2012</a>)</span>, and
        Chapter 10 of <span class="citation">James et al. (<a href="#ref-Jamesetal:13"
            role="doc-biblioref">2013</a>)</span>, among others.
      </p>
      <p>Clustering methods can be organized along a number of different dimensions. A common distinction is between
        partitioning methods, hierarchical methods, density-based methods and grid-based methods <span
          class="citation">(see, e.g., Han, Kamber, and Pei <a href="#ref-Hanetal:12" role="doc-biblioref">2012</a>,
          448–50)</span>. In addition, there are model-based approaches developed in the statistical literature, such as
        Gaussian mixture models (GMM) and Bayesian clusters.</p>
      <p>In this and the next two chapters, we focus on the two most common approaches, namely <em>partitioning</em>
        methods and <em>hierarchical</em> methods. We do not cover the model-based techniques, since they are less
        compatible with an exploratory mindset (more precisely, they require a formal probabilistic model). We also
        restrict our discussion to exact clustering methods and do not consider fuzzy clustering (where an observation
        may belong to more than one cluster). In exact clustering, the clusters are both <em>exhaustive</em> and
        <em>exclusive</em>. This means that every observation must belong to one cluster, and only one cluster.</p>
      <p>In the current chapter, we deal with <strong>k-means</strong> clustering, the most familiar example of a
        partitioning method. Hierarchical clustering is covered in the next chapter and more advanced techniques in a
        third.</p>
      <p>To illustrate these methods, we will continue to use the Guerry data set on
        moral statistics in 1830 France, which comes pre-installed with <code>GeoDa</code>.</p>
      <div id="objectives" class="section level3 unnumbered" number="">
        <h3>Objectives</h3>
        <ul>
          <li>
            <p>Understand the principles behind k-means clustering</p>
          </li>
          <li>
            <p>Know the requirements to carry out k-means clustering</p>
          </li>
          <li>
            <p>Interpret the characteristics of a cluster analysis</p>
          </li>
          <li>
            <p>Carry out a sensitivity analysis to various parameters</p>
          </li>
          <li>
            <p>Impose a bound on the clustering solutions</p>
          </li>
          <li>
            <p>Use an elbow plot to pick the best k</p>
          </li>
          <li>
            <p>Use the cluster categories as a variable</p>
          </li>
          <li>
            <p>Combine dimension reduction and cluster analysis</p>
          </li>
        </ul>
        <div id="geoda-functions-covered" class="section level4 unnumbered" number="">
          <h4>GeoDa functions covered</h4>
          <ul>
            <li>Clusters &gt; K Means
              <ul>
                <li>select variables</li>
                <li>select k-means starting algorithms</li>
                <li>select standardization methods</li>
                <li>k-means characteristics</li>
                <li>mapping the clusters</li>
                <li>changing the cluster labels</li>
                <li>saving the cluster classification</li>
                <li>setting a minimum bound</li>
              </ul>
            </li>
            <li>Explore &gt; Conditional Plot &gt; Box Plot</li>
            <li>Table &gt; Aggregate</li>
            <li>Tools &gt; Dissolve</li>
          </ul>
          <p><br></p>
        </div>
      </div>
    </div>
    <div id="k-means" class="section level2 unnumbered" number="">
      <h2>K Means</h2>
      <div id="principle" class="section level3 unnumbered" number="">
        <h3>Principle</h3>
        <div id="partitioning-methods" class="section level4 unnumbered" number="">
          <h4>Partitioning methods</h4>
          <p>A <em>partitioning</em> clustering method consists of assigning each observation to one out of <span
              class="math inline">\(k\)</span> clusters. For each observation <span class="math inline">\(i\)</span>,
            this can be symbolized by an <em>encoder</em> C, such that <span class="math inline">\(C(i) = h\)</span>,
            where <span class="math inline">\(h\)</span>, the cluster indicator is an element from the set <span
              class="math inline">\(\{1, \dots, k\}\)</span>. The cluster labels (<span
              class="math inline">\(h\)</span>) are meaningless, and could just as well be letters or other distinct
            symbols.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
          <p>The overall objective is to end up with a grouping that minimizes the dissimilarity within each cluster.
            Mathematically, we can think of an overall loss function that consists of summing the <em>distances</em>
            between all the pairs of observations (in general, any measure of dissimilarity):
            <span class="math display">\[T = (1/2) \sum_{i = 1}^n \sum_{j = 1}^n d_{ij},\]</span>
            where <span class="math inline">\(d_{ij}\)</span> is some measure of dissimilarity, such as the Euclidean
            distance between the values at observations <span class="math inline">\(i\)</span> and <span
              class="math inline">\(j\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The
            objective is then to find a grouping of the <span class="math inline">\(i\)</span> into clusters <span
              class="math inline">\(h\)</span> that minimizes this loss function, or, alternatively, maximizes the
            similarity (<span class="math inline">\(-T\)</span>).
          </p>
          <p>In any given cluster <span class="math inline">\(h\)</span>, we can separate out the distances from each of
            its members (<span class="math inline">\(i \in h\)</span>) to all other observations between those that
            belong to the cluster (<span class="math inline">\(j \in h\)</span>) and those that do not (<span
              class="math inline">\(j \notin h\)</span>):
            <span class="math display">\[T_{i \in h} = (1/2) [ \sum_{i \in h} \sum_{j \in h} d_{ij} + \sum_{i \in h}
              \sum_{j \notin h} d_{ij} ],\]</span>
            and, for all the clusters, as:
            <span class="math display">\[T = (1/2) (\sum_{h=1}^k [ \sum_{i \in h} \sum_{j \in h} d_{ij} + \sum_{i \in h}
              \sum_{j \notin h} d_{ij}]) = W + B,\]</span>
            where the first term (<span class="math inline">\(W\)</span>) is referred to as the <em>within</em>
            dissimilarity and the second (<span class="math inline">\(B\)</span>) as the <em>between</em> dissimilarity.
            In other words, the total dissimilarity decomposes into one part due to what happens within each cluster and
            another part that pertains to the between cluster dissimilarities. <span class="math inline">\(W\)</span>
            and <span class="math inline">\(B\)</span> are complementary, so the lower <span
              class="math inline">\(W\)</span>, the higher <span class="math inline">\(B\)</span>, and vice versa. We
            can thus attempt to find an optimum by <em>minimizing</em> the <em>within</em> dissimilarity,
            <span class="math inline">\(W = (1/2) \sum_{h=1}^k \sum_{i \in h} \sum_{j \in h} d_{ij}\)</span>.
          </p>
          <p>Partitioning methods differ in terms of how the dissimilarity <span class="math inline">\(d_{ij}\)</span>
            is defined and how the term <span class="math inline">\(W\)</span> is minimized. Complete enumeration of all
            the possible allocations is infeasible except for toy problems, and there is
            no analytical solution. The problem is NP-hard, so the solution has to be approached by means of a
            heuristic, as an iterative descent process. This is accomplished through an algorithm that changes the
            assignment of observations to clusters so as to improve the objective function at each step. All feasible
            approaches are based on what is called <em>iterative greedy descent</em>. A greedy algorithm is one that
            makes a locally optimal decision at each stage. It is therefore not guaranteed to end up in a
            <em>global</em> optimum, but may get stuck in a <em>local</em> one instead. The k-means method uses an
            <em>iterative relocation</em> heuristic as the optimization strategy (see <a
              href="#iterative-relocation">Iterative relocation</a>). First, we consider the associated dissimilarity
            measure more closely.</p>
        </div>
        <div id="k-means-objective-function" class="section level4 unnumbered" number="">
          <h4>K-means objective function</h4>
          <p>The K-means algorithm is based on the squared Euclidean distance as the measure of dissimilarity:
            <span class="math display">\[d_{ij}^2 = \sum_{v=1}^p (x_{iv} - x_{jv})^2 = ||x_i - x_j||^2,\]</span>
            where we have changed our customary notation and now designate the number of variables/dimensions as <span
              class="math inline">\(p\)</span> (since <span class="math inline">\(k\)</span> is traditionally used to
            designate the number of clusters).
          </p>
          <p>This gives the overall objective as finding the allocation <span class="math inline">\(C(i)\)</span> of
            each observation <span class="math inline">\(i\)</span> to a cluster
            <span class="math inline">\(h\)</span> out of the <span class="math inline">\(k\)</span> clusters so as to
            minimize the within-cluster similarity over all <span class="math inline">\(k\)</span> clusters:
            <span class="math display">\[\mbox{min}(W) = \mbox{min} (1/2) \sum_{h=1}^k \sum_{i \in h} \sum_{j \in h}
              ||x_i - x_j||^2,\]</span>
            where, in general, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are
            <span class="math inline">\(p\)</span>-dimensional vectors.
          </p>
          <p>A little bit of algebra shows how this simplifies to minimizing the squared difference between the values
            of the observations in each cluster and the corresponding cluster mean (see the <a
              href="#appendix">Appendix</a> for details):
            <span class="math display">\[\mbox{min}(W) = \mbox{min} \sum_{h=1}^k n_h \sum_{i \in h} (x_i -
              \bar{x}_h)^2.\]</span>
          </p>
          <p>In other words, minimizing the sum of (one half) of all squared distances is equivalent to minimizing
            the sum of squared deviations from the mean in each cluster, the <em>within</em> sum of squared errors.</p>
        </div>
        <div id="iterative-relocation" class="section level4 unnumbered" number="">
          <h4>Iterative relocation</h4>
          <p>The k-means algorithm is based on the principle of <em>iterative relocation</em>. In essence, this means
            that after an initial solution is established, subsequent moves (i.e., allocating observations to clusters)
            are made to improve the objective function. As mentioned, this is a greedy algorithm, that ensures that at
            each step the total across clusters of the within-cluster sums of squared errors (from the respective
            cluster means) is lowered. The algorithm stops when no improvement is possible. However, this does not
            ensure that a <em>global</em> optimum is achieved <span class="citation">(for an early discussion, see
              Hartigan and Wong <a href="#ref-HartiganWong:79" role="doc-biblioref">1979</a>)</span>. Therefore
            sensitivity analysis is essential. This is addressed by trying many different initial allocations (typically
            assigned randomly).</p>
          <p>To illustrate the logic behind the algorithm, we consider a simple toy example of seven observations, as
            depicted in Figure <a href="#fig:kmexample">1</a>. The full numerical details are given in the <a
              href="#appendix">Appendix</a>. The plot shows the location of the seven points in two-dimensional <span
              class="math inline">\(X-Y\)</span> space, with the center (the mean of <span
              class="math inline">\(X\)</span> and the mean of <span class="math inline">\(Y\)</span>) shown in a
            lighter color (note that the center is <em>not</em> one of the observations). The objective is to group the
            seven points into two clusters.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmexample"></span>
            <img src="pics7b/00_initial_step0.png" alt="K-means toy example" width="35%" />
            <p class="caption">
              Figure 1: K-means toy example
            </p>
          </div>
          <p>The initial step of the algorithm is to <em>randomly</em> pick two <em>seeds</em>, one for each cluster.
            The seeds are actual observations, not some other random location. In our example, we selected observations
            4 and 7, shown by the lighter color in the upper left panel of Figure <a href="#fig:kmsteps">2</a>. The
            other observations are allocated to the cluster whose seed they are closest to. We see that the green
            cluster has one other observation (6, in dark green), and the blue cluster four more (in dark blue). This is
            the initial layout, shown in the top right panel of the figure: two observations in one cluster and five in
            the other.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmsteps"></span>
            <img src="pics7b/00_allsteps1_markup2.png" alt="Steps in the k-means algorithm" width="75%" />
            <p class="caption">
              Figure 2: Steps in the k-means algorithm
            </p>
          </div>
          <p>Next, we compute the <em>center</em> for each cluster, i.e., the mean of each coordinate, shown in the
            figure in the lighter color. Note that from this point on, the center is no longer one of the observations
            (or only by coincidence).</p>
          <p>The first real step of the algorithm allocates observations to the cluster center they are closest to. In
            the top right panel, we see that point 5 from the original blue cluster is <em>closer</em> to the new center
            for the green cluster than to the new center for the blue cluster. Consequently, it moves to join the green
            cluster, which now consists of three observations (in the lower left panel).</p>
          <p>We repeat the process with the new centers in the lower left panel and again find point 4 from the blue
            cluster closer to the updated center for the green cluster, so it joins the latter.
            This yields one cluster of four (green) observations (closest to the center in the upper right panel) and
            one of three (blue) observations.</p>
          <p>This process is repeated until no more improvement is possible. In our example, we are done after the two
            steps, since all observations are closest to their center. The lower-right panel in the figure shows the
            final allocation with associated centers.</p>
          <p>This solution corresponds to the allocation that would result from a Voronoi diagram or Thiessen polygon
            around the cluster centers. All observations are closer to their center than to any other center. This is
            indicated by the black line in the graph,
            which is perpendicular at the midpoint to an imaginary line connecting the two centers and separates the
            area into two
            compact <em>regions</em>.</p>
        </div>
        <div id="the-choice-of-k" class="section level4 unnumbered" number="">
          <h4>The choice of K</h4>
          <p>A key element in the k-means method is the choice of the number of clusters, k. Typically,
            several values for k are considered, and the resulting clusters are then compared in terms of the
            objective function.</p>
          <p>Since the total sum of squared errors (SSE) equals the sum of the within-group SSE
            and the total between-group SSE, a common criterion is to assess the ratio of the total
            between-group sum of squares (BSS) to the total sum of squares (TSS), i.e., BSS/TSS. A higher value for this
            ratio suggests a better
            separation of the clusters. However, since this ratio increases with k, the selection
            of a <em>best</em> k is not straightforward. Several ad hoc rules have been suggested, but none is totally
            satisfactory.</p>
          <p>One useful approach is to plot the objective function against increasing values of k. This could be either
            the within sum of squares (WSS), a decreasing function with k, or the ratio BSS/TSS, a value that increases
            with k. The goal of a so-called <em>elbow plot</em> is to find a <em>kink</em> in the progression of the
            objective function against the value of k. The rationale behind this is that as long as the optimal number
            of clusters has not been reached, the improvement in the objective should be substantial, but as soon as the
            optimal k has been exceeded, the curve flattens out. This is somewhat subjective and often not that easy to
            interpret in practice. <code>GeoDa</code>’s functionality does not include an elbow plot, but all the
            information regarding the objective functions needed to create such a plot is provided in the output.</p>
          <p>A more formal approach is the so-called Gap statistic of
            <span class="citation">Tibshirani, Walther, and Hastie (<a href="#ref-Tibshiranietal:01"
                role="doc-biblioref">2001</a>)</span>, which employs the difference between the log of the WSS and the
            log of the WSS of a uniformly randomly generated reference distribution (uniform over a hypercube that
            contains the data) to construct a <em>test</em> for the optimal k. Since this approach is computationally
            quite demanding, we do not consider it here.
          </p>
        </div>
        <div id="k-means-algorithms" class="section level4 unnumbered" number="">
          <h4>K-means algorithms</h4>
          <p>The earliest solution of the k-means problem is commonly attributed to <span class="citation">Lloyd (<a
                href="#ref-Lloyd:82" role="doc-biblioref">1982</a>)</span> and referred to as <em>Lloyd’s algorithm</em>
            (the algorithm was first contained in a Bell Labs technical report by Lloyd from 1957). While the
            progression of the iterative relocation is fairly straightforward, the choice of the initial starting point
            is not.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
          <p>Typically, the assignment of the first set of k cluster centers is obtained by uniform random sampling k
            distinct observations from the full set of n observations. In other words, each observation has the same
            probability of being selected.</p>
          <p>The standard approach is to try several random assignments and start with the one that gives the best value
            for the objective function (e.g., the smallest WSS). This is one of the two approaches implemented in
            <code>GeoDa</code>. In order to ensure replicability, it
            is important to set a seed value for the random number generator. Also, to further assess the
            sensitivity of the result to the starting point, different seeds should be tried (as well as a
            different number of initial solutions). <code>GeoDa</code> implements this approach by leveraging the
            functionality contained in the <em>C Clustering Library</em> <span class="citation">(Hoon, Imoto, and Miyano
              <a href="#ref-deHoonetal:17" role="doc-biblioref">2017</a>)</span>.</p>
          <p>A second approach uses a careful consideration of initial seeds, following the procedure
            outlined in <span class="citation">Arthur and Vassilvitskii (<a href="#ref-ArthurVassilvitskii:07"
                role="doc-biblioref">2007</a>)</span>, commonly referred to as <strong>k-means++</strong>. The rationale
            behind k-means++ is that rather than sampling uniformly from the n observations, the probability of
            selecting a new cluster seed is changed in function of the distance to the nearest existing seed. Starting
            with a uniformly random selection of the first seed, say <span class="math inline">\(c_1\)</span>, the
            probabilities for the remaining observations are computed as:
            <span class="math display">\[p_{j \neq c_1} = \frac{d_{jc_1}^2}{\sum_{j \neq c_1} d_{jc_1}^2 }.\]</span>
            In other words, the probability is no longer uniform, but changes with the squared distance to the existing
            seed: the smaller the distance, the smaller the probability. This is referred to by <span
              class="citation">Arthur and Vassilvitskii (<a href="#ref-ArthurVassilvitskii:07"
                role="doc-biblioref">2007</a>)</span> as the squared distance (<span class="math inline">\(d^2\)</span>)
            weighting.
          </p>
          <p>The weighting increases the chance that the next seed is further away from the existing seeds, providing a
            better coverage over the support of the sample points. Once the second seed is selected, the probabilities
            are updated in function of the new distances to the closest seed, and the process continues until all k
            seeds are picked.</p>
          <p>While generally being
            faster and
            resulting in a superior solution in small to medium sized data sets,
            this method does not scale well (as it requires
            k passes through the whole data set to recompute the distances and the updated probabilities). Note that a
            choice of a large number of random initial allocations may
            yield a better outcome than the application of k-means++, at the expense of a
            somewhat longer execution time.</p>
        </div>
      </div>
      <div id="implementation" class="section level3 unnumbered" number="">
        <h3>Implementation</h3>
        <p>We invoke the k-means functionality from the same <strong>Clusters</strong> toolbar icon as we
          have seen previously for dimension reduction. It is part of the second subset of
          functionality, which includes several classic and more advanced (non-spatial)
          clustering techniques, as shown in Figure <a href="#fig:kmeansmenu">3</a>. From the
          menu, it is selected as <strong>Clusters &gt; K Means</strong>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansmenu"></span>
          <img src="pics7b/1_714_kmeans_option.png" alt="K Means Option" width="10%" />
          <p class="caption">
            Figure 3: K Means Option
          </p>
        </div>
        <p>This brings up the <strong>K Means Clustering Settings</strong> dialog, shown
          in Figure <a href="#fig:kmeansvars">4</a>, the main interface through which variables
          are chosen, options selected, and summary results are provided.</p>
        <div id="variable-settings-panel" class="section level4 unnumbered" number="">
          <h4>Variable Settings panel</h4>
          <p>We select the variables and set the parameters for the K Means cluster analysis
            through the options in the left hand panel of the interface. We choose the same six
            variables as in the previous chapter: <strong>Crm_prs</strong>,
            <strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>, <strong>Infants</strong>, and
            <strong>Suicids</strong>.
            These variables
            appear highlighted in the <strong>Select Variables</strong> panel.
          </p>
          <div class="figure" style="text-align: center"><span id="fig:kmeansvars"></span>
            <img src="pics7b/2_715_kmeans_variables.png" alt="K Means variable selection" width="80%" />
            <p class="caption">
              Figure 4: K Means variable selection
            </p>
          </div>
          <p>The next option is to select the <strong>Number of Clusters</strong>. The initial setting is blank. One can
            either choose a value from
            the drop-down list, or enter an integer value directly.<a href="#fn6" class="footnote-ref"
              id="fnref6"><sup>6</sup></a> In our example, we set the number of clusters to <strong>5</strong>.</p>
          <p>A default option is to use the variables in standardized form, i.e., in standard
            deviational units, expressed with <strong>Transformation</strong> set to
            <strong>Standardize (Z)</strong>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>
          </p>
          <p>The default algorithm is
            <strong>KMeans++</strong> with initialization re-runs set to 150 and maximal iterations to 1000.
            The <strong>seed</strong> is the global random number seed set for GeoDa, which can be changed by means of
            the
            <strong>Use specified seed</strong> option. Finally, the default <strong>Distance Function</strong> is
            <strong>Euclidean</strong>.
            For k-means, this is the only option available, since it is the basis for the objective function.
          </p>
          <p>The cluster classification will be
            saved in the variable specified in the <strong>Save Cluster in Field</strong> box. The default of
            <strong>CL</strong>
            is likely not very useful if several options will be explored. In our first example, we
            set the variable name to <strong>CLa</strong>.</p>
        </div>
        <div id="cluster-results" class="section level4 unnumbered" number="">
          <h4>Cluster results</h4>
          <p>After pressing <strong>Run</strong>, and keeping all the settings as above, a cluster map is created as a
            new view and the characteristics of
            the cluster are listed in the <strong>Summary</strong> panel.</p>
          <p>The cluster map in Figure <a href="#fig:kmeansmap5">5</a> reveals quite evenly balanced clusters, with 22,
            19, 18, 16 and 10 members
            respectively. Keep in mind that the clusters are based on attribute similarity and they do not respect
            contiguity or compactness (we will examine spatially constrained clustering in a later chapter).</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeansmap5"></span>
            <img src="pics7b/2_717_clustermap_5.png" alt="K Means cluster map (k=5)" width="60%" />
            <p class="caption">
              Figure 5: K Means cluster map (k=5)
            </p>
          </div>
          <p>The cluster characteristics are listed in the <strong>Summary</strong> panel, shown
            in Figure <a href="#fig:kmeanssummary5">6</a>. This lists, for each cluster, the
            method (KMeans), the value of k (here, 5), as well as
            the parameters specified (i.e., the initialization methods, number of initialization
            re-runs, the maximum iterations, transformation, and distance function).</p>
          <p>Next
            follows a table with the values of the cluster centers for each of the variables involved in the
            clustering algorithm. Note that while the algorithm uses standardized values (with the
            <strong>Standardize</strong> option on), the results are presented in the original units. This allows for a
            comparison of the cluster means to the overall mean of the variable. In our example, the overall means are
            19,961 for Crm_prs, 7,881 for Crm_prp, 39.1 for Litercy, 6,723 for Donatns, 18,983 for Infants, and
            36,517 for Suicids. The interpretation is not always straightforward due to the complex interplay among the
            variables. In our example, none of the clusters have a mean that is superior to the national average for all
            variables. Cluster 3 is below the national mean for all but one variable (litercy). In a typical
            application, a close examination of these profiles should be the basis for any kind of meaningful
            characterization of the <em>clusters</em> (e.g., as in a socio-demographic profile).</p>
          <p>Finally, a number of summary measures are provided to assess the overall quality of the cluster results.
            The total sum of
            squares is listed (504), as well as the within-cluster sum of squares for each of the clusters. The absolute
            values of these results are not of great interest, but their relative magnitudes matter. For example, we can
            see that the similarity within cluster 3 (18 observations) is more than twice as good (less than half) as
            that in clusters 4 and 5.</p>
          <p>Finally, these statistics are summarized as the total within-cluster sum of squares (253.277),
            the total between-cluster sum of squares (the difference between TSS and WSS, 250.723), and the ratio of
            between-cluster to total
            sum of squares. In our initial example, the latter is 0.497467.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanssummary5"></span>
            <img src="pics7b/2_716_cluster_chars_5.png" alt="K Means cluster characteristics (k=5)" width="50%" />
            <p class="caption">
              Figure 6: K Means cluster characteristics (k=5)
            </p>
          </div>
        </div>
        <div id="adjusting-cluster-labels" class="section level4 unnumbered" number="">
          <h4>Adjusting cluster labels</h4>
          <p>The cluster labels (and colors in the map) are arbitrary and can be changed in the cluster
            map, using the same technique we saw earlier for unique value maps (in fact, the cluster
            maps are a special case of unique value maps). For example, if we wanted to switch
            category 4 with 3 and the corresponding colors, we would move the light green rectangle
            in the legend with label 4 up to the third spot in the legend, as shown in
            Figure <a href="#fig:changelabels">7</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:changelabels"></span>
            <img src="pics7b/2_722_change_labels1.png" alt="Change cluster labels" width="15%" />
            <p class="caption">
              Figure 7: Change cluster labels
            </p>
          </div>
          <p>Once we release the cursor, an updated cluster map is produced, with the categories (and colors)
            for 3 and 4 switched, as in Figure <a href="#fig:kmeanschangelabels">8</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanschangelabels"></span>
            <img src="pics7b/2_723_changed_labels.png" alt="Relabeled cluster map" width="60%" />
            <p class="caption">
              Figure 8: Relabeled cluster map
            </p>
          </div>
        </div>
        <div id="cluster-variables" class="section level4 unnumbered" number="">
          <h4>Cluster variables</h4>
          <p>As the clusters are computed, a new categorical variable is added to the data table
            (the variable name is specified in the <strong>Save Cluster in Field</strong> option).
            It contains the assignment of each observation to one of the clusters as an
            integer variable. However, this
            is not automatically updated when we change the labels, as we did in the example above.</p>
          <p>In order to save the updated classification, we can still use the generic <strong>Save Categories</strong>
            option
            available in any map view (right click in the map).
            After specifying a variable name (e.g., <strong>cat_a</strong>), we can see both the original categorical
            variable and the new classification in the data table.</p>
          <p>In the table, shown in Figure <a href="#fig:kmeanscattable">9</a>, wherever <strong>CLa</strong> (the
            original
            classification) is <strong>3</strong>, the new classification (<strong>cat_a</strong>) is
            <strong>4</strong>.
            As always, the new variables do not become permanent additions until the table is saved.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanscattable"></span>
            <img src="pics7b/2_725_labels.png" alt="Cluster categories in table" width="20%" />
            <p class="caption">
              Figure 9: Cluster categories in table
            </p>
          </div>
        </div>
        <div id="saving-the-cluster-results" class="section level4 unnumbered" number="">
          <h4>Saving the cluster results</h4>
          <p>The summary results listed in the <strong>Summary</strong> panel can be saved to a text file. Right
            clicking
            on the panel to brings up a dialog with a <strong>Save</strong> option,
            as in Figure <a href="#fig:kmeanssave">10</a>. Selecting this and specifying a file name
            for the results will provide a permanent record of the analysis.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanssave"></span>
            <img src="pics7b/2_731_savesummary.png" alt="Saving summary results to a text file" width="40%" />
            <p class="caption">
              Figure 10: Saving summary results to a text file
            </p>
          </div>
        </div>
      </div>
      <div id="options-and-sensitivity-analysis" class="section level3 unnumbered" number="">
        <h3>Options and sensitivity analysis</h3>
        <p>The k-means algorithm depends crucially on its initialization and on the various parameters specified.
          It is important to assess the sensitivity of the results to the starting point and other parameters and
          settings. We consider a few of these next.</p>
        <div id="changing-the-number-of-k-means-initial-runs" class="section level4 unnumbered" number="">
          <h4>Changing the number of k-means++ initial runs</h4>
          <p>The default number of initial re-runs for the k-means++ algorithm is <strong>150</strong>. Sometimes,
            this is not sufficient to guarantee the best possible result (e.g., in terms of the ratio of between
            to total sum of squares). Recall that k-means++ is simply a different way to have random initialization, so
            it also offers no guarantees with respect to the optimality of the starting point.</p>
          <p>We can change the number of initial runs to obtain a starting point from the k-means++ algorithm in the
            <strong>Initialization Re-runs</strong> dialog, as
            illustrated in Figure <a href="#fig:kppinit">11</a> for a value of <strong>1000</strong> iterations.</p>
          <div class="figure" style="text-align: center"><span id="fig:kppinit"></span>
            <img src="pics7b/4_042_kppinitialize1000.png" alt="K-means++ initialization re-runs" width="30%" />
            <p class="caption">
              Figure 11: K-means++ initialization re-runs
            </p>
          </div>
          <p>The result is slightly different from what we obtained for the default setting.
            As shown in Figure <a href="#fig:kppinitmap">12</a>, the first category now has 23 elements, and
            the second 18. The other groupings remain the same.</p>
          <div class="figure" style="text-align: center"><span id="fig:kppinitmap"></span>
            <img src="pics7b/4_043_kpp1000map.png" alt="Cluster map for 1000 initial re-runs" width="60%" />
            <p class="caption">
              Figure 12: Cluster map for 1000 initial re-runs
            </p>
          </div>
          <p>The revised initialization results in a slight improvement of the sum of squares ratio,
            changing from 0.497467 to 0.497772, as shown in
            Figure <a href="#fig:kppinitresults">13</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:kppinitresults"></span>
            <img src="pics7b/4_044_kppinitresults.png" alt="Cluster characteristics for 1000 initial reruns"
              width="50%" />
            <p class="caption">
              Figure 13: Cluster characteristics for 1000 initial reruns
            </p>
          </div>
        </div>
        <div id="random-initialization" class="section level4 unnumbered" number="">
          <h4>Random initialization</h4>
          <p>The alternative to the Kmeans++ initialization is to select the traditional random initialization with
            uniform probabilities across the observations. This is accomplished by choosing <strong>Random</strong> as
            the <strong>Initialization Method</strong>,
            as shown in Figure <a href="#fig:randominit">14</a>. We keep the number of initialization re-runs
            to the default value of 150 and save the result in the
            variable <strong>CLr</strong>.</p>
          <div class="figure" style="text-align: center"><span id="fig:randominit"></span>
            <img src="pics7b/3_732_randominit.png" alt="Random initialization" width="30%" />
            <p class="caption">
              Figure 14: Random initialization
            </p>
          </div>
          <p>The result is identical to what we obtained for K-means++ with 1000 initialization re-runs,
            with the cluster map as in Figure <a href="#fig:kppinitmap">12</a> and the cluster summary
            as in Figure <a href="#fig:kppinitresults">13</a>. However, if we had run the random initialization with
            much fewer runs (e.g., 10), the results would be inferior to what we obtained before.
            This highlights the effect of the starting values on the ultimate result.</p>
        </div>
        <div id="selecting-a-different-standardization" class="section level4 unnumbered" number="">
          <h4>Selecting a different standardization</h4>
          <p>The objective function for k-means clustering is sensitive to the scale in which the variables are
            expressed. When these scales are very different (e.g., one variable is as a percentage and another is
            expressed
            as thousands of dollars), standardization
            converts the observations to more comparable magnitudes. As a result, the squared differences (or squared
            difference
            from the cluster mean) become more or less comparable across variables. More specifically, this avoids that
            variables that show a large variance dominate the objective function.</p>
          <p>The most common standardization is the so-called z-standardization, where the original value is centered
            around zero (by subtracting the mean) and rescaled to have a variance of one (by dividing by the standard
            deviation). However, this is by no means the only way to <em>correct</em> for the effect of different
            spreads
            among the variables. Some other approaches, which may be more robust to the effect of outliers (on the
            estimates of mean and variance used in the standardization) are the MAD, mentioned in the context of PCA, as
            well as range standardization. The latter is often recommended for cluster excercises <span
              class="citation">(e.g., Everitt et al. <a href="#ref-Everittetal:11"
                role="doc-biblioref">2011</a>)</span>.
            It rescales each variable such that its minimum is zero and its maximum becomes one by subtracting the
            minimum and
            dividing by the range.</p>
          <p>We assess the effect of such a standardization on the k-means result by selecting the associated option, as
            in Figure <a href="#fig:rangeinit">15</a>. We leave all the other options to the default setting.</p>
          <div class="figure" style="text-align: center"><span id="fig:rangeinit"></span>
            <img src="pics7b/22_range_standardize.png" alt="Range Standardization" width="40%" />
            <p class="caption">
              Figure 15: Range Standardization
            </p>
          </div>
          <p>The resulting clusters are depicted in Figure <a href="#fig:rangemap">16</a>. Compared to the standard
            result in Figure <a href="#fig:kmeansmap5">5</a>,
            the spatial layout
            of cluster members is quite different. Two of the clusters are larger (1 and 2 with 27 and 20 elements) and
            two others smaller
            (3 and 4 with 17 and 11). Cluster 5 has the same number of members as in the original solution, but only
            half the observations are
            in common.</p>
          <div class="figure" style="text-align: center"><span id="fig:rangemap"></span>
            <img src="pics7b/22_range_map.png" alt="Cluster map for range standardization" width="60%" />
            <p class="caption">
              Figure 16: Cluster map for range standardization
            </p>
          </div>
          <p>The difference between the two solutions is also highlighted by the cluster summary measures,
            shown in Figure <a href="#fig:rangeresults">17</a>. Clusters 3, 4 and 5 are almost equally compact and the
            overall BSS/TSS ratio is the highest achieved so far, at 0.537.</p>
          <div class="figure" style="text-align: center"><span id="fig:rangeresults"></span>
            <img src="pics7b/22_range_results.png" alt="Cluster characteristics for range standardization"
              width="50%" />
            <p class="caption">
              Figure 17: Cluster characteristics for range standardization
            </p>
          </div>
          <p>This again highlights the need to carry out sensitivity analysis and not take the default standardization
            as the only available option. Depending on the relative ranges and variances of the variables under
            consideration,
            one standardization may achieve a better grouping than another. Also, in some circumstances, when the
            variables
            are already on similar scales, standardization is not necessary.</p>
        </div>
        <div id="setting-a-minimum-bound" class="section level4 unnumbered" number="">
          <h4>Setting a minimum bound</h4>
          <p>It is also possible to <em>constrain</em> the k-means
            clustering by imposing a minimum value for a spatially extensive variable, such as a
            total population. This ensures that the clusters meet a minimum size for that variable.
            For example, in studies of socio-economic determinants of health, we may want to group
            similar census tracts into <em>neighborhoods</em>. But if we are concerned with rare diseases, privacy
            concerns often require a minimum size for the population at risk. By setting a minimum bound
            on the population variable, we
            avoid creating clusters that are <em>too small</em>.</p>
          <p>The minimum bound is set in the variable settings dialog by checking the
            box next to <strong>Minimum Bound</strong>, as in Figure <a href="#fig:minbound">18</a>. This variable does
            not
            have to be part of the variables used in the clustering exercise. In fact, it should
            be a relevant <em>size</em> variable (so-called spatially extensive variable), such as total
            population or total number of establishments, unrelated to the multivariate dimensions involved in
            the clusters.</p>
          <p>In our example, we
            select the variable <strong>Pop1831</strong> to set the constraint. Note that we specify a bound
            of <strong>16%</strong> (or <strong>5179</strong>) rather than the default <strong>10%</strong>. This is
            because the
            standard k-means solution satisfies the default constraint already, so that no actual bounding
            is carried out. Also, any percentage higher than 16 fails to yield a cluster solution that meets the
            population requirement.</p>
          <div class="figure" style="text-align: center"><span id="fig:minbound"></span>
            <img src="pics7b/7_075_minbound.png" alt="Setting a minimum bound" width="30%" />
            <p class="caption">
              Figure 18: Setting a minimum bound
            </p>
          </div>
          <p>With the cluster size at 5 and all other options back to their default value, (specifically, with 150
            kmeans++ initialization runs) we obtain
            the cluster result shown in the map in Figure <a href="#fig:minboundmap">19</a>. The categories have been
            re-ordered to show a similar color scheme to the unconstrained map in Figure <a href="#fig:kmeansmap5">5</a>
            (using <strong>CLc</strong> as the cluster variable).</p>
          <div class="figure" style="text-align: center"><span id="fig:minboundmap"></span>
            <img src="pics7b/7_076_minboundmap.png" alt="Cluster map with minimum bound constraint" width="60%" />
            <p class="caption">
              Figure 19: Cluster map with minimum bound constraint
            </p>
          </div>
          <p>Relative to the unconstrained map, we find that three clusters have increased in size (1 to 22, 2 to 19 and
            4 to 16) and two shrank (3 to 19 and 5 to 10). In addition, the configuration of the clusters changed
            slightly as well, with cluster 5 the most affected.</p>
          <p>The cluster characteristics show a slight deterioriation of our summary criterion,
            to a value of <strong>0.484033</strong> (compared to <strong>0.497772</strong>), as shown in
            Figure <a href="#fig:minboundresults">20</a>. This is the price to pay to
            satisfy the minimum population constraint. However, the effect on the individual clusters is mixed, with
            clusters 2 and 5 having a much worse WSS, whereas 1, 3 and 4 actually achieve a smaller WSS relative to the
            unconstrained version. This illustrates the complex trade-offs involved between the cases where the
            constraint has to be enforced compared to where it is already satisfied.</p>
          <div class="figure" style="text-align: center"><span id="fig:minboundresults"></span>
            <img src="pics7b/7_077_minboundsummary.png" alt="Cluster characteristics with minimum bound" width="50%" />
            <p class="caption">
              Figure 20: Cluster characteristics with minimum bound
            </p>
          </div>
        </div>
        <div id="elbow-plot" class="section level4 unnumbered" number="">
          <h4>Elbow plot</h4>
          <p>We now illustrate the use of an <em>elbow plot</em> to assess the best value for k. This plot is not
            included in the <code>GeoDa</code> functionality, but it is easy to construct from the output of a cluster
            analysis.</p>
          <p>The results for runs of k-means with all the default settings are given in Figure <a
              href="#fig:elbowdata">21</a>, for k ranging from 1 through 12. As the number of clusters increases, the
            resulting groupings become less and less informative and tend to yield several singleton clusters (with only
            one observation). Both WSS and BSS/TSS are listed, with the associated change as the value of k increases.
          </p>
          <div class="figure" style="text-align: center"><span id="fig:elbowdata"></span>
            <img src="pics7b/001_elbowdata.png" alt="Cluster characteristics for increasing values of k" width="50%" />
            <p class="caption">
              Figure 21: Cluster characteristics for increasing values of k
            </p>
          </div>
          <p>The actual elbow plots are given in Figure <a href="#fig:elbowplot">22</a>. There are two versions of this
            plot, one showing the decrease in WSS as k increases (left panel of the Figure), the other illustrating the
            increase in the ratio BST/SST with k (right panel). Unlike the usual textbook examples, in our case these
            graphs are not easy to interpret, still showing an albeit smaller improvement for <span
              class="math inline">\(k &gt; 10\)</span>, which is not a practical number of clusters for only 85
            observations. It does seem that a good value for k would be in the range 5-8, although this is by no means a
            hard and fast conclusion.</p>
          <p>This example illustrates the difficuly of deciding on the best k in practice. Considerations other than
            those offered by the simple elbow plot should be taken into account, such as how to interpret the groupings
            that result from the cluster analysis and whether the clusters are reasonably balanced. In some sense this
            is both an art and a science and it takes practice as well as familiarity with the substantive research
            questions to obtain a good insight into the trade-offs involved.</p>
          <div class="figure" style="text-align: center"><span id="fig:elbowplot"></span>
            <img src="pics7b/001_elbowplot.png" alt="Elbow plots" width="80%" />
            <p class="caption">
              Figure 22: Elbow plots
            </p>
          </div>
        </div>
      </div>
    </div>
    <div id="cluster-categories-as-variables" class="section level2 unnumbered" number="">
      <h2>Cluster Categories as Variables</h2>
      <p>Once added to the data table, the cluster categories can be visualized like any other integer variable.
        We already saw their use in the unique values map that is provided with the standard output. Two other
        particularly useful applications are as a conditioning variable in a conditional plot and as
        a variable to guide aggregation of observations to the cluster level.</p>
      <div id="conditional-plots" class="section level3 unnumbered" number="">
        <h3>Conditional plots</h3>
        <p>A particularly useful conditional plot is a conditional box plot, in which the distribution of a variable
          can be shown for each cluster. This supplements the summary characteristics of each cluster provided in the
          standard
          output, where only the mean is shown for each cluster category.</p>
        <p>The conditional box plot is invoked as <strong>Explore &gt; Conditional Plot &gt; Box Plot</strong>, or from
          the conditional plots icon on the toolbar, as in Figure <a href="#fig:condboxplot">23</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:condboxplot"></span>
          <img src="pics7b/44_condbox.png" alt="Conditional box plot option" width="10%" />
          <p class="caption">
            Figure 23: Conditional box plot option
          </p>
        </div>
        <p>For this particular application, it is most effective to only condition on one dimension, either horizontal
          or vertical.
          In the dialog shown in Figure <a href="#fig:condboxvars">24</a>, we selected the cluster variable
          <strong>CLa</strong> as the horizontal axis.
          We also selected the <em>empty</em> category for the vertical axis (this ensures that conditioning only
          happens in the
          horizontal dimension), and chose <strong>Litercy</strong> as the variable.</p>
        <div class="figure" style="text-align: center"><span id="fig:condboxvars"></span>
          <img src="pics7b/44_condbxsettings.png" alt="Conditional box plot variable selection" width="50%" />
          <p class="caption">
            Figure 24: Conditional box plot variable selection
          </p>
        </div>
        <p>The first plot that results is not very useful, since it takes a quantile
          distribution as the default for the horizontal axis. By right clicking, the options are brought up and we
          select
          <strong>Horizontal Bins Breaks &gt; Unique Values</strong>. The result is as in Figure <a
            href="#fig:condboxlitracy">25</a>.
        </p>
        <div class="figure" style="text-align: center"><span id="fig:condboxlitracy"></span>
          <img src="pics7b/44_literacybxplot.png" alt="Conditional box plot for literacy" width="70%" />
          <p class="caption">
            Figure 25: Conditional box plot for literacy
          </p>
        </div>
        <p>The box plots use the same scale, so that comparisons are easy to make. Note that even though the box plot
          fences are drawn,
          the actual range of the distribution in each cluster does not necessarily reach that far. In our example, the
          values
          for clusters 1-3 are well within these bounds. Only for cluster 5 do we observe an outlier.</p>
        <p>The plot clearly shows how
          the median value is much higher for clusters 2 and 3 relative to the others. It also shows little overlap of
          the interquartile
          range between the two groups (2 and 3, relative to 1-4-5). Conditional box plots can be a useful aide in the
          characterization
          of the different clusters in that they provide more than just a summary measure, such as a mean or median, but
          portray
          the whole distribution.</p>
      </div>
      <div id="aggregation-by-cluster" class="section level3 unnumbered" number="">
        <h3>Aggregation by cluster</h3>
        <p>With the clusters at hand, as defined for each observation by the category in the cluster
          field, we can now compute aggregate values for the new clusters and even
          create new layers for the dissolved units. We illustrate this
          as a quick check on the population totals we imposed in the bounded cluster procedure.</p>
        <p>There are two different ways to proceed. In one, we only create a new table, in the other, we end up with a
          new layers that has the original units spatially <em>dissolved</em> into the new clusters.</p>
        <p>The simple aggregation is invoked from the <strong>Table</strong> as an option by right-clicking. This
          brings up the list of options, from which we select <strong>Aggregate</strong>, as in
          Figure <a href="#fig:tabagg">26</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:tabagg"></span>
          <img src="pics7b/7_002_table_aggregate.png" alt="Table Aggregate option" width="20%" />
          <p class="caption">
            Figure 26: Table Aggregate option
          </p>
        </div>
        <p>The following dialog, shown in Figure <a href="#fig:tabaggpop">27</a>, provides the specific
          aggregation method, i.e., count, average, max, min, or <strong>Sum</strong>, the <strong>key</strong> on which
          to aggregate and a selection of variable to aggregate. In our example, we use the
          cluster field key, e.g., <strong>CLc</strong> (from the bounded clustering solution) and select only the
          population variable <strong>Pop1831</strong>,
          which we will sum over the departments that make up each cluster. This can be readily
          extended to multiple variables, as well as to different summary measures, such
          as the average.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
        <div class="figure" style="text-align: center"><span id="fig:tabaggpop"></span>
          <img src="pics7b/7_079_aggregate_vars.png" alt="Aggregation of total population by cluster" width="40%" />
          <p class="caption">
            Figure 27: Aggregation of total population by cluster
          </p>
        </div>
        <p>Pressing the <strong>Aggregate</strong> key brings up a dialog to select the file in which the new
          results will be saved. For example, we can select a <strong>dbf</strong> format and specify the
          file name. The contents of the new file are given in Figure <a href="#fig:clusterpop">28</a>,
          with the total population for each cluster. Clearly, each cluster meets the minimum
          requirement of 5179 that was specified.</p>
        <div class="figure" style="text-align: center"><span id="fig:clusterpop"></span>
          <img src="pics7b/000_dissolve_table.png" alt="Total population by cluster" width="30%" />
          <p class="caption">
            Figure 28: Total population by cluster
          </p>
        </div>
        <p>The same procedure can be used to create new values for any variable, aggregated
          to the new cluster scale. Note that since most variables in the Guerry data set are ratios, a simple sum would
          not be appropriate.</p>
        <p>A second way to create new aggregate spatial units is to invoke the <strong>Dissolve</strong> functionality
          from the <strong>Tools</strong> menu as in
          Figure <a href="#fig:clusterdissolve">29</a>. This will aggregate the variables in the same way as for the
          table, but also create a new spatial layer that <em>dissolves</em> the original units into their new aggregate
          clusters.</p>
        <div class="figure" style="text-align: center"><span id="fig:clusterdissolve"></span>
          <img src="pics7b/000_toolsdissolve.png" alt="Dissolve function" width="15%" />
          <p class="caption">
            Figure 29: Dissolve function
          </p>
        </div>
        <p>The next interface is the same as for the aggregate functionality, shown in
          Figure <a href="#fig:tabaggpop">27</a>. We again select <strong>CLc</strong> as the <em>key</em> to aggregate
          the spatial units and specify <strong>Pop1831</strong> as the variable to be summed into the new units. After
          saving the new layer, we can bring it back into GeoDa and create a unique values map of the population totals,
          as in Figure
          <a href="#fig:dissolvepoptot">30</a>.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> The
          table associated with this layer is identical to the
          one shown in Figure <a href="#fig:clusterpop">28</a>.
        </p>
        <div class="figure" style="text-align: center"><span id="fig:dissolvepoptot"></span>
          <img src="pics7b/000_dissolvemap.png" alt="Map of clusters with population totals" width="60%" />
          <p class="caption">
            Figure 30: Map of clusters with population totals
          </p>
        </div>
      </div>
    </div>
    <div id="clustering-with-dimension-reduction" class="section level2 unnumbered" number="">
      <h2>Clustering with Dimension Reduction</h2>
      <p>In practice, it is often more effective to carry out a clustering exercise after dimension reduction,
        instead of using the full set of variables. This is typically based on principal components, but can
        equally be computed from the MDS coordinates. Both approaches are illustrated below.</p>
      <div id="pca" class="section level3 unnumbered" number="">
        <h3>PCA</h3>
        <p>Instead of specifying all six variables, we carry out k-means clustering for the first two principal
          components,
          say the variables PC1 and PC2. Since the principal components are already standardized, we can set that option
          to <strong>Raw</strong>. The results for k=5 are shown in Figure <a href="#fig:pcaclustermap">31</a>, with the
          coordinates in a two-dimensional
          principal component scatter plot on the left, and the associated cluster map on the right. The clusters appear
          as clear <em>regions</em> in the scatter plot, highlighting the (multivariate) similarity among those
          observations in the
          same cluster.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
        <div class="figure" style="text-align: center"><span id="fig:pcaclustermap"></span>
          <img src="pics7b/33_pcaclustermap.png" alt="PCA plot and cluster map" width="80%" />
          <p class="caption">
            Figure 31: PCA plot and cluster map
          </p>
        </div>
        <p>A summary with the cluster centers, the associated WSS and summary statistics is given in Figure <a
            href="#fig:pcaclusterresult">32</a>.
          Strikingly, the between sum of squares to total sum of squares ratio is 0.755, by far the best cluster
          separation obtained for k=5 (and even better
          than the six-variable result for k=12). This illustrates the potential gain in efficiency obtained by reducing
          the full six variable
          dimensions to the principal components, which effectively summarize the multivariate characteristics of the
          data.</p>
        <div class="figure" style="text-align: center"><span id="fig:pcaclusterresult"></span>
          <img src="pics7b/33_pcaclusterresults.png" alt="PCA cluster summary" width="50%" />
          <p class="caption">
            Figure 32: PCA cluster summary
          </p>
        </div>
      </div>
      <div id="mds" class="section level3 unnumbered" number="">
        <h3>MDS</h3>
        <p>The same principle can be applied to the coordinates obtained from multidimensional scaling. While this is
          less used in practice,
          it is in fact equivalent to using principal components. We have already seen that for classic metric scaling
          the two-dimensional plots of the two approaches
          are equivalent, except for flipping the axes. This can also be seen from the results for k=5 shown in Figure
          <a href="#fig:mdsclustermap">33</a>.
          The variables used in the clustering exercise are the MDS coordinates V1 and V2 (again, with the
          standardization option set to <strong>Raw</strong>).
          The clusters are identical to those obtained for the first two principal components. The scatter plot is
          flipped, but otherwise contains
          the same information as before.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdsclustermap"></span>
          <img src="pics7b/33_mdsclustermap.png" alt="MDS plot and cluster map" width="80%" />
          <p class="caption">
            Figure 33: MDS plot and cluster map
          </p>
        </div>
        <p>Similarly, the summary results are the same, as shown in Figure <a href="#fig:mdsclusterresult">34</a>,
          except that the signs of the cluster
          centers for V2 are the opposite of those for PC2 in Figure <a href="#fig:pcaclusterresult">32</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdsclusterresult"></span>
          <img src="pics7b/33_mdsclusterresults.png" alt="MDS cluster summary" width="50%" />
          <p class="caption">
            Figure 34: MDS cluster summary
          </p>
        </div>
      </div>
    </div>
    <div id="appendix" class="section level2 unnumbered" number="">
      <h2>Appendix</h2>
      <div id="equivalence-of-euclidean-distances-and-sum-of-squared-errors-sse" class="section level3 unnumbered"
        number="">
        <h3>Equivalence of Euclidean distances and Sum of Squared Errors (SSE)</h3>
        <p>We start with the objective function as the within sum of squared distances. More precisely,
          this is expressed as one half the sum of the squared Euclidean distances
          for each cluster between all the pairs <span class="math inline">\(i-j\)</span> that form part of the cluster
          <span class="math inline">\(h\)</span>, i.e., for all <span class="math inline">\(i \in h\)</span>
          and <span class="math inline">\(j \in h\)</span>:
          <span class="math display">\[W = (1/2) \sum_{h=1}^k \sum_{i \in h} \sum_{j \in h} ||x_i - x_j||^2.\]</span>
          To keep things simple, we take the <span class="math inline">\(x\)</span> as univariate, so we can ignore the
          more complex notation needed for higher order manipulations. This is without a loss of generality, since for
          multiple variables the Euclidean distance is simply the
          sum of the distances for each of the variables.
        </p>
        <p>A fundamental identity exploited to move from the
          sum of all pairwised distances to the sum of squared deviations is that:
          <span class="math display">\[(1/2) \sum_i \sum_j (x_i - x_j)^2 = n \sum_i (x_i - \bar{x})^2,\]</span>
          where <span class="math inline">\(\bar{x}\)</span> is the mean of <span class="math inline">\(x\)</span>. This
          takes a little algebra to confirm.
        </p>
        <p>For each <span class="math inline">\(i\)</span> in a given cluster <span class="math inline">\(h\)</span>,
          the sum over the <span class="math inline">\(j\)</span> in the cluster is:
          <span class="math display">\[\sum_{j \in h} (x_i - x_j)^2 = \sum_{j \in h} (x_i^2 - 2x_ix_j + x_j^2),\]</span>
          or (dropping the <span class="math inline">\(\in h\)</span> notation for simplicity),
          <span class="math display">\[\sum_j x_i^2 - 2\sum_j x_i x_j + \sum_j x_j^2.\]</span>
        </p>
        <p>With <span class="math inline">\(n_h\)</span> observations in cluster <span class="math inline">\(h\)</span>,
          <span class="math inline">\(\sum_j x_i^2 = n_h x_i^2\)</span> (since <span class="math inline">\(x_i\)</span>
          does not contain the
          index <span class="math inline">\(j\)</span> and thus is simply repeated <span
            class="math inline">\(n_h\)</span> times). Also, <span class="math inline">\(\sum_j x_j = n_h
            \bar{x}_h\)</span>,
          where <span class="math inline">\(\bar{x}_h\)</span> is the mean of <span class="math inline">\(x\)</span> for
          cluster <span class="math inline">\(h\)</span> (<span class="math inline">\(\bar{x}_h = \sum_j x_j / n_h =
            \sum_i x_i / n_h\)</span>).
          As a result, <span class="math inline">\(- 2 x_i \sum_j x_j = - 2 n_h x_i \bar{x}_h\)</span>, which yields:
          <span class="math display">\[\sum_{j} (x_i - x_j)^2 = n_h x_i^2 - 2 n_h x_i \bar{x}_h + \sum_j x_j^2.\]</span>
          Next, we proceed in the same way to compute the sum over all the <span class="math inline">\(i \in h\)</span>:
          <span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = n_h \sum_i x_i^2 - 2 n_h \bar{x}_h \sum_i x_i +
            \sum_i \sum_j x_j^2\]</span>
          Using the same approach as before, and since <span class="math inline">\(\sum_j x_j^2 = \sum_i x_i^2\)</span>,
          this becomes:
          <span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = n_h \sum_i x_i^2 - 2 n_h^2 \bar{x}_h^2 + n_h
            \sum_i x_i^2 = 2 n_h \sum_i x_i^2 - 2 n_h^2 \bar{x}_h^2,\]</span>
          or,
          <span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = 2n_h^2 (\sum_i x_i^2 / n_h -
            \bar{x}_h^2).\]</span>
          From the definition of the variance, we know that the following equality holds:
          <span class="math display">\[\sigma^2 = (1/n_h) \sum_i (x_i - \bar{x}_h)^2 = \sum_i x_i^2 / n_h -
            \bar{x}_h^2.\]</span>
          Therefore:
          <span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = 2n_h^2 [(1/n_h) \sum_i (x_i - \bar{x}_h)^2] =
            2n_h [ \sum_i (x_i - \bar{x}_h)^2 ],\]</span>
          which gives the contribution of cluster <span class="math inline">\(h\)</span> to the objective function as:
          <span class="math display">\[(1/2) \sum_i \sum_{j} (x_i - x_j)^2 = n_h [ \sum_i (x_i - \bar{x}_h)^2
            ].\]</span>
        </p>
        <p>For all the clusters jointly, the objective becomes:
          <span class="math display">\[\sum_{h=1}^k n_h \sum_{i \in h} (x_i - \bar{x}_h)^2.\]</span>
        </p>
        <p>In other words, minimizing the sum of (one half) of all squared distances is equivalent to minimizing
          the sum of squared deviations from the mean in each cluster, the within sum of squared errors.</p>
      </div>
      <div id="k-means-worked-example" class="section level3 unnumbered" number="">
        <h3>K-means worked example</h3>
        <p>We now provide the details behind the k-means algorithm portrayed in
          Figures <a href="#fig:kmexample">1</a> and <a href="#fig:kmsteps">2</a>. The coordinates of the seven points
          are given in the X and Y columns of Figure <a href="#fig:kmeansex1">35</a>. The column labeled as SSE shows
          the squared distance from each point to the center of the point cloud, computed as the average of the X and Y
          coordinates (X=5.714, Y=5.143).<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> The sum of
          the squared distances constitutes the total sum of squared errors, or TSS, which equals 62.286 in this
          example. This value does not change as we iterate, since it pertains to all the observations taken together
          and ignores the cluster allocations.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansex1"></span>
          <img src="pics7b/01_coordinates.png" alt="Worked example - basic data" width="35%" />
          <p class="caption">
            Figure 35: Worked example - basic data
          </p>
        </div>
        <p>The initialization consists of randomly picking two observations as starting points, one for each potential
          cluster. In this example, we take observations 4 and 7. Next, we compute the distance from each point to the
          two <em>seeds</em> and allocate each to the closest seed. The highlighted values in columns d_i4 and d_i7 in
          Figure <a href="#fig:kmeansd1">36</a> show how the first allocation consists of five observations in cluster 1
          (1-5) and two observations in cluster 2 (6 and 7).</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansd1"></span>
          <img src="pics7b/02_disttoseed.png" alt="Squared distance to seeds" width="25%" />
          <p class="caption">
            Figure 36: Squared distance to seeds
          </p>
        </div>
        <p>Next, we compute a central point for each cluster as the average of the respective X and Y coordinates. The
          SSE follows as the squared distance between each observation in the cluster and the central point, as listed
          in
          Figure <a href="#fig:kmeanstep1a">37</a>. The sum of the total SSE in each cluster is the <em>within</em> sum
          of squares, WSS = 30.9 in this first step. Consequently, the <em>between</em> sum of squares BSS = TSS - WSS =
          62.3 - 30.9 = 31.4. The associated ratio BSS/TSS, which is an indicator of the quality of the cluster is 0.50.
        </p>
        <div class="figure" style="text-align: center"><span id="fig:kmeanstep1a"></span>
          <img src="pics7b/02_step1_centers.png" alt="Step 1 - Summary characteristics" width="80%" />
          <p class="caption">
            Figure 37: Step 1 - Summary characteristics
          </p>
        </div>
        <p>Next, we take the new cluster center points and again allocate each observation to the closest center. As
          shown in Figure <a href="#fig:kmeansd2">38</a>, this results in observation 5 moving from cluster 1 to cluster
          2, which now consists of three observations (cluster 1 has the remaining four).</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansd2"></span>
          <img src="pics7b/02_dist_to_1.png" alt="Squared distance to Step 1 centers" width="25%" />
          <p class="caption">
            Figure 38: Squared distance to Step 1 centers
          </p>
        </div>
        <p>We compute the new cluster centers and calculate the associated SSE. As suggested by Figure <a
            href="#fig:kmeanstep2a">39</a>, this results in a new WSS of 22.667, clearly an improvement of the objective
          function. The corresponding ratio of BSS/TSS becomes 0.64, also a clear improvement.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeanstep2a"></span>
          <img src="pics7b/02_step2_centers.png" alt="Step 2 - Summary characteristics" width="80%" />
          <p class="caption">
            Figure 39: Step 2 - Summary characteristics
          </p>
        </div>
        <p>We repeat the process with the updated centers, which results in observation 4 moving from cluster 1 to
          cluster 2, as illustrated in Figure <a href="#fig:kmeansd3">40</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansd3"></span>
          <img src="pics7b/02_dist_to_2.png" alt="Squared distance to Step 2 centers" width="25%" />
          <p class="caption">
            Figure 40: Squared distance to Step 2 centers
          </p>
        </div>
        <p>Figure <a href="#fig:kmeanstep3a">41</a> shows the new cluster centers and associated SSE. The updated WSS is
          15.333, yielding a ratio BSS/TSS of 0.75.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeanstep3a"></span>
          <img src="pics7b/02_step3_centers.png" alt="Step 3 - Summary characteristics" width="80%" />
          <p class="caption">
            Figure 41: Step 3 - Summary characteristics
          </p>
        </div>
        <p>Finally, this latest allocation no longer results in a change, as shown
          Figure <a href="#fig:kmeansd4">42</a>, so that we can conclude that we reached a local optimum.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansd4"></span>
          <img src="pics7b/02_dist_to_3.png" alt="Squared distance to Step 3 centers" width="25%" />
          <p class="caption">
            Figure 42: Squared distance to Step 3 centers
          </p>
        </div>
        <p>Figure <a href="#fig:kmexgeoda">43</a> demonstrates that this is the same result as obtained by
          <code>GeoDa</code> with the default settings, but the <strong>Transformation</strong> set to
          <strong>Raw</strong> in order to use the actual coordinates.<a href="#fn12" class="footnote-ref"
            id="fnref12"><sup>12</sup></a></p>
        <div class="figure" style="text-align: center"><span id="fig:kmexgeoda"></span>
          <img src="pics7b/02_geoda.png" alt="K-means results from GeoDa" width="55%" />
          <p class="caption">
            Figure 43: K-means results from GeoDa
          </p>
        </div>
        <p><br></p>
      </div>
    </div>
    <div id="references" class="section level2 unnumbered" number="">
      <h2>References</h2>
      <div id="refs" class="references hanging-indent">
        <div id="ref-ArthurVassilvitskii:07">
          <p>Arthur, David, and Sergei Vassilvitskii. 2007. “k-means++: The Advantages of Careful Seeding.” In <em>SODA
              07, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</em>, edited by Harold
            Gabow, 1027–35. Philadelphia, PA: Society for Industrial and Applied Mathematics.</p>
        </div>
        <div id="ref-Dempsteretal:77">
          <p>Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM
            Algorithm.” <em>Journal of the Royal Statistical Society, Series B</em> 39: 1–38.</p>
        </div>
        <div id="ref-Everittetal:11">
          <p>Everitt, Brian S., Sabine Landau, Morven Leese, and Daniel Stahl. 2011. <em>Cluster Analysis, 5th
              Edition</em>. New York, NY: John Wiley.</p>
        </div>
        <div id="ref-FrantiSieranoja:18">
          <p>Fränti, Pasi, and Sami Sieranoja. 2018. “K-Means Properties on Six Clustering Benchmark Datasets.”
            <em>Applied Intelligence</em> 48: 4743–59.</p>
        </div>
        <div id="ref-Hanetal:12">
          <p>Han, Jiawei, Micheline Kamber, and Jian Pei. 2012. <em>Data Mining (Third Edition)</em>. Amsterdam:
            MorganKaufman.</p>
        </div>
        <div id="ref-HartiganWong:79">
          <p>Hartigan, J. A., and M. A. Wong. 1979. “Algorithm AS 136: A k-means Clustering Algorithm.” <em>Applied
              Statistics</em> 28: 100–108.</p>
        </div>
        <div id="ref-Hartigan:72">
          <p>Hartigan, John A. 1972. “Direct Clustering of a Data Matrix.” <em>Journal of the American Statistical
              Association</em> 67: 123–29.</p>
        </div>
        <div id="ref-Hartigan:75">
          <p>———. 1975. <em>Clustering Algorithms</em>. New York, NY: John Wiley.</p>
        </div>
        <div id="ref-Hastieetal:09">
          <p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning (2nd
              Edition)</em>. New York, NY: Springer.</p>
        </div>
        <div id="ref-deHoonetal:17">
          <p>Hoon, Michiel de, Seiya Imoto, and Satoru Miyano. 2017. “The C Clustering Library.” Tokyo, Japan: The
            University of Tokyo, Institute of Medical Science, Human Genome Center.</p>
        </div>
        <div id="ref-Jain:10">
          <p>Jain, Anil K. 2010. “Data Clustering: 50 Years Beyond K-Means.” <em>Pattern Recognition Letters</em> 31
            (8): 651–66.</p>
        </div>
        <div id="ref-JainDubes:88">
          <p>Jain, Anil K., and Richard C. Dubes. 1988. <em>Algorithms for Clustering Data</em>. Englewood Cliffs, NJ:
            Prentice Hall.</p>
        </div>
        <div id="ref-Jamesetal:13">
          <p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to
              Statistical Learning, with Applications in R</em>. New York, NY: Springer-Verlag.</p>
        </div>
        <div id="ref-KaufmanRousseeuw:05">
          <p>Kaufman, L., and P. Rousseeuw. 2005. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>.
            New York, NY: John Wiley.</p>
        </div>
        <div id="ref-Lloyd:82">
          <p>Lloyd, Stuart P. 1982. “Least Squares Quantization in PCM.” <em>IEEE Transactions on Information
              Theory</em> 28: 129–36.</p>
        </div>
        <div id="ref-PadilhaCampello:17">
          <p>Padilha, Victor A., and Ricardo J. G. B. Campello. 2017. “A Systematic Comparative Evaluation of
            Biclustering Techniques.” <em>BMC Bioinformatics</em> 18: 55. <a
              href="https://doi.org/10.1186/s12859-017-1487-1">https://doi.org/10.1186/s12859-017-1487-1</a>.</p>
        </div>
        <div id="ref-Tanayetal:04">
          <p>Tanay, Amos, Roded Sharan, and Ron Shamir. 2004. “Biclustering Algorithms: A Survey.” In <em>Handbook of
              Computational Molecular Biology</em>, edited by Srinivas Aluru, 26–21–17. Boca Raton, FL: Chapman &amp;
            Hall/CRC.</p>
        </div>
        <div id="ref-Tibshiranietal:01">
          <p>Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data
            Set via the Gap Statistic.” <em>Journal of the Royal Statistical Society, Series B</em> 63: 411–23.</p>
        </div>
      </div>
    </div>
    <div class="footnotes">
      <hr />
      <ol>
        <li id="fn1">
          <p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu"
              class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn2">
          <p>While we discuss dimension reduction and clustering separately, so-called <em>biclustering</em> techniques
            group both variables and observations simultaneously. While old, going back to an article by <span
              class="citation">Hartigan (<a href="#ref-Hartigan:72" role="doc-biblioref">1972</a>)</span>, these
            techniques have gained a lot of interest more recently in the field of gene expression analysis. For
            overviews, see, e.g., <span class="citation">Tanay, Sharan, and Shamir (<a href="#ref-Tanayetal:04"
                role="doc-biblioref">2004</a>)</span> and <span class="citation">Padilha and Campello (<a
                href="#ref-PadilhaCampello:17" role="doc-biblioref">2017</a>)</span>.<a href="#fnref2"
              class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn3">
          <p>The discussion in this section is loosely based on part of
            the presentation in <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-Hastieetal:09"
                role="doc-biblioref">2009</a>)</span> Chapter 14.<a href="#fnref3" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn4">
          <p>Since each pair is counted twice, the total sum is divided by 2. While this seems arbitrary at this point,
            it helps later on when we establish the equivalence between the Euclidean distances and the sum of squared
            errors.<a href="#fnref4" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn5">
          <p>As shown in <span class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12"
                role="doc-biblioref">2012</a>)</span>, p. 505, the k-means algorithm can be considered to be a special
            case of the so-called EM (expectation-maximization) algorithm of <span class="citation">Dempster, Laird, and
              Rubin (<a href="#ref-Dempsteretal:77" role="doc-biblioref">1977</a>)</span>. The expectation step consists
            of allocating each observation to its nearest cluster center, and the maximization step is the recalculation
            of those cluster centers for each new layout.<a href="#fnref5" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn6">
          <p>The drop-down list goes from 2 to 85,
            which may be insufficient in <em>big data</em> settings. Hence <code>GeoDa</code> now offers the option to
            enter
            a value directly.<a href="#fnref6" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn7">
          <p>The Z standardization subtracts the mean and divides by the
            standard deviation. Alternative standardizations are to use the mean absolute
            deviation, MAD, the range adjust or range standardization methods.<a href="#fnref7"
              class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn8">
          <p>In the current implementation, the same summary method needs to
            be applied to all the variables.<a href="#fnref8" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn9">
          <p>The categories have been adjusted to keep the same color pattern as with the cluster map in Figure <a
              href="#fig:minboundmap">19</a>.<a href="#fnref9" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn10">
          <p>For a similar logic, see for example Chapter 2 in <span class="citation">Everitt et al. (<a
                href="#ref-Everittetal:11" role="doc-biblioref">2011</a>)</span>, where a <em>visual inspection</em> of
            two-dimensional
            scatter plots of principal components is illustrated as way to identify clusters.<a href="#fnref10"
              class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn11">
          <p>Each squared distance is <span class="math inline">\((x_i - \bar{x})^2 + (y_i - \bar{y})^2\)</span>.<a
              href="#fnref11" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn12">
          <p>This is accomplished by loading a csv file with the coordinates as the data input in <code>GeoDa</code>.<a
              href="#fnref12" class="footnote-back">↩︎</a></p>
        </li>
      </ol>
    </div>


  </section>


  <!-- code folding -->


  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>

</html>