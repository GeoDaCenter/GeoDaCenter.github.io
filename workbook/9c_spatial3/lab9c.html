<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Luc Anselin" />


<title>Spatial Clustering (2)</title>

<script src="lab9c_files/header-attrs-2.3/header-attrs.js"></script>
<link href="lab9c_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lab9c_files/highlightjs-9.12.0/highlight.js"></script>
    <title>GeoDa on Github</title>

    <style>
    	*{margin:0;padding:0;}
	    .shadowfilter {
	       -webkit-filter: drop-shadow(12px 12px 7px rgba(0,0,0,0.5));
	        filter: url(shadow.svg#drop-shadow);
	     }
	     .intro1 { margin-left: -45px;}
    </style>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
    <style>
    ul {padding-left:30px;}
	figcaption {
	  top: .70em;
   	  left: .35em;
 	  bottom: auto!important;
	  right: auto!important;
	}
    </style>

        <style>
    h1 {
        text-align: center;
    }
    h3.subtitle {
        text-align: center;
    }
    h4.author {
        text-align: center;
    }
    h4.date {
        text-align: center;
    }
    p.caption {
        font-size : 12px;
    }
    </style>

<!-- Google Analytics -->
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-72724100-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<!-- End Google Analytics -->
<!-- Google Tag Manager -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-53RVF8"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-53RVF8');</script>
<!-- End Google Tag Manager -->

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








</head>

<body>


    <section class="page-header">
      <h1 class="project-name">GeoDa</h1>
      <h2 class="project-tagline">An Introduction to Spatial Data Science</h2>
      <a href="https://geodacenter.github.io/index.html" class="btn">Homepage</a>
      <a href="https://geodacenter.github.io/download.html" class="btn">Download</a>
      <a href="https://github.com/GeoDaCenter/geoda/" class="btn">View on GitHub</a>
      <a href="https://spatial.uchicago.edu/sample-data"  target="_blank" class="btn">Data</a>
       <a href="https://geodacenter.github.io/documentation.html" class="btn">Documentation</a>
       <a href="https://geodacenter.github.io/support.html" class="btn">Support</a>
       <a href="https://geodacenter.github.io/index-cn.html" class="btn">中文</a>
    </section>

    <section class="main-content">


<h1 class="title toc-ignore">Spatial Clustering (2)</h1>
<h3 class="subtitle">Spatially Constrained Clustering - Hierarchical Methods</h3>
<h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
<h4 class="date">11/21/2020 (latest update)</h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#objectives">Objectives</a>
<ul>
<li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
</ul></li>
<li><a href="#preliminaries">Preliminaries</a></li>
</ul></li>
<li><a href="#spatially-constrained-hierarchical-clustering-schc">Spatially Constrained Hierarchical Clustering (SCHC)</a>
<ul>
<li><a href="#principle">Principle</a></li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#wards-linkage">Ward’s linkage</a></li>
<li><a href="#single-linkage">Single linkage</a></li>
<li><a href="#complete-linkage">Complete linkage</a></li>
<li><a href="#average-linkage">Average linkage</a></li>
</ul></li>
</ul></li>
<li><a href="#skater">SKATER</a>
<ul>
<li><a href="#principle-1">Principle</a></li>
<li><a href="#implementation-1">Implementation</a>
<ul>
<li><a href="#saving-the-minimum-spanning-tree">Saving the minimum spanning tree</a></li>
<li><a href="#setting-a-minimum-cluster-size">Setting a minimum cluster size</a></li>
</ul></li>
</ul></li>
<li><a href="#redcap">REDCAP</a>
<ul>
<li><a href="#principle-2">Principle</a></li>
<li><a href="#implementation-2">Implementation</a>
<ul>
<li><a href="#full-order-wards-linkage">Full-Order Ward’s linkage</a></li>
<li><a href="#full-order-average-linkage">Full-Order Average linkage</a></li>
<li><a href="#full-order-complete-linkage">Full-Order Complete linkage</a></li>
<li><a href="#full-order-single-linkage">Full-Order Single linkage</a></li>
</ul></li>
</ul></li>
<li><a href="#assessment">Assessment</a></li>
<li><a href="#appendix">Appendix</a>
<ul>
<li><a href="#arizona-counties-sample-data-set">Arizona counties sample data set</a></li>
<li><a href="#schc-complete-linkage-worked-example">SCHC Complete Linkage worked example</a></li>
<li><a href="#skater-worked-example">SKATER worked example</a></li>
<li><a href="#redcap-worked-example">REDCAP worked example</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p><br></p>
<div id="introduction" class="section level2 unnumbered" number="">
<h2>Introduction</h2>
<p>We now move our focus to methods that impose contiguity as a <em>hard</em> constraint in a clustering procedure.
Such methods are known under a number of different terms, including <em>zonation</em>, <em>districting</em>, <em>regionalization</em>, <em>spatially constrained clustering</em>,
and the <em>p-region problem</em>. They are concerned with dividing
an original set of <em>n</em> spatial units
into <em>p</em> internally connected regions that maximize within similarity
<span class="citation">(for recent reviews, see, e.g., Murray and Grubesic <a href="#ref-MurrayGrubesic:02" role="doc-biblioref">2002</a>; Duque, Ramos, and Suriñach <a href="#ref-Duqueetal:07" role="doc-biblioref">2007</a>; Duque, Church, and Middleton <a href="#ref-Duqueatal:11a" role="doc-biblioref">2011</a>)</span>.</p>
<p>In the previous chapter, we covered approaches that impose a <em>soft</em> constraint, in the form of a trade-off between
attribute similarity and spatial similarity.
In the methods considered in the current and next chapter, the contiguity
is a strict constraint, in that clusters can only consist of entities that are geographically connected.
As a result, in
some cases the resulting attribute similarity may be of poor quality, when dissimilar units are grouped together
primarily due to the contiguity constraint.</p>
<p>We consider three sets of methods. We start by introducing spatial constraints into an agglomerative hierarchical
clustering procedure, following the approach reviewed in <span class="citation">Murtagh (<a href="#ref-Murtagh:85" role="doc-biblioref">1985</a>)</span> and <span class="citation">Gordon (<a href="#ref-Gordon:96" role="doc-biblioref">1996</a>)</span>, among others. Next, we outline two common algorithms,
i.e., <em>SKATER</em> <span class="citation">(Assunção et al. <a href="#ref-Assuncaoetal:06" role="doc-biblioref">2006</a>)</span> and <em>REDCAP</em> <span class="citation">(Guo <a href="#ref-Guo:08" role="doc-biblioref">2008</a>; Guo and Wang <a href="#ref-GuoWang:11" role="doc-biblioref">2011</a>)</span>. SKATER stands for
<em>Spatial `K’luster Analysis by Tree Edge Removal</em>, and obtains regionalization through a graph partitioning approach. REDCAP stands for
<em>REgionalization with Dynamically Constrained Agglomerative clustering and Partitioning</em>, and consists of a family
of six hierarchical regionalization methods.</p>
<p>As before, the methods considered here share many of the same options with previously discussed techniques as implemented
in <code>GeoDa</code>. Common options will not be considered, but the focus will be on aspects that are specific to the spatial
perspective.</p>
<p>To illustrate these methods, we continue to use the Guerry data set.</p>
<div id="objectives" class="section level3 unnumbered" number="">
<h3>Objectives</h3>
<ul>
<li><p>Understand how imposing spatial constraints affects hierarchical agglomerative clustering</p></li>
<li><p>Understand the tree partioning method underlying the SKATER algorithm</p></li>
<li><p>Identify contiguous clusters by means of the SKATER algorithm</p></li>
<li><p>Understand the connection between SCHC, SKATER and the REDCAP family of methods</p></li>
<li><p>Identify contiguous clusters by means of the REDCAP algorithm</p></li>
</ul>
<div id="geoda-functions-covered" class="section level4 unnumbered" number="">
<h4>GeoDa functions covered</h4>
<ul>
<li>Clusters &gt; SCHC</li>
<li>Clusters &gt; skater
<ul>
<li>set minimum bound variable</li>
<li>set minimum cluster size</li>
</ul></li>
<li>Clusters &gt; redcap</li>
</ul>
<p><br></p>
</div>
</div>
<div id="preliminaries" class="section level3 unnumbered" number="">
<h3>Preliminaries</h3>
<p>We continue to use the Guerry data set and also need a spatial weights matrix in the form
of queen contiguity.</p>
</div>
</div>
<div id="spatially-constrained-hierarchical-clustering-schc" class="section level2 unnumbered" number="">
<h2>Spatially Constrained Hierarchical Clustering (SCHC)</h2>
<div id="principle" class="section level3 unnumbered" number="">
<h3>Principle</h3>
<p>Spatially constrained hierarchical clustering is a special form of constrained clustering, where the
constraint is based on contiguity (common borders). We have earlier seen how a minimum size constraint
can be imposed on classic clustering algorithms. The spatial constraint is more complex in that
it directly affects the way in which elemental units can be merged into larger entities.</p>
<p>The idea of including contiguity constraints into agglomerative hierarchical clustering goes back
a long time, with early overviews of the principles involved in <span class="citation">Lankford (<a href="#ref-Lankford:69" role="doc-biblioref">1969</a>)</span>, <span class="citation">Murtagh (<a href="#ref-Murtagh:85" role="doc-biblioref">1985</a>)</span> and
<span class="citation">Gordon (<a href="#ref-Gordon:96" role="doc-biblioref">1996</a>)</span>. Recent software implementations can be found in <span class="citation">Guo (<a href="#ref-Guo:09" role="doc-biblioref">2009</a>)</span> and <span class="citation">Recchia (<a href="#ref-Recchia:10" role="doc-biblioref">2010</a>)</span>.</p>
<p>The clustering logic is identical to that of unconstrained hierarchical clustering, and the same
expressions are used for linkage and updating formulas, i.e., single linkage, complete linkage,
average linkage, and Ward’s method (we refer to the relevant chapter for details). The only difference is that now a contiguity constraint is
imposed.</p>
<p>More specifically, two entities <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are merged when the dissimilarity measure <span class="math inline">\(d_{ij}\)</span> is the smallest among
all pairs of entities, subject to <span class="math inline">\(w_{ij} = 1\)</span> (i.e., a non-zero spatial weight). In other words, merger is only carried out
when the corresponding entry in the spatial weights (contiguity) matrix is non-zero. Another way
to phrase this is that the minimum of the dissimilarity measure is only searched for those pairs of observations
that are contiguous.</p>
<p>As
before, the dissimilarity measure is updated for the newly merged unit using the appropriate formula.
In addition, the weights matrix needs to be updated to reflect the contiguity structure
of the newly merged units.</p>
<p>Consider this more closely. In the first step, the weights matrix is of dimension <span class="math inline">\(n \times n\)</span>. If
observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are merged into a new entity, say <span class="math inline">\(A\)</span>, then the resulting matrix will be of
dimension <span class="math inline">\((n - 1) \times (n-1)\)</span> and the two original rows/columns for <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> will be replaced
by a new row/column for <span class="math inline">\(A\)</span>. For the new matrix, the row elements <span class="math inline">\(w_{Ah} = 1\)</span> if either <span class="math inline">\(w_{ih} = 1\)</span> <em>or</em> <span class="math inline">\(w_{jh} = 1\)</span>
(or both are non-zero),
and similarly for the column elements.</p>
<p>The next step thus consists of a new dissimilarity matrix and new contiguity matrix, both of
dimension <span class="math inline">\((n - 1) \times (n-1)\)</span>. At this point the process repeats itself. As for other hierarchical
clustering methods, the end result is a single cluster that contains all the observations. The process
of merging consecutive entities is graphically represented in an <em>dendrogram</em>, as before. A fully
worked example is included in the <a href="#appendix">Appendix</a>.</p>
<p>One potential complication is so-called inversion, when the dissimilarity criterion for the
newly merged unit with respect to remaining observations is <em>better</em> than for the merged
units themselves. This link reversal occurs when <span class="math inline">\(d_{i \cup j, k} \lt d_{ij}\)</span>. It is only avoided
in the complete linkage case <span class="citation">(for details, see Murtagh <a href="#ref-Murtagh:85" role="doc-biblioref">1985</a>; Gordon <a href="#ref-Gordon:96" role="doc-biblioref">1996</a>)</span>. This problem is primarily one
of interpretation and does not preclude the clustering methods from being applied.</p>
</div>
<div id="implementation" class="section level3 unnumbered" number="">
<h3>Implementation</h3>
<p>Spatially constrained hierarchical clustering is invoked as the first item in the corresponding group
in the list associated with the <strong>Clusters</strong> item on the toolbar, as in Figure <a href="#fig:schc">1</a>, or from the
menu, as <strong>Clusters &gt; SCHC</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:schc"></span>
<img src="pics9c/00_schc.png" alt="SCHC cluster option" width="10%" />
<p class="caption">
Figure 1: SCHC cluster option
</p>
</div>
<div id="wards-linkage" class="section level4 unnumbered" number="">
<h4>Ward’s linkage</h4>
<p>The menu brings up the usual cluster variable interface with the same general structure as for the unconstrained
hierarchical clustering. The variable selection is carried out in the left-hand panel of Figure <a href="#fig:schcvars">2</a>.
We choose the same six variables as before, <strong>Crm_prs</strong>, <strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>, <strong>Infants</strong> and <strong>Suicids</strong>.
We use the variables in z-standardized form. In addition, we need to select a <strong>Spatial Weight</strong>, here set to queen
contiguity in the <strong>guerry_85_q</strong> file. The default methods is <strong>Ward’s-linkage</strong>. As before, we also set the variable to which to save the cluster indicator, here <strong>CLw</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcvars"></span>
<img src="pics9c/44_schc_variables.png" alt="SCHC variable selection and initial run" width="60%" />
<p class="caption">
Figure 2: SCHC variable selection and initial run
</p>
</div>
<p>The algorithm proceeds in two steps. Note that the <strong>Number of Clusters</strong> initially is set to 2, which
we change to <strong>6</strong>. Clicking on <strong>Run</strong>
generates the dendrogram, shown in the right-hand panel of Figure <a href="#fig:schcvars">2</a>. We can move the <em>cut</em> line (dashed red line) in the dendrogram to the desired number of clusters if it is different from
the <strong>Number of Clusters</strong> in the interface. At this point, selecting <strong>Save/Show Map</strong> generates the cluster map shown in Figure <a href="#fig:schcwardmap">3</a>.</p>
<p>We notice two well-balanced clusters of 27 departments, one with 21, one smaller one with 8 observations, and two singletons. As required, all clusters consist of contiguous spatial units.</p>
<div class="figure" style="text-align: center"><span id="fig:schcwardmap"></span>
<img src="pics9c/44_schc_ward_map.png" alt="SCHC Ward method cluster map, k=6" width="60%" />
<p class="caption">
Figure 3: SCHC Ward method cluster map, k=6
</p>
</div>
<p>The <strong>Summary</strong> button in the right-hand panel of Figure <a href="#fig:schcvars">2</a> brings up the usual summary
characteristics, shown in Figure <a href="#fig:schcwardsummary">4</a>. The usual descriptors provide the cluster centers
(in the original units), the within-cluster sum of squares for each cluster as well as the overall within and
between cluster sum or squares. The ratio of between to total sum of squares amounts to 0.461812.</p>
<div class="figure" style="text-align: center"><span id="fig:schcwardsummary"></span>
<img src="pics9c/44_schc_ward_summary.png" alt="SCHC Ward method cluster characteristics, k=6" width="40%" />
<p class="caption">
Figure 4: SCHC Ward method cluster characteristics, k=6
</p>
</div>
<p>In order to put this into perspective, we show the results of the unconstrained hierarchical clustering routine
with Ward’s linkage in Figure <a href="#fig:hierwardresults">5</a>. The spatial pattern is quite different, with the larger
clusters from Figure <a href="#fig:schcwardmap">3</a> split into the elements of several smaller clusters. There are no singletons,
and the clusters range from 4 to 25 spatial units. Because the algorithm is unconstrained, the results for the
ratio of between to total sum of squares is much better, at 0.532476. The difference with the constrained version
gives a measure of the price to pay in order to achieve contiguity.</p>
<div class="figure" style="text-align: center"><span id="fig:hierwardresults"></span>
<img src="pics9c/00_hierarchical_ward_6.png" alt="Ward method unconstrained hierarchical clustering, k=6" width="80%" />
<p class="caption">
Figure 5: Ward method unconstrained hierarchical clustering, k=6
</p>
</div>
</div>
<div id="single-linkage" class="section level4 unnumbered" number="">
<h4>Single linkage</h4>
<p>As in the unconstrained case, the Ward linkage methods is the default because it tends to yield compact
clusters. As we saw earlier, the other linkage tend to result in a lot of unbalanced clusters and yield
several singletons. The same is the case in the spatially constrained algorithms.</p>
<p>For completeness sake, we provide the results for the other three linkage methods, single linkage first.</p>
<p>The cluster map and summary characteristics for single linkage are shown in Figure <a href="#fig:schcsingle">6</a>. We
obtain one large cluster with 80 observations and five singletons. The between to total ratio is 0.236071. These
results are pretty dismal relative to the Ward linkage method.</p>
<div class="figure" style="text-align: center"><span id="fig:schcsingle"></span>
<img src="pics9c/00_schc_single.png" alt="SCHC single linkage clustering, k=6" width="80%" />
<p class="caption">
Figure 6: SCHC single linkage clustering, k=6
</p>
</div>
</div>
<div id="complete-linkage" class="section level4 unnumbered" number="">
<h4>Complete linkage</h4>
<p>As shown in Figure <a href="#fig:schccomplete">7</a>, the results for complete linkage are slightly better, with three non-singleton
clusters and between to total ratio of 0.306119.</p>
<div class="figure" style="text-align: center"><span id="fig:schccomplete"></span>
<img src="pics9c/00_schc_complete.png" alt="SCHC complete linkage clustering, k=6" width="80%" />
<p class="caption">
Figure 7: SCHC complete linkage clustering, k=6
</p>
</div>
</div>
<div id="average-linkage" class="section level4 unnumbered" number="">
<h4>Average linkage</h4>
<p>Finally, average linkage, shown in Figure <a href="#fig:schcavg">8</a>, again yields five singleton clusters, with
a low between to total ratio of 0.246467, only slightly better than single linkage.</p>
<div class="figure" style="text-align: center"><span id="fig:schcavg"></span>
<img src="pics9c/00_schc_average.png" alt="SCHC average linkage clustering, k=6" width="80%" />
<p class="caption">
Figure 8: SCHC average linkage clustering, k=6
</p>
</div>
</div>
</div>
</div>
<div id="skater" class="section level2 unnumbered" number="">
<h2>SKATER</h2>
<div id="principle-1" class="section level3 unnumbered" number="">
<h3>Principle</h3>
<p>The SKATER algorithm introduced
by <span class="citation">Assunção et al. (<a href="#ref-Assuncaoetal:06" role="doc-biblioref">2006</a>)</span> is based on the optimal pruning of a minimum spanning tree that reflects
the contiguity structure among the observations.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>The point of departure is a dissimilarity matrix that only contains weights for contiguous observations.
In other words, we consider the <em>distance</em> <span class="math inline">\(d_{ij}\)</span> between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, but only for those pairs
where <span class="math inline">\(w_{ij} = 1\)</span> (contiguous). This matrix is represented as a graph with the observations as nodes
and the contiguity relations as edges.</p>
<p>The full graph is reduced to a minimum spanning tree (MST), i.e., such that there is a path
that connects all observations (nodes),
but each is only visited once. In other words, the n nodes are connected by n-1 edges, such that the overall between-node dissimilarity is minimized. This yields a starting sum of squared deviations or SSD as <span class="math inline">\(\sum_i (x_i - \bar{x})^2\)</span>,
where <span class="math inline">\(\bar{x}\)</span> is the overall mean.</p>
<p>The objective is to reduce the overall SSD by maximizing the between SSD, or, alternatively, minimizing the
sum of within SSD. The MST is <em>pruned</em> by selecting the edge whose removal increases the objective function (between group dissimilarity) the most. To accomplish this, each potential split is evaluated in terms of its contribution
to the objective function.</p>
<p>More precisely, for each tree T, we consider <span class="math inline">\(\mbox{SSD}_T - (\mbox{SSD}_a + \mbox{SSD}_b)\)</span>, where
<span class="math inline">\(\mbox{SSD}_a, \mbox{SSD}_b\)</span> are the contributions of each subtree. The contribution is computed by first
calculating the average for that subtree and then obtaining the sum of squared deviations.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> We select the cut in the subtree where the difference
<span class="math inline">\(\mbox{SSD}_T - (\mbox{SSD}_a + \mbox{SSD}_b)\)</span> is the largest.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>At this point, we repeat the process for the new set of subtrees to select an optimal cut. We continue
until we have reached the desired number of clusters (k). A fully worked out example is given in
the <a href="#appendix">Appendix</a>.</p>
<p>Like SCHC, this is a hierarchical clustering method, but here the approach is divisive instead of
agglomerative. In other words, it starts with a single cluster, and finds the optimal split into
subclusters until the value of k is satisfied. Because of this hierarchical nature,
once the tree is cut at one point, all subsequent cuts are limited to the resulting subtrees.
In other words, once an observation ends up in a pruned branch of the tree, it cannot switch
back to a previous branch. This is sometimes viewed as a limitation of this algorithm.</p>
<p>In addition, the contiguity constraint is based on the original configurations and does not take
into account new neighbor relations that follow from the combination of different observations into
clusters, as was the case for SCHC.</p>
</div>
<div id="implementation-1" class="section level3 unnumbered" number="">
<h3>Implementation</h3>
<p>In <code>GeoDa</code>, the SKATER algorithm is invoked as the second item in the hierarchical group on the <strong>Clusters</strong> toolbar icon
(Figure <a href="#fig:schc">1</a>), or from the
main menu as <strong>Clusters &gt; skater</strong>.</p>
<p>Selecting this option brings up the <strong>Skater Settings</strong> dialog, shown in Figure <a href="#fig:skatervars">9</a>.
This interface has essentially the
same structure as in all other clustering approaches.
The panel provides a way to select the variables, the number of clusters and different
options to determine the clusters. As for SCHC, we again have
a <strong>Weights</strong> drop down list, where the contiguity weights must be specified. The SKATER algorithm does not work without a spatial weights file.</p>
<p>The <strong>Distance Function</strong> and <strong>Transformation</strong> options work in the same manner as
for classic clustering. At the bottom of the dialog, we specify the <strong>Field</strong> or
variable name where the classification will be saved.
We consider the <strong>Minimum Bound</strong> and <strong>Min Region Size</strong> options below.</p>
<p>We continue with the Guerry example data set and select the same six variables as before: <strong>Crm_prs</strong>, <strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>,
<strong>Infants</strong>,
and <strong>Suicids</strong>. The spatial weights are based on queen contiguity (<strong>guerry_85_q</strong>). We initially
set the number of clusters to <strong>4</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:skatervars"></span>
<img src="pics9c/33_skatervars.png" alt="SKATER cluster settings" width="30%" />
<p class="caption">
Figure 9: SKATER cluster settings
</p>
</div>
<p>Clicking on <strong>Run</strong> brings up the SKATER cluster map as a separate window and
lists the cluster characteristics in the <strong>Summary</strong> panel. We only give the map for now, as
Figure <a href="#fig:skatermap4">10</a>, and consider the summary for k=6 below.</p>
<div class="figure" style="text-align: center"><span id="fig:skatermap4"></span>
<img src="pics9c/33_skater4_map.png" alt="SKATER cluster map (k=4)" width="60%" />
<p class="caption">
Figure 10: SKATER cluster map (k=4)
</p>
</div>
<p>The spatial clusters generated by SKATER follow the minimum spanning tree closely and tend
to reflect the hierarchical nature of the pruning. In this example, the country initially gets divided
into three largely horizontal regions, of which the top one gets split into an eastern and a
western part.</p>
<p>The hierarchical nature of the SKATER algorithm can be further illustrated by increasing the
number of clusters to <strong>6</strong> for easier comparison with SCHC (all other options remain the same). The resulting cluster map in Figure <a href="#fig:skatermap6">11</a> shows
how the previous regions 3 and 4 each get split into an eastern and a western part.</p>
<div class="figure" style="text-align: center"><span id="fig:skatermap6"></span>
<img src="pics9c/33_skater6_map.png" alt="SKATER cluster map (k=6)" width="60%" />
<p class="caption">
Figure 11: SKATER cluster map (k=6)
</p>
</div>
<p>The summary in Figure <a href="#fig:skater6summary">12</a> reveals a ratio of between sum or squares to total sum of squares of about 0.420. This is inferior
to the result we obtained for SCHC for Ward’s method (about 0.462), but better than for the other linkage
functions.</p>
<div class="figure" style="text-align: center"><span id="fig:skater6summary"></span>
<img src="pics9c/33_skater6_summary.png" alt="SKATER cluster summary (k=6)" width="40%" />
<p class="caption">
Figure 12: SKATER cluster summary (k=6)
</p>
</div>
<div id="saving-the-minimum-spanning-tree" class="section level4 unnumbered" number="">
<h4>Saving the minimum spanning tree</h4>
<p>Near the bottom of the settings dialog is an option to save the minimum spanning tree. As
shown in Figure <a href="#fig:mstoptions">13</a>, either the complete MST can be saved, with the
corresponding box checked, or the MST that corresponds to the final result (box unchecked,
the default).</p>
<div class="figure" style="text-align: center"><span id="fig:mstoptions"></span>
<img src="pics9c/33_save_spanning_tree.png" alt="Minimum spanning tree options" width="35%" />
<p class="caption">
Figure 13: Minimum spanning tree options
</p>
</div>
<p>The MST is saved as a <strong>gwt</strong> weights file and is loaded into the <strong>Weights Manager</strong> upon creation.
This allows the graph to be displayed using the <strong>Connectivity &gt; Show Graph</strong> option in the cluster map
(or any map). In Figure <a href="#fig:skatermst">14</a>, the complete MST is shown.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> We clearly see the edges that correspond
with the transition between clusters.</p>
<div class="figure" style="text-align: center"><span id="fig:skatermst"></span>
<img src="pics9c/33_skater_mst_all.png" alt="Complete minimum spanning tree" width="60%" />
<p class="caption">
Figure 14: Complete minimum spanning tree
</p>
</div>
<p>In Figure <a href="#fig:skatermst6">15</a>, these edges are removed and the MST corresponding with the final solution
(for k=6) is displayed. This illustrates both the hierarchical nature of the algorithm as well as
how the result follows from pruning the MST.</p>
<div class="figure" style="text-align: center"><span id="fig:skatermst6"></span>
<img src="pics9c/33_skater_mst_6.png" alt="Minimum spanning tree for k=6" width="60%" />
<p class="caption">
Figure 15: Minimum spanning tree for k=6
</p>
</div>
<p>Since the MST is added to the <strong>Weights Manager</strong> as the last file, it becomes the default for future
spatial analyses. This is typically not what one wants, so the active weight needs to be reset to a proper
spatial weights definition before proceeding further.</p>
</div>
<div id="setting-a-minimum-cluster-size" class="section level4 unnumbered" number="">
<h4>Setting a minimum cluster size</h4>
<p>In several applications of spatially constrained clustering, a minimum cluster size needs to
be taken into account. For example, this is the case when the new regional groupings are intended
to be used in a computation of rates. In those instances, the denominator should be sufficiently large to avoid extreme variance instability, which is accomplished by setting a minimum population size.</p>
<p>In <code>GeoDa</code>, there are two options to implement this additional constraint. One is through
the <strong>Minimum Bound</strong> settings, as shown in Figure <a href="#fig:skaterminbd">16</a>. The check box activates a drop down list of
variables where a <em>spatialy extensive</em> variable can be selected for the minimum bound
constraint, such as the population (or, in other examples, total number of households, housing
units, etc.). In our example, we select the departmental population, <strong>Pop1831</strong>. The default
is to take <strong>10%</strong> of the total over all observations as the minimum bound. This can be adjusted
by means of a slider bar (to set the percentage), or by typing in a different value in the minimum
bound box. Here, we take the default.</p>
<div class="figure" style="text-align: center"><span id="fig:skaterminbd"></span>
<img src="pics9c/33_skater_mb.png" alt="SKATER minimum bound settings" width="30%" />
<p class="caption">
Figure 16: SKATER minimum bound settings
</p>
</div>
<p>The resulting spatial alignment of clusters is quite different from the unconstrained
solution, but again can be seen to be the result of a hierarchical subdivision of the three
large horizontal subregions. As shown in the left panel of Figure <a href="#fig:skaterminbdmap">17</a>, the northernmost region is divided into three,
and the southernmost one into two subregions. Compared to the unconstrained solution, where
the clusters consisted of 29, 28, 11, 8, 5, and 4 departments, the constrained regions have
a much more even distribution of 21, 17, 16, and 12 (twice) and 7 elements.</p>
<div class="figure" style="text-align: center"><span id="fig:skaterminbdmap"></span>
<img src="pics9c/33_skater_minbnd.png" alt="SKATER cluster map, min pop 10% (k=6)" width="80%" />
<p class="caption">
Figure 17: SKATER cluster map, min pop 10% (k=6)
</p>
</div>
<p>The effect of the constraint is to lower the objective function from 0.420 to 0.376. The
within-cluster sum of squares of the six regions is much more evenly distributed than in
the unconstrained solution, due to the similar number of elements in each cluster.</p>
<p>The second way to constrain the regionalization process is to specify a minimum number of
units that each cluster should contain. Again, this is a way to ensure that all clusters have
somewhat similar size, although it is not based on a substantive variable, only on the
number of elemental units. The constraint is set in the <strong>Min Region Size</strong> box of the parameters
panel in Figure <a href="#fig:skaterminbd">16</a>. In our example, we set the value to <strong>12</strong>.</p>
<p>The result is slighlty different from that generated by the minimum population constraint.
As shown in Figure <a href="#fig:skaterminsize">18</a>, we now have the three smallest clusters with size 12
(as mandated by the constraint). The ratio of between to total sum of squares is slightly
inferior at 0.374.</p>
<div class="figure" style="text-align: center"><span id="fig:skaterminsize"></span>
<img src="pics9c/33_skater_minsize.png" alt="SKATER cluster map, min size 12 (k=6)" width="80%" />
<p class="caption">
Figure 18: SKATER cluster map, min size 12 (k=6)
</p>
</div>
<p>Note that the minimum region size setting will override the number of clusters when it is set
too high. For example, setting the minimum cluster size to 14 will only yield clusters with k=5.
Again, this is a consequence of the hierarchical nature of the minimum spanning tree pruning used
in the skater algorithm. More specifically, a given sub-tree may not have sufficient nodes to
allow for further subdivisions that meet the minimum size requirement. A similar problem
occurs when the mininum population size is set too high.</p>
</div>
</div>
</div>
<div id="redcap" class="section level2 unnumbered" number="">
<h2>REDCAP</h2>
<div id="principle-2" class="section level3 unnumbered" number="">
<h3>Principle</h3>
<p>Whereas SCHC involves an agglomerative hierarchical approach and SKATER takes a divisive perspective, the
REDCAP collection of methods suggested by <span class="citation">Guo (<a href="#ref-Guo:08" role="doc-biblioref">2008</a>)</span> combines the two ideas <span class="citation">(see also Guo and Wang <a href="#ref-GuoWang:11" role="doc-biblioref">2011</a>)</span>. In this,
a distinction is made between the linkage update function (originally, single, complete and average linkage), and between
the treatment of contiguity.</p>
<p>As <span class="citation">Guo (<a href="#ref-Guo:08" role="doc-biblioref">2008</a>)</span> points out, the MST that is at the basis of the SKATER algorithm only
takes into account first order contiguity among pairs of observations. Unlike what we saw in SCHC, the
contiguity relations are not updated to consider the newly formed clusters. As a result of this,
observations that are part of a cluster that borders on a given spatial unit are not considered to be
neighbors of that unit unless they are also first order contiguous. The distinction between a fixed contiguity
relation and an updated spatial weights matrix is called <em>FirstOrder</em> and <em>FullOrder</em>. As a result, there are six
possible methods, combining the three linkage functions with the two views of contiguity. In later work,
Ward’s linkage was implemented as well.</p>
<p>We already saw earlier in the discussion of density-based clustering how the single-linkage dendrogram corresponds with a minimum spanning tree (MST). As a result,
REDCAP’s <em>FirstOrder-SingleLinkage</em> and SKATER are identical. Since the <em>FirstOrder</em> methods are generally inferior to the
<em>FullOrder</em> approaches, the latter are the main focus in <code>GeoDa</code> (for comparison to SKATER, <em>FirstOrder-SingleLinkage</em> is
included as well, but the other <em>FirstOrder</em> methods are not).</p>
<p>A careful consideration of the various REDCAP algorithms reveals that they essentially consist of three steps.
First, a dendrogram for contiguity constrained hierarchical clustering is constructed, using the given linkage
function. This yields the exact same dendrogram as produced by SCHC. Next, this dendrogram is turned into a spanning tree,
using standard graph manipulation principles. Finally, the optimal cuts in the spanning tree are obtained using the
same logic (and computations) as in SKATER, up to the desired level of k.</p>
</div>
<div id="implementation-2" class="section level3 unnumbered" number="">
<h3>Implementation</h3>
<p>The collection of REDCAP algorithms is invoked as the third item in the hierarchical group on the
<strong>Clusters</strong> toolbar icon
(Figure <a href="#fig:schc">1</a>), or from the
main menu as <strong>Clusters &gt; redcap</strong>.</p>
<p>Selecting this option brings up the <strong>REDCAP Settings</strong> dialog, shown in Figure <a href="#fig:redcapvars">19</a>.
This interface has the same structure as for SKATER, except that multiple methods are available.
The panel provides a way to select the variables, the number of clusters and different
options to determine the clusters. As for the previous methods, we again have
a <strong>Weights</strong> drop down list, where the contiguity weights must be specified. The REDCAP algorithms do not work without a spatial weights file.</p>
<p>The <strong>Distance Function</strong> and <strong>Transformation</strong> options operate in the same manner as
before. At the bottom of the dialog, we specify the <strong>Field</strong> or
variable name where the classification will be saved. The
<strong>Minimum Bound</strong> and <strong>Min Region Size</strong> options work in the same way as for SKATER and will not
be considered separately here.</p>
<p>We continue with the Guerry example data set and select the same six variables as before: <strong>Crm_prs</strong>, <strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>,
<strong>Infants</strong>,
and <strong>Suicids</strong>. The spatial weights are based on queen contiguity (<strong>guerry_85_q</strong>) and the
the number of clusters is set to <strong>6</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:redcapvars"></span>
<img src="pics9c/55_redcap_vars.png" alt="REDCAP cluster settings" width="30%" />
<p class="caption">
Figure 19: REDCAP cluster settings
</p>
</div>
<div id="full-order-wards-linkage" class="section level4 unnumbered" number="">
<h4>Full-Order Ward’s linkage</h4>
<p>We select the default method as <strong>FullOrder-WardLinkage</strong>, which implements Ward’s linkage with dynamically
updated spatial weights. This yields the cluster map shown in the left-hand panel of Figure <a href="#fig:redcapw">20</a>.</p>
<p>The results differ only marginally from the cluster map for SCHC using Ward’s linkage, shown in Figure <a href="#fig:schcwardmap">3</a>. One department moves from the cluster containing eight units to the cluster in the south with 27 units,
making the latter now the largest with 28 departments.</p>
<p>The cluster summary statistics, shown in the right-hand panel of Figure <a href="#fig:redcapw">20</a> are essentially the
same as for the SCHC case, but slightly worse, with a between to total SS ratio of 0.460.</p>
<div class="figure" style="text-align: center"><span id="fig:redcapw"></span>
<img src="pics9c/55_redcap_ward.png" alt="REDCAP Full Order, Ward's linkage, k=6" width="80%" />
<p class="caption">
Figure 20: REDCAP Full Order, Ward’s linkage, k=6
</p>
</div>
<p>As for SKATER, we can save the spanning tree associated with the linkage method, shown in Figure <a href="#fig:redcapspanning">21</a>.
There are some general similarities with the SKATER MST (Figure <a href="#fig:skatermst">14</a>), but a close comparison
reveals several important differences as well.</p>
<div class="figure" style="text-align: center"><span id="fig:redcapspanning"></span>
<img src="pics9c/55_redcapw_spanning.png" alt="Spanning tree for REDCAP Full Order Ward's linkage" width="60%" />
<p class="caption">
Figure 21: Spanning tree for REDCAP Full Order Ward’s linkage
</p>
</div>
<p>The options for minimum size work in the same way as for SKATER and are not further considered.</p>
<p>For the sake of completeness, we show the results for the three other FullOrder linkage methods as well.
<code>GeoDa</code> also includes <strong>FirstOrder-SingleLinkage</strong>, but the results are identical to those of SKATER and
are not further reported here.</p>
</div>
<div id="full-order-average-linkage" class="section level4 unnumbered" number="">
<h4>Full-Order Average linkage</h4>
<p>The cluster map and summary characteristics of Full-Order Average linkage are shown in Figure <a href="#fig:redcapavg">22</a>.
Compared to Ward’s results, the clusters are more balanced and there are no longer any singletons.
The dominance of the large northern and southern clusters remains, but the departments in the middle of the
country are arranged in different clusters. The ratio of between to total sum of squares is 0.429, somewhat
worse than for Ward’s.</p>
<div class="figure" style="text-align: center"><span id="fig:redcapavg"></span>
<img src="pics9c/55_redcap_avg.png" alt="REDCAP Full Order, average linkage, k=6" width="80%" />
<p class="caption">
Figure 22: REDCAP Full Order, average linkage, k=6
</p>
</div>
</div>
<div id="full-order-complete-linkage" class="section level4 unnumbered" number="">
<h4>Full-Order Complete linkage</h4>
<p>The result for Full-Order Complete linkage is shown in Figure <a href="#fig:redcapcomp">23</a>. We again have two singletons, one
of which is in the same (northern) location as for Ward’s linkage, but the other one is not. As before, the main
differences are for the departments in the middle of the country. The ratio of between to total sum of squares
is slightly better than for average linkage, at 0.435.</p>
<div class="figure" style="text-align: center"><span id="fig:redcapcomp"></span>
<img src="pics9c/55_redcap_comp.png" alt="REDCAP Full Order, complete linkage, k=6" width="80%" />
<p class="caption">
Figure 23: REDCAP Full Order, complete linkage, k=6
</p>
</div>
</div>
<div id="full-order-single-linkage" class="section level4 unnumbered" number="">
<h4>Full-Order Single linkage</h4>
<p>Finally, the results for Full-Order Single linkage are the worst. As shown in Figure <a href="#fig:redcapsingle">24</a>, the clusters
share a singleton with Ward’s linkage and another cluster with average linkage, but otherwise the spatial pattern
is quite distinct from the previous cases. The ratio of between to total sum of squares is the worst of the
four options, at 0.388.</p>
<div class="figure" style="text-align: center"><span id="fig:redcapsingle"></span>
<img src="pics9c/55_redcap_single.png" alt="REDCAP Full Order, single linkage, k=6" width="80%" />
<p class="caption">
Figure 24: REDCAP Full Order, single linkage, k=6
</p>
</div>
</div>
</div>
</div>
<div id="assessment" class="section level2 unnumbered" number="">
<h2>Assessment</h2>
<p>Clearly, different assumptions and different algorithms yield greatly varying results. This may be discomforting, but
it is important to keep in mind that each of these approaches have a slightly different way to handle the tension
between attribute and locational similarity.</p>
<p>In the end, the results can be evaluated on a number of different criteria. <code>GeoDa</code> reports the ratio of the between
sum of squares to the total sum of squares, but that is only one of a number of possible metrics, such as compactness,
balance, etc. Also, the commonalities and differences between the various approaches highlight where the tradeoffs
are particularly critical.</p>
<p>Finally, it should be kept in mind that the solutions offered by the different algorithms have no guarantee to yield
<em>global</em> optima. Therefore, it is important to consider more than one method, in order to gain insight into the
sensitivity of the results to the approach chosen.</p>
</div>
<div id="appendix" class="section level2 unnumbered" number="">
<h2>Appendix</h2>
<div id="arizona-counties-sample-data-set" class="section level3 unnumbered" number="">
<h3>Arizona counties sample data set</h3>
<p>We will use a small enough data set to illustrate the various spatially constrained algorithms.
As it turns out, the state of Arizona has only 14 counties, which is perfect for our purposes.
We create this data set by starting with the <strong>natregimes</strong> data set and selecting the counties
with <strong>STFIPS</strong> equal to 4, which are the counties in Arizona. If we then use <strong>Save Selected As</strong>
we obtain the data set shown in Figure <a href="#fig:schcaz">25</a>. We created a new variable <strong>AZID</strong> which
gives an id to the counties in alphabetical order. This identifier is shown in Figure <a href="#fig:schcaz">25</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz"></span>
<img src="pics9c/11_azcounties.png" alt="Arizona counties" width="50%" />
<p class="caption">
Figure 25: Arizona counties
</p>
</div>
<p>To keep things simple, we will only use a single variable, the unemployment rate in 1990, <strong>UE90</strong>. We standardize
this variable (using the <strong>Calculator</strong>) as <strong>SUE</strong> and show the associated values in Figure <a href="#fig:schcazvars">26</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcazvars"></span>
<img src="pics9c/11_azvariables.png" alt="County identifiers and standardized variable" width="20%" />
<p class="caption">
Figure 26: County identifiers and standardized variable
</p>
</div>
<p>The final piece to complete our analysis is a spatial weight matrix. In Figure <a href="#fig:schcazgal">27</a> we show
the contents of the queen contiguity gal file.</p>
<div class="figure" style="text-align: center"><span id="fig:schcazgal"></span>
<img src="pics9c/11_az_gal.png" alt="AZ county queen contiguity information" width="20%" />
<p class="caption">
Figure 27: AZ county queen contiguity information
</p>
</div>
</div>
<div id="schc-complete-linkage-worked-example" class="section level3 unnumbered" number="">
<h3>SCHC Complete Linkage worked example</h3>
<p>We illustrate the spatially constrained hierarchical clustering using complete linkage. Even though this is not
an ideal linkage selection, it is easy to implement and helps to illustrate the logic of the SCHC.</p>
<p>The point of departure is the matrix of Euclidean distance in attribute space. In our example, there is
only one variable, so the distance is equivalent to the absolute difference between the values at two locations.
In Figure <a href="#fig:schcaz1">28</a>, the full distance matrix is shown. Distinct from the example in the
unconstrained case, we limit our attention to those pairs of observations that are spatially contiguous,
shown highlighted in red in the matrix.</p>
<p>The first step is to identify the pair of observations that are closest in attribute space and
contiguous (red in the table). Inspection
of the distance matrix in Figure <a href="#fig:schcaz1">28</a> finds the pair 8-14 as the least different, with a
distance value of 0.03.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz1"></span>
<img src="pics9c/11_az_step1.png" alt="SCHC complete linkage step 1" width="65%" />
<p class="caption">
Figure 28: SCHC complete linkage step 1
</p>
</div>
<p>The next step is to combine observations 8 and 14 and recompute the distance from other observations
as the <em>largest</em> from either 8 or 14. The updated matrix is shown in Figure <a href="#fig:schcaz2">29</a>. In
addition, the contiguity matrix must be updated with neigbors to the cluster 8-14 as those who
were either neighbors to 8 or 14, or to both. The updated neighbor relation is shown as the red
values in the matrix. The smallest value is between 9 and 8-14, which yields a new cluster of 8-9-14.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz2"></span>
<img src="pics9c/11_az_step2.png" alt="SCHC complete linkage step 2" width="60%" />
<p class="caption">
Figure 29: SCHC complete linkage step 2
</p>
</div>
<p>The remaining steps proceed in the same manner. The contiguity relations are updated and the largest distance
between the two elements of the cluster is entered as the new distance in the matrix. In step 3,
shown in Figure <a href="#fig:schcaz3">30</a>, we identify the pair with
the smallest value of 0.12, between 2 and 13, which is our new cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz3"></span>
<img src="pics9c/11_az_step3.png" alt="SCHC complete linkage step 3" width="55%" />
<p class="caption">
Figure 30: SCHC complete linkage step 3
</p>
</div>
<p>With the updated contiguities, step 4 identifies the pair 4-12 as the new cluster, with a distance of 0.26,
as in Figure <a href="#fig:schcaz4">31</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz4"></span>
<img src="pics9c/11_az_step4.png" alt="SCHC complete linkage step 4" width="50%" />
<p class="caption">
Figure 31: SCHC complete linkage step 4
</p>
</div>
<p>We continue in the same way with step 5, identifying a new cluster of 3-4-12 with a distance of 0.29.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz5"></span>
<img src="pics9c/11_az_step5.png" alt="SCHC complete linkage step 5" width="45%" />
<p class="caption">
Figure 32: SCHC complete linkage step 5
</p>
</div>
<p>In step 6, we add 11 to the existing cluster of 8-9-14, with a distance of 0.34, as shown in Figure <a href="#fig:schcaz6">33</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz6"></span>
<img src="pics9c/11_az_step6.png" alt="SCHC complete linkage step 6" width="40%" />
<p class="caption">
Figure 33: SCHC complete linkage step 6
</p>
</div>
<p>Next, we add 6 to 2-13, with a distance of 0.37, as in Figure <a href="#fig:schcaz">25</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz7"></span>
<img src="pics9c/11_az_step7.png" alt="SCHC complete linkage step 7" width="35%" />
<p class="caption">
Figure 34: SCHC complete linkage step 7
</p>
</div>
<p>Now, 5 joins 3-4-12, with a distance of 0.77, in Figure <a href="#fig:schcaz8">35</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz8"></span>
<img src="pics9c/11_az_step8.png" alt="SCHC complete linkage step 8" width="30%" />
<p class="caption">
Figure 35: SCHC complete linkage step 8
</p>
</div>
<p>The final few steps continue to reduce the number of clusters. In this stage, the two clusters
3-4-5-12 and 2-6-13 are joined, with a distance of 0.85, as in Figure <a href="#fig:schcaz9">36</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz9"></span>
<img src="pics9c/11_az_step9.png" alt="SCHC complete linkage step 9" width="25%" />
<p class="caption">
Figure 36: SCHC complete linkage step 9
</p>
</div>
<p>Next, 7 is added to 8-9-11-14, with a distance of 1.04, in Figure <a href="#fig:schcaz10">37</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz10"></span>
<img src="pics9c/11_az_step10.png" alt="SCHC complete linkage step 10" width="25%" />
<p class="caption">
Figure 37: SCHC complete linkage step 10
</p>
</div>
<p>At this point, 1 and 10 are joined, with a distance of 1.38, shown in Figure <a href="#fig:schcaz11">38</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz11"></span>
<img src="pics9c/11_az_step11.png" alt="SCHC complete linkage step 11" width="25%" />
<p class="caption">
Figure 38: SCHC complete linkage step 11
</p>
</div>
<p>The next to final step, resulting in two clusters, merges 3-4-5-12-2-6-13 with 7-8-9-11-14, with
a distance of 1.42, as in Figure <a href="#fig:schcaz12">39</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:schcaz12"></span>
<img src="pics9c/11_az_step12.png" alt="SCHC complete linkage step 12" width="20%" />
<p class="caption">
Figure 39: SCHC complete linkage step 12
</p>
</div>
<p>One interesting aspect of these steps is that at each stage, the distance involved is larger.
As mentioned above, the complete linkage approach avoids the inversion problem, so that we indeed
see a uniform increase in the distances involved.</p>
<p>We can also visualize the process by the dendrogram generate by <code>GeoDa</code> for this example, shown
in Figure <a href="#fig:schcazdendro4">40</a>. We see from the distance associated with each pair how the
first merger is between 8 and 14, followed by adding 9, etc. The steps in the dendrogram identically follow
the steps illustrated above.</p>
<div class="figure" style="text-align: center"><span id="fig:schcazdendro4"></span>
<img src="pics9c/11_az_k4_dendro.png" alt="SCHC complete linkage dendrogram, k=4" width="40%" />
<p class="caption">
Figure 40: SCHC complete linkage dendrogram, k=4
</p>
</div>
<p>Finally, we show the spatial layout of the clusters for k=4 in Figure <a href="#fig:schcazmap4">41</a>. Clearly,
the contiguity constraint has been satisfied.</p>
<div class="figure" style="text-align: center"><span id="fig:schcazmap4"></span>
<img src="pics9c/44_az_k4.png" alt="SCHC complete linkage cluster map, k=4" width="60%" />
<p class="caption">
Figure 41: SCHC complete linkage cluster map, k=4
</p>
</div>
</div>
<div id="skater-worked-example" class="section level3 unnumbered" number="">
<h3>SKATER worked example</h3>
<p>We illustrate the SKATER algorithm with the same Arizona county example, with k (the number of clusters) set to 4. In Figure <a href="#fig:skaterv">42</a>, we list the
observations and their squares, which form the basis for the computation of the relevant sums of squared deviations
(SSD) for each subset.</p>
<div class="figure" style="text-align: center"><span id="fig:skaterv"></span>
<img src="pics9c/22_skater_vars.png" alt="AZ county variables" width="20%" />
<p class="caption">
Figure 42: AZ county variables
</p>
</div>
<p>The first step in the process is to reduce the information for contiguous pairs in the distance matrix of Figure <a href="#fig:schcaz1">28</a> (shown in red) to
a minimum spanning tree (MST). The result is shown in Figure <a href="#fig:mstfull">43</a>, against the backdrop of the Arizona
county map.</p>
<div class="figure" style="text-align: center"><span id="fig:mstfull"></span>
<img src="pics9c/22_az_mst.png" alt="SKATER minimum spanning tree" width="40%" />
<p class="caption">
Figure 43: SKATER minimum spanning tree
</p>
</div>
<p>We now have to evaluate every possible cut in the MST in terms of its contribution to reducing the overall sum of
squared deviations (SSD). We know the data in third column of Figure <a href="#fig:skaterv">42</a> is standardized, so the
mean is zero by construction. As a result, the total sum of squared deviations is the sum of squares, i.e., the
sum of the values listed in the fourth column. This sum is 13.000 (rounded).<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>The vertices for each possible cut are shown as the first two columns in Figure <a href="#fig:skater1">44</a>. For each
subtree, we need to compute the corresponding SSD. The next step then implements the optimal cut such that the
total SSD decreases the most, i.e., max[<span class="math inline">\(\mbox{SSD}_T - (\mbox{SSD}_a + \mbox{SSD}_b)\)</span>], where <span class="math inline">\(\mbox{SSD}_T\)</span> is the SSD for the corresponding
tree, and <span class="math inline">\(\mbox{SSD}_a\)</span> and <span class="math inline">\(\mbox{SSD}_b\)</span> are the totals for the subtrees corresponding to the cut.</p>
<p>In order to accomplish this, we first compute the mean of the values associated
with the vertices of each subtree, and then calculate the SSD for each subtree k as <span class="math inline">\(\sum_i x_i^2 - n_k \bar{x}_k^2\)</span>,
with <span class="math inline">\(\bar{x}_k\)</span> as the mean for the average value for the subtree and <span class="math inline">\(n_k\)</span> as the number of elements in the subtree.</p>
<p>In the interest of space, we do not show all the calculations, but give a few examples. For the cut
1-10, the vertex 1 becomes a singleton, so its SSD is 0 (<span class="math inline">\(SSD_a\)</span> in the fourth column and first row in Figure <a href="#fig:skater1">44</a>).
For vertex 10, the subtree consists of all 13 elements other than 1, with a mean of -0.211 and an SSD of 4.894 (
<span class="math inline">\(SSD_b\)</span> in the fifth column
and first row in Figure <a href="#fig:skater1">44</a>). Consequently, the contribution to reducing the overall SSD amounts to
13.000 - (0 + 4.894) = 8.106.</p>
<p>In a similar fashion, we compute the SSD for each possible subtree and enter the values in Figure <a href="#fig:skater1">44</a>.
The largest decrease in overall SSD is achieved by the cut between 5 and 10, which yields a reduction of 9.820.</p>
<div class="figure" style="text-align: center"><span id="fig:skater1"></span>
<img src="pics9c/22_skater_step1.png" alt="SKATER step 1" width="30%" />
<p class="caption">
Figure 44: SKATER step 1
</p>
</div>
<p>The new MST after cutting the link between 5 and 10 is shown in Figure <a href="#fig:mstsstep1">45</a>, yielding two initial
clusters.</p>
<div class="figure" style="text-align: center"><span id="fig:mstsstep1"></span>
<img src="pics9c/22_az_skater_2.png" alt="SKATER minimum spanning tree - first split" width="40%" />
<p class="caption">
Figure 45: SKATER minimum spanning tree - first split
</p>
</div>
<p>We now repeat the process, looking for the greatest decrease in overall SSD. We separate the data into two subtrees,
one for 1-10 and the other for the remaining vertices. In each, we recalculate the SSD for the subtree, which yields
0.958 for 1-10 and 2.222 for the other cluster. For each possible cut, we compute the SSD for the corresponding subtrees,
with the results shown in Figure <a href="#fig:skater2">46</a>. At this point, the greatest contribution towards reducing the
overall SSD is offered by a cut between 8 and 11, with a contribution of 1.441.</p>
<div class="figure" style="text-align: center"><span id="fig:skater2"></span>
<img src="pics9c/22_skater_step2.png" alt="SKATER step 2" width="30%" />
<p class="caption">
Figure 46: SKATER step 2
</p>
</div>
<p>The corresponding MST for the three clusters is shown in Figure <a href="#fig:mstsstep2">47</a>. We now have one cluster of
two units, one with three, and one with nine.</p>
<div class="figure" style="text-align: center"><span id="fig:mstsstep2"></span>
<img src="pics9c/22_az_skater_3.png" alt="SKATER minimum spanning tree - second split" width="40%" />
<p class="caption">
Figure 47: SKATER minimum spanning tree - second split
</p>
</div>
<p>At this point, we only need to make one more cut (k=4). When we compute the SSD for each subtree, we find a total of 0.0009 for 8-9-14, and
a value of 0.779 for the remainder, both well below the contribution to decreasing the SSD
of 0.958 obtained by 1-10 (since the split yields two single observations, the decrease in SSD equals the total
SSD for the subtree). Consequently, we do not need to carry out the detailed calculations for each other subtree, since the final cut is between 1 and 10, yielding two singletons. As a result, we have four clusters, with the final MST shown in Figure <a href="#fig:mstsstep3">48</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:mstsstep3"></span>
<img src="pics9c/22_az_skater_4.png" alt="SKATER minimum spanning tree - third split" width="40%" />
<p class="caption">
Figure 48: SKATER minimum spanning tree - third split
</p>
</div>
<p>The matching cluster map created by <code>GeoDa</code> for SKATER with k=4 is shown in Figure <a href="#fig:skateraz">49</a>.
The four regions match the subtrees in the MST, with 1 and 10 being singletons.</p>
<div class="figure" style="text-align: center"><span id="fig:skateraz"></span>
<img src="pics9c/44_skater_cluster.png" alt="SKATER cluster map, k=4" width="60%" />
<p class="caption">
Figure 49: SKATER cluster map, k=4
</p>
</div>
</div>
<div id="redcap-worked-example" class="section level3 unnumbered" number="">
<h3>REDCAP worked example</h3>
<p>We now use the Arizona counties example to illustrate the logic of the REDCAP <em>FullOrder-CompleteLinkage</em> option. As it turns out, we can reuse the results from SCHC and the logic from SKATER.</p>
<p>The first stage in the REDCAP algorithm is to construct a <em>spanning tree</em> that corresponds to a spatially constrained
complete linkage hierarchical clustering dendrogram.</p>
<p>In our illustration, we implement <em>FullOrder</em>, i.e., the contiguity
relations between clusters is dynamically updated. Consequently, the steps to follow are the same as for complete linkage SCHC, yielding
the dendrogram in Figure <a href="#fig:schcazdendro4">40</a>.</p>
<p>To create a spanning tree representation of this dendrogram, we connect the observations following the merger
of entities in the dendrogram. In our example, this means we first connect 8 and 14. In the second step, we connect
9 to the cluster 8-14, but since 9 is only contiguous to 14, we make the edge 9-14. The full sequence of edges is
given in the list below. Whenever we connect a new entity to a cluster, we either connect it to the only contiguous
unit, or to the contiguous one that is closest (using the distance measures in Figure <a href="#fig:schcaz1">28</a> ):</p>
<ul>
<li>Step 1: 8-14</li>
<li>Step 2: 9-14</li>
<li>Step 3: 2-13</li>
<li>Step 4: 4-12</li>
<li>Step 5: 3-4</li>
<li>Step 6: 11-8</li>
<li>Step 7: 6-2</li>
<li>Step 8: 5-4</li>
<li>Step 9: 2-5</li>
<li>Step 10: 7-11</li>
<li>Step 11: 1-10</li>
<li>Step 12: 11-12</li>
<li>Step 13: 10-5</li>
</ul>
<p><br></p>
<p>The resulting spanning tree is shown in Figure <a href="#fig:redcaptree">50</a>, with the sequence of steps marked in blue.
The tree is largely the same as the MST in Figure <a href="#fig:mstfull">43</a>, except that the link between 11 and 2 is
replaced by a link between 5 and 2.</p>
<div class="figure" style="text-align: center"><span id="fig:redcaptree"></span>
<img src="pics9c/44_redcap_az_mst.png" alt="REDCAP FullOrder-CompleteLinkage spanning tree" width="40%" />
<p class="caption">
Figure 50: REDCAP FullOrder-CompleteLinkage spanning tree
</p>
</div>
<p>At this point, we search for an optimal cut using the same approach as for SKATER.
Given the similarity
of the two trees, we will be able to reuse a lot of the previous results. Only those subtrees affected by the new
link 5-2 replacing the link 11-2 need to be considered again. Specifically, in the first step, these are
the edges 5-2, 5-4, 4-12 and 11-12. The other results can be borrowed from the first step in SKATER, shown
in Figure <a href="#fig:skater1">44</a>. The new results are given in Figure <a href="#fig:redcap1">51</a>. As in the SKATER case,
the first cut is between 5 and 10.</p>
<div class="figure" style="text-align: center"><span id="fig:redcap1"></span>
<img src="pics9c/44_redcap_step1.png" alt="REDCAP step 1" width="30%" />
<p class="caption">
Figure 51: REDCAP step 1
</p>
</div>
<p>We proceed in the same manner for the second step, where again we can borrow several results directly from
the SKATER case. Figure <a href="#fig:redcap2">52</a> lists the updated values. Here again, the optimal cut is between 8 and
11.</p>
<div class="figure" style="text-align: center"><span id="fig:redcap2"></span>
<img src="pics9c/44_redcap_step2.png" alt="REDCAP step 2" width="30%" />
<p class="caption">
Figure 52: REDCAP step 2
</p>
</div>
<p>The final step is the same as for SKATER, yielding the same layout as in Figure <a href="#fig:skateraz">49</a> for
our four clusters.</p>
<p><br></p>
</div>
</div>
<div id="references" class="section level2 unnumbered" number="">
<h2>References</h2>
<div id="refs" class="references hanging-indent">
<div id="ref-Assuncaoetal:06">
<p>Assunção, Renato M., M. C. Neves, G. Câmara, and C. Da Costa Freitas. 2006. “Efficient Regionalization Techniques for Socio-Economic Geographical Units Using Minimum Spanning Trees.” <em>International Journal of Geographical Information Science</em> 20 (7): 797–811.</p>
</div>
<div id="ref-Duqueetal:07">
<p>Duque, Juan Carlos, Raúl Ramos, and Jordi Suriñach. 2007. “Supervised Regionalization Methods: A Survey.” <em>International Regional Science Review</em> 30: 195–220.</p>
</div>
<div id="ref-Duqueatal:11a">
<p>Duque, Juan C., Richard L. Church, and Richard S. Middleton. 2011. “The p-Regions Problem.” <em>Geographical Analysis</em> 43: 104–26.</p>
</div>
<div id="ref-Gordon:96">
<p>Gordon, A. D. 1996. “A Survey of Constrained Classification.” <em>Computational Statistics and Data Analysis</em> 21: 17–29.</p>
</div>
<div id="ref-Guo:08">
<p>Guo, Diansheng. 2008. “Regionalization with Dynamically Constrained Agglomerative Clustering and Partitioning (REDCAP).” <em>International Journal of Geographical Information Science</em> 22 (7): 801–23.</p>
</div>
<div id="ref-Guo:09">
<p>———. 2009. “Greedy Optimization for Contiguity-Constrained Hierarchical Clustering.” In <em>2013 IEEE 13th International Conference on Data Mining Workshops</em>, 591–96. Los Alamitos, CA, USA: IEEE Computer Society. <a href="https://doi.org/10.1109/ICDMW.2009.75">https://doi.org/10.1109/ICDMW.2009.75</a>.</p>
</div>
<div id="ref-GuoWang:11">
<p>Guo, Diansheng, and Hu Wang. 2011. “Automatic Region Building for Spatial Analysis.” <em>Transactions in GIS</em> 15: 29–45.</p>
</div>
<div id="ref-Lankford:69">
<p>Lankford, Philip M. 1969. “Regionalization: Theory and Alternative Algorithms.” <em>Geographical Analysis</em> 1: 196–212.</p>
</div>
<div id="ref-MurrayGrubesic:02">
<p>Murray, Alan T., and Tony H. Grubesic. 2002. “Identifying Non-Hierarchical Spatial Clusters.” <em>International Journal of Industrial Engineering</em> 9: 86–95.</p>
</div>
<div id="ref-Murtagh:85">
<p>Murtagh, Fionn. 1985. “A Survey of Algorithms for Contiguity-Constrained Clustering and Related Problems.” <em>The Computer Journal</em> 28: 82–88.</p>
</div>
<div id="ref-Recchia:10">
<p>Recchia, Anthony. 2010. “Contiguity-Constrained Hierarchical Agglomerative Clustering Using SAS.” <em>Journal of Statistical Software</em> 22 (2).</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu" class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>This <em>skater</em> algorithm is not to be confused with tools to interpret the results of deep learning <a href="https://github.com/oracle/Skater" class="uri">https://github.com/oracle/Skater</a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>This can readily
be computed as <span class="math inline">\(\sum_i x_i^2 - n_T \bar{x}_T^2\)</span>, where <span class="math inline">\(\bar{x}_T\)</span> is the average for the subtree, and
<span class="math inline">\(n_T\)</span> is the number of nodes in the subtree.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>While this exhaustive evaluation
of all potential cuts is inherently slow, it can be speeded up by exploiting certain heuristics, as
described in <span class="citation">Assunção et al. (<a href="#ref-Assuncaoetal:06" role="doc-biblioref">2006</a>)</span>. In <code>GeoDa</code>, we use full enumeration, but implement parallelization to obtain
better performance.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>To obtain this graph, use
<strong>Change Edge Thickness &gt; Strong</strong> and set the color to yellow.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Since the variable is standardized, the estimated variance <span class="math inline">\(\hat{\sigma}^2 = \sum_i x_i^2 / (n - 1) = 1\)</span>. Therefore, <span class="math inline">\(\sum_i x_i^2 = n - 1\)</span>, or 13 in our example.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<footer class="site-footer">
  <span class="site-footer-owner"><a href="https://github.com/lixun910/geoda">GeoDa</a> is maintained by <a href="#">lixun910</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
</footer>

</section>


<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
