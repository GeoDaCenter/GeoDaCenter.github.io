<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

  <meta charset="utf-8" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="pandoc" />

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="author" content="Luc Anselin" />


  <title>Cluster Analysis (3)</title>

  <link href="lab8_files/highlightjs-1.1/default.css" rel="stylesheet" />
  <script src="lab8_files/highlightjs-1.1/highlight.js"></script>
  <title>GeoDa on Github</title>

  <style>
    * {
      margin: 0;
      padding: 0;
    }

    .shadowfilter {
      -webkit-filter: drop-shadow(12px 12px 7px rgba(0, 0, 0, 0.5));
      filter: url(shadow.svg#drop-shadow);
    }

    .intro1 {
      margin-left: -45px;
    }
  </style>
  <link rel="stylesheet" type="text/css" href="//geodacenter.github.io/stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="//geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="//geodacenter.github.io/stylesheets/github-light.css" media="screen">
  <link rel="stylesheet" href="//geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
  <style>
    ul {
      padding-left: 30px;
    }

    figcaption {
      top: .70em;
      left: .35em;
      bottom: auto !important;
      right: auto !important;
    }
  </style>

  <style>
    h1 {
      text-align: center;
    }

    h3.subtitle {
      text-align: center;
    }

    h4.author {
      text-align: center;
    }

    h4.date {
      text-align: center;
    }

    p.caption {
      font-size: 12px;
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LC0QJ53WFS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-LC0QJ53WFS');
  </script>
  <!-- End Google tag -->

  <style type="text/css">
    code {
      white-space: pre;
    }
  </style>
  <script type="text/javascript">
    if (window.hljs && document.readyState && document.readyState === "complete") {
      window.setTimeout(function () {
        hljs.initHighlighting();
      }, 0);
    }
  </script>





</head>

<body>


  <section class="main-content">


    <h1 class="title toc-ignore">Cluster Analysis (3)</h1>
    <h3 class="subtitle"><em>Spatially Constrained Clustering Methods</em></h3>
    <h4 class="author"><em>Luc Anselin<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></em></h4>
    <h4 class="date"><em>11/28/2017 [in progress, updated]</em></h4>


    <div id="TOC">
      <ul>
        <li><a href="#introduction">Introduction</a>
          <ul>
            <li><a href="#objectives">Objectives</a>
              <ul>
                <li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
              </ul>
            </li>
            <li><a href="#getting-started">Getting started</a></li>
          </ul>
        </li>
        <li><a href="#k-means-with-spatial-constraints">K Means with Spatial Constraints</a>
          <ul>
            <li><a href="#principle">Principle</a></li>
            <li><a href="#geometric-centroids-as-attributes">Geometric centroids as attributes</a>
              <ul>
                <li><a href="#preliminaries">Preliminaries</a></li>
                <li><a href="#k-means-benchmark">K Means benchmark</a></li>
                <li><a href="#centroids-included-as-variables">Centroids included as variables</a></li>
              </ul>
            </li>
            <li><a href="#weighted-optimization">Weighted optimization</a></li>
            <li><a href="#an-optimization-bisection-search">An optimization bisection search</a></li>
          </ul>
        </li>
        <li><a href="#skater">Skater</a>
          <ul>
            <li><a href="#principle-1">Principle</a></li>
            <li><a href="#implementation">Implementation</a>
              <ul>
                <li><a href="#setting-a-minimum-cluster-size">Setting a minimum cluster size</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#max-p-region-problem">Max-p Region Problem</a>
          <ul>
            <li><a href="#principle-2">Principle</a></li>
            <li><a href="#implementation-1">Implementation</a></li>
            <li><a href="#sensitivity-analysis">Sensitivity analysis</a>
              <ul>
                <li><a href="#changing-the-number-of-iterations">Changing the number of iterations</a></li>
                <li><a href="#setting-initial-groups">Setting initial groups</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#summary">Summary</a></li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <p><br></p>
    <div id="introduction" class="section level2 unnumbered">
      <h2>Introduction</h2>
      <p>In this lab, we will explore spatially constrained clustering, i.e., when contiguity constraints are imposed on
        a multivariate clustering method. These methods highlight the tension between the two fundamental criteria
        behind spatial autocorrelation, i.e., the combination of attribute similarity with locational similarity. To
        illustrate these techniques, we will continue to use the Guerry data set on moral statistics in 1830 France,
        which comes pre-installed with GeoDa.</p>
      <p>We will consider three methods. One is an extension of k-means clustering that includes the observation
        centroids (x,y coordinates) as part of the optimization routine, e.g., as suggested in <span
          class="citation">Haining, Wise, and Ma (2000)</span>, among others. The other two approaches explicitly
        incorporate the contiguity constraint in the optimization process. The <strong>skater</strong> algorithm
        introduced by <span class="citation">Assuncao et al. (2006)</span> is based on the optimal pruning of a minimum
        spanning tree that reflects the contiguity structure among the observations. The so-called <strong>max-p
          regions</strong> model <span class="citation">(outlined in Duque, Anselin, and Rey 2012)</span> uses a
        different approach and considers the regionalization problem as an application of integer programming. In
        addition, the number of regions is determined endogenously.</p>
      <p><strong>Note</strong>: the cluster methods in GeoDa are under active development and are <strong>not yet
          production grade</strong>. Some of them are quite slow and others do not scale well to data sets with more
        than 1,000 observations. Also, new methods will be added, so this is a work in progress. Always make sure to
        have the latest version of the notes.</p>
      <div id="objectives" class="section level3 unnumbered">
        <h3>Objectives</h3>
        <ul>
          <li>
            <p>Introduce contiguity constraints into k-means clusering</p>
          </li>
          <li>
            <p>Identify contiguous clusters by means of the skater algorithm</p>
          </li>
          <li>
            <p>Identify contiguous clusters with the number of clusters as endogenous with max-p</p>
          </li>
          <li>
            <p>Comparing the characteristics of the clusters</p>
          </li>
        </ul>
        <div id="geoda-functions-covered" class="section level4 unnumbered">
          <h4>GeoDa functions covered</h4>
          <ul>
            <li>Clusters &gt; K Means
              <ul>
                <li>K Means settings</li>
                <li>cluster summary statistics</li>
                <li>cluster map</li>
                <li>introducting geometric centroids</li>
                <li>weighted optimization</li>
              </ul>
            </li>
            <li>Clusters &gt; Skater
              <ul>
                <li>set minimum bound variable</li>
                <li>set minimum cluster size</li>
              </ul>
            </li>
            <li>Clusters &gt; Max-p
              <ul>
                <li>set number of iterations</li>
                <li>set initial groups</li>
              </ul>
            </li>
          </ul>
          <p><br></p>
        </div>
      </div>
      <div id="getting-started" class="section level3 unnumbered">
        <h3>Getting started</h3>
        <p>With GeoDa launched and all previous projects closed, we again load the Guerry sample data set from the
          <strong>Connect to Data Source</strong> interface. We either load it from the sample data collection and then
          save the file in a working directory, or we use a previously saved version. The process should yield the
          familiar themeless base map, showing the 85 French departments.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/0_547_themelessbasemap.png" alt="French departments themeless map" width="80%" />
          <p class="caption">
            French departments themeless map
          </p>
        </div>
        <p>To carry out the spatially constrained cluster analysis, we will need a spatial weights file, either created
          from scratch, or loaded from a previous analysis (ideally, contained in a project file). The <strong>Weights
            Manager</strong> should have at least one spatial weights file included, e.g., <strong>Guerry_85_q</strong>
          for first order queen contiguity, as shown here.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/0_548_weightsmanager.png" alt="Weights manager contents" width="35%" />
          <p class="caption">
            Weights manager contents
          </p>
        </div>
      </div>
    </div>
    <div id="k-means-with-spatial-constraints" class="section level2 unnumbered">
      <h2>K Means with Spatial Constraints</h2>
      <div id="principle" class="section level3 unnumbered">
        <h3>Principle</h3>
        <p>As we have seen in a previous chapter, the K Means cluster procedure only deals with attribute similarity and
          does not guarantee that the resulting <em>clusters</em> are spatially contiguous. A number of ad hoc
          procedures have been suggested to create clusters where all members of the cluster are contiguous, but these
          are generally unsatisfactory.</p>
        <p>An alternative is to include the geometric centroids of the observations as part of the optimization process.
          There are two general approaches to carry this out. In one, the x and y coordinates are simply added as
          additional variables in the collection of attributes. While this tends to yield more geographically compact
          clusters, it does not guarantee contiguity. The second approach consists of a weighted multi-objective
          optimization, that combines the objective of attribute similarity with that of geometric similarity. This
          allows for the identification of a cluster with maximal between-cluster dissimilarity that consists of
          spatially contiguous units.</p>
        <p>We consider each approach in turn.</p>
      </div>
      <div id="geometric-centroids-as-attributes" class="section level3 unnumbered">
        <h3>Geometric centroids as attributes</h3>
        <div id="preliminaries" class="section level4 unnumbered">
          <h4>Preliminaries</h4>
          <p>As we saw in an earier chapter, the K Means procedure is started from the main menu as <strong>Clusters
              &gt; K Means</strong>, or from the <strong>Clusters</strong> toolbar icon.</p>
          <p>Before we can proceed, we need to make sure the centroids are part of the data table. Strictly speaking,
            this is not necessary for the multi-objective optimization (the centroids are computed behind the scenes),
            but we do need the x, y coordinates in order to include them as part of the attributes to be clustered in
            the unweighted version.</p>
          <p>To this end, we bring up the options menu in the themeless map (or any map), and select <strong>Shape
              Centers &gt; Add Centroids to Table</strong>.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/1_615_addcentroids.png" alt="Add centroids to table" width="50%" />
            <p class="caption">
              Add centroids to table
            </p>
          </div>
          <p>We can use the default variable names <strong>COORD_X</strong> and <strong>COORD_Y</strong>, or replace
            them by something more meaningful.</p>
        </div>
        <div id="k-means-benchmark" class="section level4 unnumbered">
          <h4>K Means benchmark</h4>
          <p>In order to provide a frame of reference, we launch K Means and select the usual six variables (Crm_prs,
            Crm_prp, Litercy, Donatns, Infants and Suicids) from the <strong>Input</strong> panel. We leave all other
            options to the default, except that we set the <strong>Number of Clusters</strong> to <strong>4</strong>. We
            also set the <strong>Field</strong> name for the cluster categories to <strong>CL_k4</strong>, or something
            similar, so that we can later keep track of all the different cases.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/2_623_k4_base_vars.png" alt="Variables for benchmark case" width="50%" />
            <p class="caption">
              Variables for benchmark case
            </p>
          </div>
          <p>Clicking on <strong>Run</strong> generates the cluster map as well as the cluster summary statistics in the
            <strong>Summary</strong> panel.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
          <div class="figure" style="text-align: center">
            <img src="pics8/2_624_k4_base_map.png" alt="K Means benchmark cluster map (k=4)" width="80%" />
            <p class="caption">
              K Means benchmark cluster map (k=4)
            </p>
          </div>
          <p>The geographic grouping of the observations that are part of each cluster is far from contiguous. The
            number of disconnected “clusters” ranges from three for group 2 (but note that the labels are arbitrary), to
            five for group 1.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/2_625_k4_base_summary.png" alt="K Means benchmark summary (k=4)" width="80%" />
            <p class="caption">
              K Means benchmark summary (k=4)
            </p>
          </div>
          <p>The main descriptive statistic we will use in future comparisons is the ratio of between to total sum of
            squares, 0.433.</p>
        </div>
        <div id="centroids-included-as-variables" class="section level4 unnumbered">
          <h4>Centroids included as variables</h4>
          <p>We now repeat the process, but include the two centroids (<strong>COORD_X</strong> and
            <strong>COORD_Y</strong>) among the attributes of the cluster analysis. Again, we set k=4, but keep all
            other default settings.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/3_628_k4_c_vars.png" alt="Variables for benchmark case" width="50%" />
            <p class="caption">
              Variables for benchmark case
            </p>
          </div>
          <p>The resulting cluster map is still far from contiguous. Of the four groups, both group 2 and group 3
            achieve contiguity among their elements. But group 1 still consists of three parts (including two
            singletons), and group 4 consists of four parts (including two singletons).</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/3_629_k4_c_map.png" alt="K Means cluster map with centroids (k=4)" width="80%" />
            <p class="caption">
              K Means cluster map with centroids (k=4)
            </p>
          </div>
          <p>The summary now also includes the x, y coordinates of the cluster centers among the descriptive statistics.
            The overall between cluster dissimilarity increased slightly relative to the base case, to 0.459. Note that
            this measure now includes the geometric coordinates as part of the dissimilarity measure, so the resulting
            ratio is not really comparable to the base case.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/3_630_k4_c_summary.png" alt="K Means cluster map with centroids summary (k=4)"
              width="80%" />
            <p class="caption">
              K Means cluster map with centroids summary (k=4)
            </p>
          </div>
        </div>
      </div>
      <div id="weighted-optimization" class="section level3 unnumbered">
        <h3>Weighted optimization</h3>
        <p>The previous approach included the x and y coordinates as individual variables treated in exactly the same
          manner as the other attributes (the algorithm does not know that these are coordinates). In contrast, in the
          weighted optimization, the coordinate variables are treated separately from the regular attributes, in the
          sense that there are now <em>two objective functions</em>. One is focused on the similarity of the regular
          attributes (in our example, the six variables), the other on the similarity of the geometric centroids. A
          weight changes the relative importance of each objective.</p>
        <p>To illustrate this, we first consider the case where all the weight is given to the geometric coordinates. We
          select the same six variables as before, <strong>not</strong> including the coordinates. In the K-Means
          Settings dialog, we check the box next to <strong>Use geometric centroids</strong>. For now, we leave the
          weight to the value of <strong>1.00</strong>. This means that the actual attributes (the six variables) are
          excluded from the optimization process. The K-Means algorithm is applied to the x, y coordinates only. By
          setting the weight, the coordinates do not need to be specified explicitly, all the calculations are done in
          background.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/4_631_weights1.png" alt="Weighted K Means weights setting" width="60%" />
          <p class="caption">
            Weighted K Means weights setting
          </p>
        </div>
        <p>The resulting cluster map shows four compact regions with all members of each cluster contiguous to another
          member.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/4_632_weights1_map.png" alt="Weighted K Means cluster map (w=1,k=4)" width="80%" />
          <p class="caption">
            Weighted K Means cluster map (w=1,k=4)
          </p>
        </div>
        <p>Even though the clustering algorithm ignores the actual attributes, the resulting properties can still be
          calculated. As is to be expected, the results are not that good relative to the <em>pure</em> K-Means. The
          overall ratio of between to total sum of squares is 0.246, compared to 0.433 in the base case.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/4_634_k4_w1_summary.png" alt="Weighted K Means cluster summary (w=1,k=4)" width="80%" />
          <p class="caption">
            Weighted K Means cluster summary (w=1,k=4)
          </p>
        </div>
        <p>With the weight set to <strong>0</strong>, the same result as in the base case is obtained (not illustrated
          here). Before moving to an optimization strategy, we check the results with the weight set at
          <strong>0.5</strong>.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/4_635_k4_w50_map.png" alt="Weighted K Means cluster map (w=0.50,k=4)" width="80%" />
          <p class="caption">
            Weighted K Means cluster map (w=0.50,k=4)
          </p>
        </div>
        <p>The cluster map shows four contiguous subregions, but different from the result obtained in the pure centroid
          solution.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/4_636_k4_w50_summary.png" alt="Weighted K Means cluster summary (w=0.50,k=4)" width="80%" />
          <p class="caption">
            Weighted K Means cluster summary (w=0.50,k=4)
          </p>
        </div>
        <p>Relative to the pure centroid solution, the overall characteristics have improved substantially, to 0.347 for
          the between to total sum of squares ratio, but still well below the benchmark a-spatial k-means.</p>
      </div>
      <div id="an-optimization-bisection-search" class="section level3 unnumbered">
        <h3>An optimization bisection search</h3>
        <p>The selection of a value for the weight in the geometric centroids setting constitutes a compromise between
          attribute similarity (best solution for w=0) and locational similarity (best solution for w=1). We could thus
          approach the selection of a value for the weight as an optimization problem, where we try to maximize the
          between to total ratio of the sum of squares, while ensuring contiguity among cluster members.</p>
        <p>In a small example like the 85 French departments, we can pursue this <em>manually</em> by means of a
          bisection search. The logic behind the bisection search is to enclose the optimal solution between an upper
          and a lower bound for a parameter. At each step, a mid-point is selected that brings us closer to an optimal
          solution.</p>
        <p>In the cluster example, the natural bounds are 0 (pure K-means) and 1 (pure centroids). The midpoint is 0.50.
          As we just observed, with this value for the weight, the solution is contiguous. This implies that we can do
          better on the ratio objective by moving closer to the full K-means solution. We thus take as the next value
          for the weight the midpoint between 0 and 0.50, i.e., 0.25, and check for contiguity.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/5_637_k4_w25map.png" alt="Weighted K Means cluster map (w=0.25,k=4)" width="80%" />
          <p class="caption">
            Weighted K Means cluster map (w=0.25,k=4)
          </p>
        </div>
        <p>The result has a better ratio of 0.417, but two of the regions no longer have contiguous members, with group
          4 consisting of four different sub-regions.</p>
        <p>Given that 0.25 does not satisfy the contiguity criterion, we now have to move to the right, to the midpoint
          between 0.25 and 0.50, or 0.375.</p>
        <p>This yields a map that almost meets the contiguity constraint, with a ratio of 0.368. Cluster 1 has one
          observation that is geographically separated from the rest of the cluster.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/5_638_k4_w375map.png" alt="Weighted K Means cluster map (w=0.375,k=4)" width="80%" />
          <p class="caption">
            Weighted K Means cluster map (w=0.375,k=4)
          </p>
        </div>
        <p>This means that the next step should be the midpoint between 0.375 and 0.5, or 0.4375. We continue the
          process until a contiguous solution provides no further meaningful improvement in the sum of squares ratio. In
          our example, this is for a weight of 0.4500, which yields the following map, with a sum or squares ratio of
          0.361.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/5_639_k4_4500_map.png" alt="Weighted K Means cluster map (w=0.4500,k=4)" width="80%" />
          <p class="caption">
            Weighted K Means cluster map (w=0.4500,k=4)
          </p>
        </div>
        <p>This is the best we can do in terms of the between to total sum of squares ratio while ensuring contiguity
          among the members of each cluster. We can now repeat this process for different values of k to assess which
          combination of number of clusters and centroid weights yields the most satisfactory solution.</p>
      </div>
    </div>
    <div id="skater" class="section level2 unnumbered">
      <h2>Skater</h2>
      <div id="principle-1" class="section level3 unnumbered">
        <h3>Principle</h3>
        <p>An approach that explicitly takes into account the contiguity constraints in the clustering process is based
          on the so-called skater algorithm outlined in <span class="citation">Assuncao et al. (2006)</span>. The
          algorithm carries out a <em>pruning</em> of the <em>minimum spanning tree</em> created from the spatial
          weights matrix for the observations. The <em>weights</em> in the spatial weights matrix correspond to the
          pair-wise dissimilarity between observations, which become the edges in the graph representation of the
          weights (the observations are the nodes).</p>
        <p>Starting from this weights matrix, a minimum spanning tree is obtained, which is a path that connects all
          observations (nodes), but each is only visited once. In other words, the n nodes are connected by n-1 edges,
          such that the between-node dissimilarity is minimized. This tree is then pruned by selecting the edge whose
          removal increases the objective function (between group dissimilarity) the most. The process continues until
          convergence. This is a hierarchical process, in that once the tree is cut at one point, all subsequent cuts
          are limited to the resulting subtrees. In other words, once an observation ends up in a pruned branch of the
          tree, it cannot switch back to a previous branch. This is sometimes viewed as a limitation of this algorithm.
        </p>
      </div>
      <div id="implementation" class="section level3 unnumbered">
        <h3>Implementation</h3>
        <p>In GeoDa, the skater algorithm is invoked from the <strong>Clusters</strong> toolbar icon, or from the main
          menu as <strong>Clusters &gt; Skater</strong>.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/6_640_skater_item.png" alt="Skater cluster option" width="25%" />
          <p class="caption">
            Skater cluster option
          </p>
        </div>
        <p>Selecting this option brings up the <strong>Skater Settings</strong> dialog, which has essentially the same
          structure as the corresponding dialog for K-Means and other classic clustering approaches. The left panel
          provides a way to select the variables, the number of clusters and different options to determine the
          clusters. Different from previous approaches is the presence of a <strong>Weigths</strong> drop down list,
          where the contiguity weights must be specified. The skater algorithm does not work without a spatial weights
          file.</p>
        <p>The <strong>Distance Function</strong>, <strong>Transformation</strong> and <strong>seed</strong> otpions
          work in the same manner as for classic clustering. Again, at the bottom of the dialog, we specify the
          <strong>Field</strong> or variable name where the classification will be saved.</p>
        <p>We consider the <strong>Minimum Bound</strong> and <strong>Min Region Size</strong> options below.</p>
        <p>We select the same six variables as before: <strong>Crm_prs</strong>, <strong>Crm_prp</strong>,
          <strong>Litercy</strong>, <strong>Donatns</strong>, <strong>Infants</strong>, and <strong>Suicids</strong>.
          The spatial weights are based on queen contiguity (<strong>Guerry_85_q</strong>).</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/6_641_skater_settings.png" alt="Skater cluster settings" width="50%" />
          <p class="caption">
            Skater cluster settings
          </p>
        </div>
        <p>Clicking on <strong>Run</strong> brings up the skater cluster map as a separate window and lists the cluster
          characteristics in the <strong>Summary</strong> panel.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/6_642_skater_clustermap4.png" alt="Skater cluster map (k=4)" width="80%" />
          <p class="caption">
            Skater cluster map (k=4)
          </p>
        </div>
        <p>The spatial clusters generated by skater follow the minimum spanning tree closely and tend to reflect the
          hierarchical nature of the pruning. In this example, the country initially gets divided into three largely
          horizontal regions, of which the top one gets split into an eastern and a western part.</p>
        <p>The ratio of between sum of squares to total sum of squares is 0.316, somewhat lower than the 0.361 we
          obtained in the bisection weighted optimization above.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/6_643_skater_summary_4.png" alt="Skater cluster summary (k=4)" width="80%" />
          <p class="caption">
            Skater cluster summary (k=4)
          </p>
        </div>
        <p>The hierarchical nature of the skater algorithm can be further illustrated by increasing the number of
          clusters to <strong>6</strong> (all other options remain the same). The resulting cluster map shows how the
          previous regions 2 and 3 each get split into an eastern and a western part.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/6_644_skatermap_k6.png" alt="Skater cluster map (k=6)" width="80%" />
          <p class="caption">
            Skater cluster map (k=6)
          </p>
        </div>
        <p>The ratio of between sum or squares to total sum of squares is now 0.420.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/6_645_skatersummary_k6.png" alt="Skater cluster summary (k=6)" width="80%" />
          <p class="caption">
            Skater cluster summary (k=6)
          </p>
        </div>
        <div id="setting-a-minimum-cluster-size" class="section level4 unnumbered">
          <h4>Setting a minimum cluster size</h4>
          <p>In several applications of spatially constrained clustering, a minimum cluster size needs to be taken into
            account. For example, this is the case when the new regional groupings are intended to be used in a
            computation of rates. In those instances, the denominator should be sufficiently large to avoid extreme
            variance instability, which is accomplished by setting a minimum population size.</p>
          <p>In GeoDa, there are two options to implement this additional constraint. One is through the <strong>Minimum
              Bound</strong> settings, as shown below. The check box activates a drop down list of variables where a
            <em>spatialy extensive</em> variable can be selected for the minimum bound constraint, such as the
            population (or, in other examples, total number of households, housing units, etc.). In our example, we
            select the departmental population, <strong>Pop1831</strong>. The default is to take <strong>10%</strong> of
            the total over all observations as the minimum bound. This can be adjusted by means of a slider bar (to set
            the percentage), or by typing in a different value in the minimum bound box. Here, we take the default.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/7_646_minbound.png" alt="Skater minimum bound settings" width="50%" />
            <p class="caption">
              Skater minimum bound settings
            </p>
          </div>
          <p>The resulting spatial alignment of clusters is quite different from the unconstrained solution, but again
            can be seen to be the result of a hierarchical subdivision of the three large horizontal subregions. In this
            case, the norther most region is divided into three, and the southernmost one into two subregions. Compared
            to the unconstrained solution, where the clusters consisted of 29, 28, 11, 8, 5, and 4 departments, the
            constrained regions have a much more even distribution of 17, 16 (twice), and 12 (three times) elements.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/7_648_skater6_pop10_map.png" alt="Skater cluster map, min pop 10% (k=6)" width="80%" />
            <p class="caption">
              Skater cluster map, min pop 10% (k=6)
            </p>
          </div>
          <p>The effect of the constraint is to lower the objective function from 0.420 to 0.374. The within-cluster sum
            of squares of the six regions is much more evenly distributed than in the unconstrained solution, due to the
            similar number of elements in each cluster.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/7_649_skater6_pop10_summary.png" alt="Skater cluster summary, min pop 10% (k=6)"
              width="80%" />
            <p class="caption">
              Skater cluster summary, min pop 10% (k=6)
            </p>
          </div>
          <p>The second way to constrain the regionalization process is to specify a minimum number of units that each
            cluster should contain. Again, this is a way to ensure that all clusters have somewhat similar size,
            although it is not based on a substantive variable, only on the number of elemental units. The constraint is
            set in the <strong>Min Region Size</strong> box of the parameters panel. In our example, we set the value to
            <strong>12</strong>, i.e., the minimum size obtained from the population constraint above.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/7_650_minregionsize.png" alt="Skater minimum region size settings" width="50%" />
            <p class="caption">
              Skater minimum region size settings
            </p>
          </div>
          <p>The result is identical to that generated by the minimum population constraint, except for the cluster
            labels (and colors).</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/7_651_skater8_min12.png" alt="Skater cluster map, min size 12 (k=6)" width="80%" />
            <p class="caption">
              Skater cluster map, min size 12 (k=6)
            </p>
          </div>
          <p>Note that the minimum region size setting will override the number of clusters when it is set too high. For
            example, setting the minimum cluster size to 14 will only yield clusters with k=5. Again, this is a
            consequence of the hierarchical nature of the minimum spanning tree pruning used in the skater algorithm.
            More specifically, a given sub-tree may not have sufficient nodes to allow for further subdivisions that
            meet the minimum size requirement. A similar problem occurs when the mininum population size is set too
            high.</p>
        </div>
      </div>
    </div>
    <div id="max-p-region-problem" class="section level2 unnumbered">
      <h2>Max-p Region Problem</h2>
      <div id="principle-2" class="section level3 unnumbered">
        <h3>Principle</h3>
        <p>The max-p region problem, outlined in <span class="citation">Duque, Anselin, and Rey (2012)</span>, makes the
          number of regions (k) part of the solution process. This is accomplished by introducing a minimum size
          constraint for each cluster. In contrast to the skater method, where this was optional, for max-p the
          <em>constraint is required</em>.</p>
        <p>The algorithm itself consists of a search process that starts with an initial feasible solution and
          iteratively improves upon it while maintaining contiguity among the elements of each cluster. This can take
          some time, especially for larger data sets. In addition, the iterative process tends to be quite sensitive to
          the starting point, so an extensive sensitivity analysis is highly recommended.</p>
      </div>
      <div id="implementation-1" class="section level3 unnumbered">
        <h3>Implementation</h3>
        <p>The max-p option is available from the main menu as <strong>Clusters &gt; Max-p</strong> or as a similarly
          named option from the <strong>Clusters</strong> toolbar icon.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/8_654_maxp.png" alt="Max-p cluster option" width="25%" />
          <p class="caption">
            Max-p cluster option
          </p>
        </div>
        <p>The variable settings dialog is similar to the one for the skater algorithm. We select the same six
          variables. Also, spatial weights need to be specified in the <strong>Weights</strong> drop down list (here, we
          take first order queen contiguity as in <strong>Guerry_q_85</strong>). In addition, we have to set the
          <strong>MIninum Bound</strong> variable and threshold. In our example, we again select
          <strong>Pop1831</strong> and take the <strong>10%</strong> default. Finally, we set the number of
          <strong>Iterations</strong> to <strong>1000</strong> and leave all other options to their default settings. We
          specify the <strong>Field</strong> for the resulting cluster membership as <strong>CL_p1k</strong>, as shown
          below.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/8_665_maxpvars.png" alt="Max-p variables and parameter settings" width="50%" />
          <p class="caption">
            Max-p variables and parameter settings
          </p>
        </div>
        <p>The resulting cluster map yields eight clusters, consisting of 14 to 8 elements.</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/8_659_maxp1k_map.png" alt="Max-p cluster map, 1000 iterations" width="80%" />
          <p class="caption">
            Max-p cluster map, 1000 iterations
          </p>
        </div>
        <p>The corresponding ratio of between sum of squares to total sum of squares is 0.511. The within sum of squares
          are fairly evenly distributed, ranging from a low of 11.2 for cluster 5 (9 elements) to a high of 49.2 for
          cluster 6 (also with 9 elements).</p>
        <div class="figure" style="text-align: center">
          <img src="pics8/8_660_maxp1k_summary.png" alt="Max-p cluster summary, 1000 iterations" width="80%" />
          <p class="caption">
            Max-p cluster summary, 1000 iterations
          </p>
        </div>
      </div>
      <div id="sensitivity-analysis" class="section level3 unnumbered">
        <h3>Sensitivity analysis</h3>
        <p>The max-p optimization algorithm is an iterative process, that moves from an initial feasible solution to a
          superior solution. However, this process may be slow and can get trapped in local optima. Hence, it behooves
          us to carry out an extensive sensitivity analysis. There are two main approaches to implement this. One is to
          increase the number of iterations, starting from the same initial feasible solution. An alternative approach
          is to adjust the initial feasible solution (the starting point for the search), for example, to start from a
          solution from an earlier set of iterations. Subsequent iterations may improve on this solution.</p>
        <p>The motivation behind the sensitivity analysis is to continue iterating until the resulting solution is
          stable, i.e., it no longer changes and it reaches a maximum for the objective function.</p>
        <div id="changing-the-number-of-iterations" class="section level4 unnumbered">
          <h4>Changing the number of iterations</h4>
          <p>The first aspect we consider is to increase the number of iterations. This provides a larger set of
            alternatives that are considered in the local search process and thereby facilitates moving out of a
            possible <em>local</em> maximum. For example, we can increase the number of iterations to
            <strong>5000</strong>, which yields the cluster map below. The corresponding ratio of between sum of squares
            to total sum of squares is 0.519, slightly better than the solution for 1000 iterations (0.511).</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_661_maxp5k_map.png" alt="Max-p cluster map, 5000 iterations" width="80%" />
            <p class="caption">
              Max-p cluster map, 5000 iterations
            </p>
          </div>
          <p>As it turns out, this solution can be improved upon by increasing the number of iterations to 10000. The
            associated cluster map is slightly different from the solutions obtained before.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_663_maxp10k_map.png" alt="Max-p cluster map, 10000 iterations" width="80%" />
            <p class="caption">
              Max-p cluster map, 10000 iterations
            </p>
          </div>
          <p>The characteristics reveal a very even distribution among clusters, with an overall ratio of between sum of
            squares to total sum of squares of 0.525, again slightly better.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_664_maxp10k_summary.png" alt="Max-p cluster summary, 10000 iterations" width="80%" />
            <p class="caption">
              Max-p cluster summary, 10000 iterations
            </p>
          </div>
          <p>Further increases in the number of iterations do not yield improvements in the objective function. In other
            words, after 10000 iterations, the same result is obtained, which suggests that the algorithm has yielded a
            solution that cannot be improved upon with this particular starting point (the default random assignment).
            However, because of the iterative nature of the algorithm, there is no actual guarantee that a global
            optimum was obtained.</p>
        </div>
        <div id="setting-initial-groups" class="section level4 unnumbered">
          <h4>Setting initial groups</h4>
          <p>An alternative or complement to increasing the number of iterations is to adjust the starting point for the
            search. We can specify a feasible initial solution by means of the <strong>Initial Groups</strong> option in
            the panel. This refers to a categorical variable that assigns each observation to a cluster. Note this
            solution has to be <em>feasible</em>, i.e., it must conform to the contiguity constraint. Good candidates
            for the initial groups are the solutions obtained in an earlier iteration, since these are guaranteed to be
            feasible. Here, we use the outcome of the 1000 initial iterations, stored in the variable
            <strong>CL_mp1</strong>. We set the number of iterations to <strong>10000</strong>.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_679_initialgroup.png" alt="Initial group setting" width="50%" />
            <p class="caption">
              Initial group setting
            </p>
          </div>
          <p>The result is again slightly different from the previous solution.</p>
          <p>In order to make the two maps more comparable, we use the category label switching feature of the unique
            maps in GeoDa (see the geovisualization chapter) to match the colors and labels of clusters 2, 3 and 4. The
            resulting cluster map is shown below.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_667_maxp1K_10K_map.png" alt="Max-p cluster map, 10000 iterations after CL1k"
              width="80%" />
            <p class="caption">
              Max-p cluster map, 10000 iterations after CL1k
            </p>
          </div>
          <p>At first sight, the maps seem the same, with an identical distribution of departments by cluster, except
            for Cluster 2 (from 12 to 11), as shown in the map legend. Careful examination of the two maps reveals a
            difference in the allocation of two departments, one moving from Cluster 6 to Cluster 4 (using the new
            consistent labeling between the two maps), the other moving from Cluster 2 to Cluster 6. This only affects
            the total in Cluster 2.</p>
          <p>We can illustrate this by means of GeoDa’s <strong>co-location map</strong>. First, we save the categories
            from the adjusted cluster map (the new categories no longer match the original categorical variable). Then,
            we create a co-location map for the first cluster map, <strong>CL_mp10</strong>, and the saved categories,
            say <strong>new_mp11</strong>. The resulting unique values map (ignore the color scheme, but the category
            labels match) clearly shows the two locations that do not match between the two cluster maps.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_682_maxp_colocatino.png" alt="Max-p cluster co-location map" width="80%" />
            <p class="caption">
              Max-p cluster co-location map
            </p>
          </div>
          <p>Selecting these locations in the co-location map will also highlight them in the two cluster maps (through
            the process of linking), which clearly illustrates the two swaps.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_681_maxp_compared.png" alt="Max-p cluster map comparisons" width="100%" />
            <p class="caption">
              Max-p cluster map comparisons
            </p>
          </div>
          <p>The summary characteristics turn out to be very similar to the previous solution, but with a slightly
            better value for the ratio objective, which now is 0.526.</p>
          <div class="figure" style="text-align: center">
            <img src="pics8/8_668_maxp1K_10K_summary.png" alt="Max-p cluster summary, 10000 iterations after CL1"
              width="80%" />
            <p class="caption">
              Max-p cluster summary, 10000 iterations after CL1
            </p>
          </div>
          <p>In practice, further experimentation with initial starting values and the number of iterations should be
            carried out to ensure that the solution is stable.</p>
        </div>
      </div>
    </div>
    <div id="summary" class="section level2 unnumbered">
      <h2>Summary</h2>
      <p>Finally, we run each method using k=8 to allow for comparison with max-p, and summarize the value of the
        objective function in the table below. In this particular example, the max-p method obtains the solution that is
        closest to the unconstrained K-means.</p>
      <table>
        <thead>
          <tr class="header">
            <th>Method</th>
            <th>Ratio</th>
          </tr>
        </thead>
        <tbody>
          <tr class="odd">
            <td>K-Means</td>
            <td>0.627</td>
          </tr>
          <tr class="even">
            <td>K-Means (centroids)</td>
            <td>0.632</td>
          </tr>
          <tr class="odd">
            <td>K-Means weighted 1.0</td>
            <td>0.357</td>
          </tr>
          <tr class="even">
            <td>K-Means weighted 0.89</td>
            <td>0.429</td>
          </tr>
          <tr class="odd">
            <td>Skater</td>
            <td>0.496</td>
          </tr>
          <tr class="even">
            <td>Max-p</td>
            <td>0.526</td>
          </tr>
        </tbody>
      </table>
      <p><br></p>
    </div>
    <div id="references" class="section level2 unnumbered">
      <h2>References</h2>
      <div id="refs" class="references">
        <div id="ref-Assuncaoetal06">
          <p>Assuncao, Renato, M Neves, G Camara, and C Da Costa Freitas. 2006. “Efficient Regionalization Techniques
            for Socio-Economic Geographical Units Using Minimum Spanning Trees.” <em>International Journal of
              Geographical Information Science</em> 20: 797–811.</p>
        </div>
        <div id="ref-Duqueetal12">
          <p>Duque, Juan C., Luc Anselin, and Sergio J. Rey. 2012. “The Max-P Regions Problem.” <em>Journal of Regional
              Science</em> 52: 397–419.</p>
        </div>
        <div id="ref-Hainingetal00">
          <p>Haining, R, S Wise, and J Ma. 2000. “Designing and Implementing Software for Spatial Statistical Analysis
            in a GIS Environment.” <em>Journal of Geographical Systems</em> 2: 257–86.</p>
        </div>
      </div>
    </div>
    <div class="footnotes">
      <hr />
      <ol>
        <li id="fn1">
          <p>University of Chicago, Center for Spatial Data Science – <a
              href="mailto:anselin@uchicago.edu">anselin@uchicago.edu</a><a href="#fnref1">↩</a></p>
        </li>
        <li id="fn2">
          <p>For detailed instructions, see the previous chapter.<a href="#fnref2">↩</a></p>
        </li>
      </ol>
    </div>

  </section>

  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
  <script type="text/javascript">
    try {
      var pageTracker = _gat._getTracker("UA-72724100-1");
      pageTracker._trackPageview();
    } catch (err) { }
  </script>


  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>

</html>