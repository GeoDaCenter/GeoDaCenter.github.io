<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Luc Anselin" />


<title>Cluster Analysis (1)</title>

<script src="lab7b_files/header-attrs-2.3/header-attrs.js"></script>
<link href="lab7b_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lab7b_files/highlightjs-9.12.0/highlight.js"></script>
    <title>GeoDa on Github</title>

    <style>
    	*{margin:0;padding:0;}
	    .shadowfilter {
	       -webkit-filter: drop-shadow(12px 12px 7px rgba(0,0,0,0.5));
	        filter: url(shadow.svg#drop-shadow);
	     }
	     .intro1 { margin-left: -45px;}
    </style>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
    <style>
    ul {padding-left:30px;}
	figcaption {
	  top: .70em;
   	  left: .35em;
 	  bottom: auto!important;
	  right: auto!important;
	}
    </style>

        <style>
    h1 {
        text-align: center;
    }
    h3.subtitle {
        text-align: center;
    }
    h4.author {
        text-align: center;
    }
    h4.date {
        text-align: center;
    }
    p.caption {
        font-size : 12px;
    }
    </style>

<!-- Google Analytics -->
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-72724100-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<!-- End Google Analytics -->
<!-- Google Tag Manager -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-53RVF8"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-53RVF8');</script>
<!-- End Google Tag Manager -->

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








</head>

<body>


    <section class="page-header">
      <h1 class="project-name">GeoDa</h1>
      <h2 class="project-tagline">An Introduction to Spatial Data Analysis</h2>
      <a href="https://geodacenter.github.io/index.html" class="btn">Homepage</a>
      <a href="https://geodacenter.github.io/download.html" class="btn">Download</a>
      <a href="https://github.com/GeoDaCenter/geoda/" class="btn">View on GitHub</a>
      <a href="https://spatial.uchicago.edu/sample-data"  target="_blank" class="btn">Data</a>
       <a href="https://geodacenter.github.io/documentation.html" class="btn">Documentation</a>
       <a href="https://geodacenter.github.io/support.html" class="btn">Support</a>
       <a href="https://geodacenter.github.io/index-cn.html" class="btn">中文</a>
    </section>

    <section class="main-content">


<h1 class="title toc-ignore">Cluster Analysis (1)</h1>
<h3 class="subtitle">K-Means Clustering</h3>
<h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
<h4 class="date">07/29/2020 (latest update)</h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#objectives">Objectives</a>
<ul>
<li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
</ul></li>
</ul></li>
<li><a href="#k-means">K Means</a>
<ul>
<li><a href="#principle">Principle</a>
<ul>
<li><a href="#partitioning-methods">Partitioning methods</a></li>
<li><a href="#k-means-objective-function">K-means objective function</a></li>
<li><a href="#iterative-relocation">Iterative relocation</a></li>
<li><a href="#the-choice-of-k">The choice of K</a></li>
<li><a href="#k-means-algorithms">K-means algorithms</a></li>
</ul></li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#variable-settings-panel">Variable Settings panel</a></li>
<li><a href="#cluster-results">Cluster results</a></li>
<li><a href="#adjusting-cluster-labels">Adjusting cluster labels</a></li>
<li><a href="#cluster-variables">Cluster variables</a></li>
<li><a href="#saving-the-cluster-results">Saving the cluster results</a></li>
</ul></li>
<li><a href="#options-and-sensitivity-analysis">Options and sensitivity analysis</a>
<ul>
<li><a href="#changing-the-number-of-k-means-initial-runs">Changing the number of k-means++ initial runs</a></li>
<li><a href="#random-initialization">Random initialization</a></li>
<li><a href="#selecting-a-different-standardization">Selecting a different standardization</a></li>
<li><a href="#setting-a-minimum-bound">Setting a minimum bound</a></li>
<li><a href="#elbow-plot">Elbow plot</a></li>
</ul></li>
</ul></li>
<li><a href="#cluster-categories-as-variables">Cluster Categories as Variables</a>
<ul>
<li><a href="#conditional-plots">Conditional plots</a></li>
<li><a href="#aggregation-by-cluster">Aggregation by cluster</a></li>
</ul></li>
<li><a href="#clustering-with-dimension-reduction">Clustering with Dimension Reduction</a>
<ul>
<li><a href="#pca">PCA</a></li>
<li><a href="#mds">MDS</a></li>
</ul></li>
<li><a href="#appendix">Appendix</a>
<ul>
<li><a href="#equivalence-of-euclidean-distances-and-sum-of-squared-errors-sse">Equivalence of Euclidean distances and Sum of Squared Errors (SSE)</a></li>
<li><a href="#k-means-worked-example">K-means worked example</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p><br></p>
<div id="introduction" class="section level2 unnumbered" number="">
<h2>Introduction</h2>
<p>We now move from reducing the dimensionality of the variables to reducing the number of observations, or <em>clustering</em>. In general terms, clustering methods group <em>n</em> observations into <em>k</em> <strong>clusters</strong> such that
the intra-cluster similarity is maximized, and the
between-cluster similarity is minimized. Equivalently, we can think of it as minimizing intra-cluster
dissimilarity and maximizing between-cluster dissimilarity.</p>
<p>In other words, the goal of clustering methods is to achieve compact groups of <em>similar observations</em> that are separated as much as possible from the other
groups.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>There are a very large number of clustering techniques and algorithms. They are standard tools of so-called <em>unsupervised learning</em> and constitute a core element in
any machine learning toolbox. Classic texts include <span class="citation">Hartigan (<a href="#ref-Hartigan:75" role="doc-biblioref">1975</a>)</span>, <span class="citation">Jain and Dubes (<a href="#ref-JainDubes:88" role="doc-biblioref">1988</a>)</span>,
<span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>, and <span class="citation">Everitt et al. (<a href="#ref-Everittetal:11" role="doc-biblioref">2011</a>)</span>. A fairly recent overview of methods can be found in <span class="citation">Jain (<a href="#ref-Jain:10" role="doc-biblioref">2010</a>)</span>. In addition, excellent treatments of some of the more technical aspects are contained in Chapter 14 of <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-Hastieetal:09" role="doc-biblioref">2009</a>)</span>, Chapters 10 and 11 of <span class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12" role="doc-biblioref">2012</a>)</span>, and Chapter 10 of <span class="citation">James et al. (<a href="#ref-Jamesetal:13" role="doc-biblioref">2013</a>)</span>, among others.</p>
<p>Clustering methods can be organized along a number of different dimensions. A common distinction is between partitioning methods, hierarchical methods, density-based methods and grid-based methods <span class="citation">(see, e.g., Han, Kamber, and Pei <a href="#ref-Hanetal:12" role="doc-biblioref">2012</a>, 448–50)</span>. In addition, there are model-based approaches developed in the statistical literature, such as Gaussian mixture models (GMM) and Bayesian clusters.</p>
<p>In this and the next two chapters, we focus on the two most common approaches, namely <em>partitioning</em> methods and <em>hierarchical</em> methods. We do not cover the model-based techniques, since they are less compatible with an exploratory mindset (more precisely, they require a formal probabilistic model). We also restrict our discussion to exact clustering methods and do not consider fuzzy clustering (where an observation may belong to more than one cluster). In exact clustering, the clusters are both <em>exhaustive</em> and <em>exclusive</em>. This means that every observation must belong to one cluster, and only one cluster.</p>
<p>In the current chapter, we deal with <strong>k-means</strong> clustering, the most familiar example of a partitioning method. Hierarchical clustering is covered in the next chapter and more advanced techniques in a third.</p>
<p>To illustrate these methods, we will continue to use the Guerry data set on
moral statistics in 1830 France, which comes pre-installed with <code>GeoDa</code>.</p>
<div id="objectives" class="section level3 unnumbered" number="">
<h3>Objectives</h3>
<ul>
<li><p>Understand the principles behind k-means clustering</p></li>
<li><p>Know the requirements to carry out k-means clustering</p></li>
<li><p>Interpret the characteristics of a cluster analysis</p></li>
<li><p>Carry out a sensitivity analysis to various parameters</p></li>
<li><p>Impose a bound on the clustering solutions</p></li>
<li><p>Use an elbow plot to pick the best k</p></li>
<li><p>Use the cluster categories as a variable</p></li>
<li><p>Combine dimension reduction and cluster analysis</p></li>
</ul>
<div id="geoda-functions-covered" class="section level4 unnumbered" number="">
<h4>GeoDa functions covered</h4>
<ul>
<li>Clusters &gt; K Means
<ul>
<li>select variables</li>
<li>select k-means starting algorithms</li>
<li>select standardization methods</li>
<li>k-means characteristics</li>
<li>mapping the clusters</li>
<li>changing the cluster labels</li>
<li>saving the cluster classification</li>
<li>setting a minimum bound</li>
</ul></li>
<li>Explore &gt; Conditional Plot &gt; Box Plot</li>
<li>Table &gt; Aggregate</li>
<li>Tools &gt; Dissolve</li>
</ul>
<p><br></p>
</div>
</div>
</div>
<div id="k-means" class="section level2 unnumbered" number="">
<h2>K Means</h2>
<div id="principle" class="section level3 unnumbered" number="">
<h3>Principle</h3>
<div id="partitioning-methods" class="section level4 unnumbered" number="">
<h4>Partitioning methods</h4>
<p>A <em>partitioning</em> clustering method consists of assigning each observation to one out of <span class="math inline">\(k\)</span> clusters. For each observation <span class="math inline">\(i\)</span>, this can be symbolized by an <em>encoder</em> C, such that <span class="math inline">\(C(i) = h\)</span>, where <span class="math inline">\(h\)</span>, the cluster indicator is an element from the set <span class="math inline">\(\{1, \dots, k\}\)</span>. The cluster labels (<span class="math inline">\(h\)</span>) are meaningless, and could just as well be letters or other distinct symbols.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>The overall objective is to end up with a grouping that minimizes the dissimilarity within each cluster. Mathematically, we can think of an overall loss function that consists of summing the <em>distances</em> between all the pairs of observations (in general, any measure of dissimilarity):
<span class="math display">\[T = (1/2) \sum_{i = 1}^n \sum_{j = 1}^n d_{ij},\]</span>
where <span class="math inline">\(d_{ij}\)</span> is some measure of dissimilarity, such as the Euclidean distance between the values at observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The objective is then to find a grouping of the <span class="math inline">\(i\)</span> into clusters <span class="math inline">\(h\)</span> that minimizes this loss function, or, alternatively, maximizes the similarity (<span class="math inline">\(-T\)</span>).</p>
<p>In any given cluster <span class="math inline">\(h\)</span>, we can separate out the distances from each of its members (<span class="math inline">\(i \in h\)</span>) to all other observations between those that belong to the cluster (<span class="math inline">\(j \in h\)</span>) and those that do not (<span class="math inline">\(j \notin h\)</span>):
<span class="math display">\[T_{i \in h} = (1/2) [ \sum_{i \in h} \sum_{j \in h} d_{ij} + \sum_{i \in h} \sum_{j \notin h} d_{ij} ],\]</span>
and, for all the clusters, as:
<span class="math display">\[T = (1/2) (\sum_{h=1}^k [ \sum_{i \in h} \sum_{j \in h} d_{ij} + \sum_{i \in h} \sum_{j \notin h} d_{ij}]) = W + B,\]</span>
where the first term (<span class="math inline">\(W\)</span>) is referred to as the <em>within</em> dissimilarity and the second (<span class="math inline">\(B\)</span>) as the <em>between</em> dissimilarity. In other words, the total dissimilarity decomposes into one part due to what happens within each cluster and another part that pertains to the between cluster dissimilarities. <span class="math inline">\(W\)</span> and <span class="math inline">\(B\)</span> are complementary, so the lower <span class="math inline">\(W\)</span>, the higher <span class="math inline">\(B\)</span>, and vice versa. We can thus attempt to find an optimum by <em>minimizing</em> the <em>within</em> dissimilarity,
<span class="math inline">\(W = (1/2) \sum_{h=1}^k \sum_{i \in h} \sum_{j \in h} d_{ij}\)</span>.</p>
<p>Partitioning methods differ in terms of how the dissimilarity <span class="math inline">\(d_{ij}\)</span> is defined and how the term <span class="math inline">\(W\)</span> is minimized. Complete enumeration of all the possible allocations is infeasible except for toy problems, and there is
no analytical solution. The problem is NP-hard, so the solution has to be approached by means of a heuristic, as an iterative descent process. This is accomplished through an algorithm that changes the assignment of observations to clusters so as to improve the objective function at each step. All feasible approaches are based on what is called <em>iterative greedy descent</em>. A greedy algorithm is one that makes a locally optimal decision at each stage. It is therefore not guaranteed to end up in a <em>global</em> optimum, but may get stuck in a <em>local</em> one instead. The k-means method uses an <em>iterative relocation</em> heuristic as the optimization strategy (see <a href="#iterative-relocation">Iterative relocation</a>). First, we consider the associated dissimilarity measure more closely.</p>
</div>
<div id="k-means-objective-function" class="section level4 unnumbered" number="">
<h4>K-means objective function</h4>
<p>The K-means algorithm is based on the squared Euclidean distance as the measure of dissimilarity:
<span class="math display">\[d_{ij}^2 = \sum_{v=1}^p (x_{iv} - x_{jv})^2 = ||x_i - x_j||^2,\]</span>
where we have changed our customary notation and now designate the number of variables/dimensions as <span class="math inline">\(p\)</span> (since <span class="math inline">\(k\)</span> is traditionally used to designate the number of clusters).</p>
<p>This gives the overall objective as finding the allocation <span class="math inline">\(C(i)\)</span> of each observation <span class="math inline">\(i\)</span> to a cluster
<span class="math inline">\(h\)</span> out of the <span class="math inline">\(k\)</span> clusters so as to minimize the within-cluster similarity over all <span class="math inline">\(k\)</span> clusters:
<span class="math display">\[\mbox{min}(W) = \mbox{min} (1/2) \sum_{h=1}^k \sum_{i \in h} \sum_{j \in h} ||x_i - x_j||^2,\]</span>
where, in general, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are <span class="math inline">\(p\)</span>-dimensional vectors.</p>
<p>A little bit of algebra shows how this simplifies to minimizing the squared difference between the values of the observations in each cluster and the corresponding cluster mean (see the <a href="#appendix">Appendix</a> for details):
<span class="math display">\[\mbox{min}(W) = \mbox{min} \sum_{h=1}^k n_h \sum_{i \in h} (x_i - \bar{x}_h)^2.\]</span></p>
<p>In other words, minimizing the sum of (one half) of all squared distances is equivalent to minimizing
the sum of squared deviations from the mean in each cluster, the <em>within</em> sum of squared errors.</p>
</div>
<div id="iterative-relocation" class="section level4 unnumbered" number="">
<h4>Iterative relocation</h4>
<p>The k-means algorithm is based on the principle of <em>iterative relocation</em>. In essence, this means that after an initial solution is established, subsequent moves (i.e., allocating observations to clusters) are made to improve the objective function. As mentioned, this is a greedy algorithm, that ensures that at each step the total across clusters of the within-cluster sums of squared errors (from the respective cluster means) is lowered. The algorithm stops when no improvement is possible. However, this does not ensure that a <em>global</em> optimum is achieved <span class="citation">(for an early discussion, see Hartigan and Wong <a href="#ref-HartiganWong:79" role="doc-biblioref">1979</a>)</span>. Therefore sensitivity analysis is essential. This is addressed by trying many different initial allocations (typically assigned randomly).</p>
<p>To illustrate the logic behind the algorithm, we consider a simple toy example of seven observations, as depicted in Figure <a href="#fig:kmexample">1</a>. The full numerical details are given in the <a href="#appendix">Appendix</a>. The plot shows the location of the seven points in two-dimensional <span class="math inline">\(X-Y\)</span> space, with the center (the mean of <span class="math inline">\(X\)</span> and the mean of <span class="math inline">\(Y\)</span>) shown in a lighter color (note that the center is <em>not</em> one of the observations). The objective is to group the seven points into two clusters.</p>
<div class="figure" style="text-align: center"><span id="fig:kmexample"></span>
<img src="pics7b/00_initial_step0.png" alt="K-means toy example" width="35%" />
<p class="caption">
Figure 1: K-means toy example
</p>
</div>
<p>The initial step of the algorithm is to <em>randomly</em> pick two <em>seeds</em>, one for each cluster. The seeds are actual observations, not some other random location. In our example, we selected observations 4 and 7, shown by the lighter color in the upper left panel of Figure <a href="#fig:kmsteps">2</a>. The other observations are allocated to the cluster whose seed they are closest to. We see that the green cluster has one other observation (6, in dark green), and the blue cluster four more (in dark blue). This is the initial layout, shown in the top right panel of the figure: two observations in one cluster and five in the other.</p>
<div class="figure" style="text-align: center"><span id="fig:kmsteps"></span>
<img src="pics7b/00_allsteps1_markup2.png" alt="Steps in the k-means algorithm" width="75%" />
<p class="caption">
Figure 2: Steps in the k-means algorithm
</p>
</div>
<p>Next, we compute the <em>center</em> for each cluster, i.e., the mean of each coordinate, shown in the figure in the lighter color. Note that from this point on, the center is no longer one of the observations (or only by coincidence).</p>
<p>The first real step of the algorithm allocates observations to the cluster center they are closest to. In the top right panel, we see that point 5 from the original blue cluster is <em>closer</em> to the new center for the green cluster than to the new center for the blue cluster. Consequently, it moves to join the green cluster, which now consists of three observations (in the lower left panel).</p>
<p>We repeat the process with the new centers in the lower left panel and again find point 4 from the blue cluster closer to the updated center for the green cluster, so it joins the latter.
This yields one cluster of four (green) observations (closest to the center in the upper right panel) and one of three (blue) observations.</p>
<p>This process is repeated until no more improvement is possible. In our example, we are done after the two steps, since all observations are closest to their center. The lower-right panel in the figure shows the final allocation with associated centers.</p>
<p>This solution corresponds to the allocation that would result from a Voronoi diagram or Thiessen polygon around the cluster centers. All observations are closer to their center than to any other center. This is indicated by the black line in the graph,
which is perpendicular at the midpoint to an imaginary line connecting the two centers and separates the area into two
compact <em>regions</em>.</p>
</div>
<div id="the-choice-of-k" class="section level4 unnumbered" number="">
<h4>The choice of K</h4>
<p>A key element in the k-means method is the choice of the number of clusters, k. Typically,
several values for k are considered, and the resulting clusters are then compared in terms of the
objective function.</p>
<p>Since the total sum of squared errors (SSE) equals the sum of the within-group SSE
and the total between-group SSE, a common criterion is to assess the ratio of the total
between-group sum of squares (BSS) to the total sum of squares (TSS), i.e., BSS/TSS. A higher value for this ratio suggests a better
separation of the clusters. However, since this ratio increases with k, the selection
of a <em>best</em> k is not straightforward. Several ad hoc rules have been suggested, but none is totally satisfactory.</p>
<p>One useful approach is to plot the objective function against increasing values of k. This could be either the within sum of squares (WSS), a decreasing function with k, or the ratio BSS/TSS, a value that increases with k. The goal of a so-called <em>elbow plot</em> is to find a <em>kink</em> in the progression of the objective function against the value of k. The rationale behind this is that as long as the optimal number of clusters has not been reached, the improvement in the objective should be substantial, but as soon as the optimal k has been exceeded, the curve flattens out. This is somewhat subjective and often not that easy to interpret in practice. <code>GeoDa</code>’s functionality does not include an elbow plot, but all the information regarding the objective functions needed to create such a plot is provided in the output.</p>
<p>A more formal approach is the so-called Gap statistic of
<span class="citation">Tibshirani, Walther, and Hastie (<a href="#ref-Tibshiranietal:01" role="doc-biblioref">2001</a>)</span>, which employs the difference between the log of the WSS and the log of the WSS of a uniformly randomly generated reference distribution (uniform over a hypercube that contains the data) to construct a <em>test</em> for the optimal k. Since this approach is computationally quite demanding, we do not consider it here.</p>
</div>
<div id="k-means-algorithms" class="section level4 unnumbered" number="">
<h4>K-means algorithms</h4>
<p>The earliest solution of the k-means problem is commonly attributed to <span class="citation">Lloyd (<a href="#ref-Lloyd:82" role="doc-biblioref">1982</a>)</span> and referred to as <em>Lloyd’s algorithm</em> (the algorithm was first contained in a Bell Labs technical report by Lloyd from 1957). While the progression of the iterative relocation is fairly straightforward, the choice of the initial starting point is not.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>Typically, the assignment of the first set of k cluster centers is obtained by uniform random sampling k distinct observations from the full set of n observations. In other words, each observation has the same probability of being selected.</p>
<p>The standard approach is to try several random assignments and start with the one that gives the best value for the objective function (e.g., the smallest WSS). This is one of the two approaches implemented in <code>GeoDa</code>. In order to ensure replicability, it
is important to set a seed value for the random number generator. Also, to further assess the
sensitivity of the result to the starting point, different seeds should be tried (as well as a
different number of initial solutions). <code>GeoDa</code> implements this approach by leveraging the functionality contained in the <em>C Clustering Library</em> <span class="citation">(Hoon, Imoto, and Miyano <a href="#ref-deHoonetal:17" role="doc-biblioref">2017</a>)</span>.</p>
<p>A second approach uses a careful consideration of initial seeds, following the procedure
outlined in <span class="citation">Arthur and Vassilvitskii (<a href="#ref-ArthurVassilvitskii:07" role="doc-biblioref">2007</a>)</span>, commonly referred to as <strong>k-means++</strong>. The rationale behind k-means++ is that rather than sampling uniformly from the n observations, the probability of selecting a new cluster seed is changed in function of the distance to the nearest existing seed. Starting with a uniformly random selection of the first seed, say <span class="math inline">\(c_1\)</span>, the probabilities for the remaining observations are computed as:
<span class="math display">\[p_{j \neq c_1} = \frac{d_{jc_1}^2}{\sum_{j \neq c_1} d_{jc_1}^2 }.\]</span>
In other words, the probability is no longer uniform, but changes with the squared distance to the existing seed: the smaller the distance, the smaller the probability. This is referred to by <span class="citation">Arthur and Vassilvitskii (<a href="#ref-ArthurVassilvitskii:07" role="doc-biblioref">2007</a>)</span> as the squared distance (<span class="math inline">\(d^2\)</span>) weighting.</p>
<p>The weighting increases the chance that the next seed is further away from the existing seeds, providing a better coverage over the support of the sample points. Once the second seed is selected, the probabilities are updated in function of the new distances to the closest seed, and the process continues until all k seeds are picked.</p>
<p>While generally being
faster and
resulting in a superior solution in small to medium sized data sets,
this method does not scale well (as it requires
k passes through the whole data set to recompute the distances and the updated probabilities). Note that a choice of a large number of random initial allocations may
yield a better outcome than the application of k-means++, at the expense of a
somewhat longer execution time.</p>
</div>
</div>
<div id="implementation" class="section level3 unnumbered" number="">
<h3>Implementation</h3>
<p>We invoke the k-means functionality from the same <strong>Clusters</strong> toolbar icon as we
have seen previously for dimension reduction. It is part of the second subset of
functionality, which includes several classic and more advanced (non-spatial)
clustering techniques, as shown in Figure <a href="#fig:kmeansmenu">3</a>. From the
menu, it is selected as <strong>Clusters &gt; K Means</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansmenu"></span>
<img src="pics7b/1_714_kmeans_option.png" alt="K Means Option" width="10%" />
<p class="caption">
Figure 3: K Means Option
</p>
</div>
<p>This brings up the <strong>K Means Clustering Settings</strong> dialog, shown
in Figure <a href="#fig:kmeansvars">4</a>, the main interface through which variables
are chosen, options selected, and summary results are provided.</p>
<div id="variable-settings-panel" class="section level4 unnumbered" number="">
<h4>Variable Settings panel</h4>
<p>We select the variables and set the parameters for the K Means cluster analysis
through the options in the left hand panel of the interface. We choose the same six
variables as in the previous chapter: <strong>Crm_prs</strong>,
<strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>, <strong>Infants</strong>, and <strong>Suicids</strong>.
These variables
appear highlighted in the <strong>Select Variables</strong> panel.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansvars"></span>
<img src="pics7b/2_715_kmeans_variables.png" alt="K Means variable selection" width="80%" />
<p class="caption">
Figure 4: K Means variable selection
</p>
</div>
<p>The next option is to select the <strong>Number of Clusters</strong>. The initial setting is blank. One can either choose a value from
the drop-down list, or enter an integer value directly.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> In our example, we set the number of clusters to <strong>5</strong>.</p>
<p>A default option is to use the variables in standardized form, i.e., in standard
deviational units, expressed with <strong>Transformation</strong> set to
<strong>Standardize (Z)</strong>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>The default algorithm is
<strong>KMeans++</strong> with initialization re-runs set to 150 and maximal iterations to 1000.
The <strong>seed</strong> is the global random number seed set for GeoDa, which can be changed by means of the
<strong>Use specified seed</strong> option. Finally, the default <strong>Distance Function</strong> is <strong>Euclidean</strong>.
For k-means, this is the only option available, since it is the basis for the objective function.</p>
<p>The cluster classification will be
saved in the variable specified in the <strong>Save Cluster in Field</strong> box. The default of <strong>CL</strong>
is likely not very useful if several options will be explored. In our first example, we
set the variable name to <strong>CLa</strong>.</p>
</div>
<div id="cluster-results" class="section level4 unnumbered" number="">
<h4>Cluster results</h4>
<p>After pressing <strong>Run</strong>, and keeping all the settings as above, a cluster map is created as a new view and the characteristics of
the cluster are listed in the <strong>Summary</strong> panel.</p>
<p>The cluster map in Figure <a href="#fig:kmeansmap5">5</a> reveals quite evenly balanced clusters, with 22, 19, 18, 16 and 10 members
respectively. Keep in mind that the clusters are based on attribute similarity and they do not respect
contiguity or compactness (we will examine spatially constrained clustering in a later chapter).</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansmap5"></span>
<img src="pics7b/2_717_clustermap_5.png" alt="K Means cluster map (k=5)" width="60%" />
<p class="caption">
Figure 5: K Means cluster map (k=5)
</p>
</div>
<p>The cluster characteristics are listed in the <strong>Summary</strong> panel, shown
in Figure <a href="#fig:kmeanssummary5">6</a>. This lists, for each cluster, the
method (KMeans), the value of k (here, 5), as well as
the parameters specified (i.e., the initialization methods, number of initialization
re-runs, the maximum iterations, transformation, and distance function).</p>
<p>Next
follows a table with the values of the cluster centers for each of the variables involved in the
clustering algorithm. Note that while the algorithm uses standardized values (with the <strong>Standardize</strong> option on), the results are presented in the original units. This allows for a comparison of the cluster means to the overall mean of the variable. In our example, the overall means are 19,961 for Crm_prs, 7,881 for Crm_prp, 39.1 for Litercy, 6,723 for Donatns, 18,983 for Infants, and
36,517 for Suicids. The interpretation is not always straightforward due to the complex interplay among the variables. In our example, none of the clusters have a mean that is superior to the national average for all variables. Cluster 3 is below the national mean for all but one variable (litercy). In a typical application, a close examination of these profiles should be the basis for any kind of meaningful characterization of the <em>clusters</em> (e.g., as in a socio-demographic profile).</p>
<p>Finally, a number of summary measures are provided to assess the overall quality of the cluster results. The total sum of
squares is listed (504), as well as the within-cluster sum of squares for each of the clusters. The absolute values of these results are not of great interest, but their relative magnitudes matter. For example, we can see that the similarity within cluster 3 (18 observations) is more than twice as good (less than half) as that in clusters 4 and 5.</p>
<p>Finally, these statistics are summarized as the total within-cluster sum of squares (253.277),
the total between-cluster sum of squares (the difference between TSS and WSS, 250.723), and the ratio of between-cluster to total
sum of squares. In our initial example, the latter is 0.497467.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanssummary5"></span>
<img src="pics7b/2_716_cluster_chars_5.png" alt="K Means cluster characteristics (k=5)" width="50%" />
<p class="caption">
Figure 6: K Means cluster characteristics (k=5)
</p>
</div>
</div>
<div id="adjusting-cluster-labels" class="section level4 unnumbered" number="">
<h4>Adjusting cluster labels</h4>
<p>The cluster labels (and colors in the map) are arbitrary and can be changed in the cluster
map, using the same technique we saw earlier for unique value maps (in fact, the cluster
maps are a special case of unique value maps). For example, if we wanted to switch
category 4 with 3 and the corresponding colors, we would move the light green rectangle
in the legend with label 4 up to the third spot in the legend, as shown in
Figure <a href="#fig:changelabels">7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:changelabels"></span>
<img src="pics7b/2_722_change_labels1.png" alt="Change cluster labels" width="15%" />
<p class="caption">
Figure 7: Change cluster labels
</p>
</div>
<p>Once we release the cursor, an updated cluster map is produced, with the categories (and colors)
for 3 and 4 switched, as in Figure <a href="#fig:kmeanschangelabels">8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanschangelabels"></span>
<img src="pics7b/2_723_changed_labels.png" alt="Relabeled cluster map" width="60%" />
<p class="caption">
Figure 8: Relabeled cluster map
</p>
</div>
</div>
<div id="cluster-variables" class="section level4 unnumbered" number="">
<h4>Cluster variables</h4>
<p>As the clusters are computed, a new categorical variable is added to the data table
(the variable name is specified in the <strong>Save Cluster in Field</strong> option).
It contains the assignment of each observation to one of the clusters as an
integer variable. However, this
is not automatically updated when we change the labels, as we did in the example above.</p>
<p>In order to save the updated classification, we can still use the generic <strong>Save Categories</strong> option
available in any map view (right click in the map).
After specifying a variable name (e.g., <strong>cat_a</strong>), we can see both the original categorical
variable and the new classification in the data table.</p>
<p>In the table, shown in Figure <a href="#fig:kmeanscattable">9</a>, wherever <strong>CLa</strong> (the original
classification) is <strong>3</strong>, the new classification (<strong>cat_a</strong>) is <strong>4</strong>.
As always, the new variables do not become permanent additions until the table is saved.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanscattable"></span>
<img src="pics7b/2_725_labels.png" alt="Cluster categories in table" width="20%" />
<p class="caption">
Figure 9: Cluster categories in table
</p>
</div>
</div>
<div id="saving-the-cluster-results" class="section level4 unnumbered" number="">
<h4>Saving the cluster results</h4>
<p>The summary results listed in the <strong>Summary</strong> panel can be saved to a text file. Right clicking
on the panel to brings up a dialog with a <strong>Save</strong> option,
as in Figure <a href="#fig:kmeanssave">10</a>. Selecting this and specifying a file name
for the results will provide a permanent record of the analysis.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanssave"></span>
<img src="pics7b/2_731_savesummary.png" alt="Saving summary results to a text file" width="40%" />
<p class="caption">
Figure 10: Saving summary results to a text file
</p>
</div>
</div>
</div>
<div id="options-and-sensitivity-analysis" class="section level3 unnumbered" number="">
<h3>Options and sensitivity analysis</h3>
<p>The k-means algorithm depends crucially on its initialization and on the various parameters specified.
It is important to assess the sensitivity of the results to the starting point and other parameters and settings. We consider a few of these next.</p>
<div id="changing-the-number-of-k-means-initial-runs" class="section level4 unnumbered" number="">
<h4>Changing the number of k-means++ initial runs</h4>
<p>The default number of initial re-runs for the k-means++ algorithm is <strong>150</strong>. Sometimes,
this is not sufficient to guarantee the best possible result (e.g., in terms of the ratio of between
to total sum of squares). Recall that k-means++ is simply a different way to have random initialization, so it also offers no guarantees with respect to the optimality of the starting point.</p>
<p>We can change the number of initial runs to obtain a starting point from the k-means++ algorithm in the <strong>Initialization Re-runs</strong> dialog, as
illustrated in Figure <a href="#fig:kppinit">11</a> for a value of <strong>1000</strong> iterations.</p>
<div class="figure" style="text-align: center"><span id="fig:kppinit"></span>
<img src="pics7b/4_042_kppinitialize1000.png" alt="K-means++ initialization re-runs" width="30%" />
<p class="caption">
Figure 11: K-means++ initialization re-runs
</p>
</div>
<p>The result is slightly different from what we obtained for the default setting.
As shown in Figure <a href="#fig:kppinitmap">12</a>, the first category now has 23 elements, and
the second 18. The other groupings remain the same.</p>
<div class="figure" style="text-align: center"><span id="fig:kppinitmap"></span>
<img src="pics7b/4_043_kpp1000map.png" alt="Cluster map for 1000 initial re-runs" width="60%" />
<p class="caption">
Figure 12: Cluster map for 1000 initial re-runs
</p>
</div>
<p>The revised initialization results in a slight improvement of the sum of squares ratio,
changing from 0.497467 to 0.497772, as shown in
Figure <a href="#fig:kppinitresults">13</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kppinitresults"></span>
<img src="pics7b/4_044_kppinitresults.png" alt="Cluster characteristics for 1000 initial reruns" width="50%" />
<p class="caption">
Figure 13: Cluster characteristics for 1000 initial reruns
</p>
</div>
</div>
<div id="random-initialization" class="section level4 unnumbered" number="">
<h4>Random initialization</h4>
<p>The alternative to the Kmeans++ initialization is to select the traditional random initialization with uniform probabilities across the observations. This is accomplished by choosing <strong>Random</strong> as the <strong>Initialization Method</strong>,
as shown in Figure <a href="#fig:randominit">14</a>. We keep the number of initialization re-runs
to the default value of 150 and save the result in the
variable <strong>CLr</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:randominit"></span>
<img src="pics7b/3_732_randominit.png" alt="Random initialization" width="30%" />
<p class="caption">
Figure 14: Random initialization
</p>
</div>
<p>The result is identical to what we obtained for K-means++ with 1000 initialization re-runs,
with the cluster map as in Figure <a href="#fig:kppinitmap">12</a> and the cluster summary
as in Figure <a href="#fig:kppinitresults">13</a>. However, if we had run the random initialization with
much fewer runs (e.g., 10), the results would be inferior to what we obtained before.
This highlights the effect of the starting values on the ultimate result.</p>
</div>
<div id="selecting-a-different-standardization" class="section level4 unnumbered" number="">
<h4>Selecting a different standardization</h4>
<p>The objective function for k-means clustering is sensitive to the scale in which the variables are
expressed. When these scales are very different (e.g., one variable is as a percentage and another is expressed
as thousands of dollars), standardization
converts the observations to more comparable magnitudes. As a result, the squared differences (or squared difference
from the cluster mean) become more or less comparable across variables. More specifically, this avoids that
variables that show a large variance dominate the objective function.</p>
<p>The most common standardization is the so-called z-standardization, where the original value is centered
around zero (by subtracting the mean) and rescaled to have a variance of one (by dividing by the standard
deviation). However, this is by no means the only way to <em>correct</em> for the effect of different spreads
among the variables. Some other approaches, which may be more robust to the effect of outliers (on the
estimates of mean and variance used in the standardization) are the MAD, mentioned in the context of PCA, as
well as range standardization. The latter is often recommended for cluster excercises <span class="citation">(e.g., Everitt et al. <a href="#ref-Everittetal:11" role="doc-biblioref">2011</a>)</span>.
It rescales each variable such that its minimum is zero and its maximum becomes one by subtracting the minimum and
dividing by the range.</p>
<p>We assess the effect of such a standardization on the k-means result by selecting the associated option, as
in Figure <a href="#fig:rangeinit">15</a>. We leave all the other options to the default setting.</p>
<div class="figure" style="text-align: center"><span id="fig:rangeinit"></span>
<img src="pics7b/22_range_standardize.png" alt="Range Standardization" width="40%" />
<p class="caption">
Figure 15: Range Standardization
</p>
</div>
<p>The resulting clusters are depicted in Figure <a href="#fig:rangemap">16</a>. Compared to the standard result in Figure <a href="#fig:kmeansmap5">5</a>,
the spatial layout
of cluster members is quite different. Two of the clusters are larger (1 and 2 with 27 and 20 elements) and two others smaller
(3 and 4 with 17 and 11). Cluster 5 has the same number of members as in the original solution, but only half the observations are
in common.</p>
<div class="figure" style="text-align: center"><span id="fig:rangemap"></span>
<img src="pics7b/22_range_map.png" alt="Cluster map for range standardization" width="60%" />
<p class="caption">
Figure 16: Cluster map for range standardization
</p>
</div>
<p>The difference between the two solutions is also highlighted by the cluster summary measures,
shown in Figure <a href="#fig:rangeresults">17</a>. Clusters 3, 4 and 5 are almost equally compact and the
overall BSS/TSS ratio is the highest achieved so far, at 0.537.</p>
<div class="figure" style="text-align: center"><span id="fig:rangeresults"></span>
<img src="pics7b/22_range_results.png" alt="Cluster characteristics for range standardization" width="50%" />
<p class="caption">
Figure 17: Cluster characteristics for range standardization
</p>
</div>
<p>This again highlights the need to carry out sensitivity analysis and not take the default standardization
as the only available option. Depending on the relative ranges and variances of the variables under consideration,
one standardization may achieve a better grouping than another. Also, in some circumstances, when the variables
are already on similar scales, standardization is not necessary.</p>
</div>
<div id="setting-a-minimum-bound" class="section level4 unnumbered" number="">
<h4>Setting a minimum bound</h4>
<p>It is also possible to <em>constrain</em> the k-means
clustering by imposing a minimum value for a spatially extensive variable, such as a
total population. This ensures that the clusters meet a minimum size for that variable.
For example, in studies of socio-economic determinants of health, we may want to group
similar census tracts into <em>neighborhoods</em>. But if we are concerned with rare diseases, privacy concerns often require a minimum size for the population at risk. By setting a minimum bound
on the population variable, we
avoid creating clusters that are <em>too small</em>.</p>
<p>The minimum bound is set in the variable settings dialog by checking the
box next to <strong>Minimum Bound</strong>, as in Figure <a href="#fig:minbound">18</a>. This variable does not
have to be part of the variables used in the clustering exercise. In fact, it should
be a relevant <em>size</em> variable (so-called spatially extensive variable), such as total
population or total number of establishments, unrelated to the multivariate dimensions involved in
the clusters.</p>
<p>In our example, we
select the variable <strong>Pop1831</strong> to set the constraint. Note that we specify a bound
of <strong>16%</strong> (or <strong>5179</strong>) rather than the default <strong>10%</strong>. This is because the
standard k-means solution satisfies the default constraint already, so that no actual bounding
is carried out. Also, any percentage higher than 16 fails to yield a cluster solution that meets the population requirement.</p>
<div class="figure" style="text-align: center"><span id="fig:minbound"></span>
<img src="pics7b/7_075_minbound.png" alt="Setting a minimum bound" width="30%" />
<p class="caption">
Figure 18: Setting a minimum bound
</p>
</div>
<p>With the cluster size at 5 and all other options back to their default value, (specifically, with 150 kmeans++ initialization runs) we obtain
the cluster result shown in the map in Figure <a href="#fig:minboundmap">19</a>. The categories have been re-ordered to show a similar color scheme to the unconstrained map in Figure <a href="#fig:kmeansmap5">5</a> (using <strong>CLc</strong> as the cluster variable).</p>
<div class="figure" style="text-align: center"><span id="fig:minboundmap"></span>
<img src="pics7b/7_076_minboundmap.png" alt="Cluster map with minimum bound constraint" width="60%" />
<p class="caption">
Figure 19: Cluster map with minimum bound constraint
</p>
</div>
<p>Relative to the unconstrained map, we find that three clusters have increased in size (1 to 22, 2 to 19 and 4 to 16) and two shrank (3 to 19 and 5 to 10). In addition, the configuration of the clusters changed slightly as well, with cluster 5 the most affected.</p>
<p>The cluster characteristics show a slight deterioriation of our summary criterion,
to a value of <strong>0.484033</strong> (compared to <strong>0.497772</strong>), as shown in
Figure <a href="#fig:minboundresults">20</a>. This is the price to pay to
satisfy the minimum population constraint. However, the effect on the individual clusters is mixed, with clusters 2 and 5 having a much worse WSS, whereas 1, 3 and 4 actually achieve a smaller WSS relative to the unconstrained version. This illustrates the complex trade-offs involved between the cases where the constraint has to be enforced compared to where it is already satisfied.</p>
<div class="figure" style="text-align: center"><span id="fig:minboundresults"></span>
<img src="pics7b/7_077_minboundsummary.png" alt="Cluster characteristics with minimum bound" width="50%" />
<p class="caption">
Figure 20: Cluster characteristics with minimum bound
</p>
</div>
</div>
<div id="elbow-plot" class="section level4 unnumbered" number="">
<h4>Elbow plot</h4>
<p>We now illustrate the use of an <em>elbow plot</em> to assess the best value for k. This plot is not included in the <code>GeoDa</code> functionality, but it is easy to construct from the output of a cluster analysis.</p>
<p>The results for runs of k-means with all the default settings are given in Figure <a href="#fig:elbowdata">21</a>, for k ranging from 1 through 12. As the number of clusters increases, the resulting groupings become less and less informative and tend to yield several singleton clusters (with only one observation). Both WSS and BSS/TSS are listed, with the associated change as the value of k increases.</p>
<div class="figure" style="text-align: center"><span id="fig:elbowdata"></span>
<img src="pics7b/001_elbowdata.png" alt="Cluster characteristics for increasing values of k" width="50%" />
<p class="caption">
Figure 21: Cluster characteristics for increasing values of k
</p>
</div>
<p>The actual elbow plots are given in Figure <a href="#fig:elbowplot">22</a>. There are two versions of this plot, one showing the decrease in WSS as k increases (left panel of the Figure), the other illustrating the increase in the ratio BST/SST with k (right panel). Unlike the usual textbook examples, in our case these graphs are not easy to interpret, still showing an albeit smaller improvement for <span class="math inline">\(k &gt; 10\)</span>, which is not a practical number of clusters for only 85 observations. It does seem that a good value for k would be in the range 5-8, although this is by no means a hard and fast conclusion.</p>
<p>This example illustrates the difficuly of deciding on the best k in practice. Considerations other than those offered by the simple elbow plot should be taken into account, such as how to interpret the groupings that result from the cluster analysis and whether the clusters are reasonably balanced. In some sense this is both an art and a science and it takes practice as well as familiarity with the substantive research questions to obtain a good insight into the trade-offs involved.</p>
<div class="figure" style="text-align: center"><span id="fig:elbowplot"></span>
<img src="pics7b/001_elbowplot.png" alt="Elbow plots" width="80%" />
<p class="caption">
Figure 22: Elbow plots
</p>
</div>
</div>
</div>
</div>
<div id="cluster-categories-as-variables" class="section level2 unnumbered" number="">
<h2>Cluster Categories as Variables</h2>
<p>Once added to the data table, the cluster categories can be visualized like any other integer variable.
We already saw their use in the unique values map that is provided with the standard output. Two other
particularly useful applications are as a conditioning variable in a conditional plot and as
a variable to guide aggregation of observations to the cluster level.</p>
<div id="conditional-plots" class="section level3 unnumbered" number="">
<h3>Conditional plots</h3>
<p>A particularly useful conditional plot is a conditional box plot, in which the distribution of a variable
can be shown for each cluster. This supplements the summary characteristics of each cluster provided in the standard
output, where only the mean is shown for each cluster category.</p>
<p>The conditional box plot is invoked as <strong>Explore &gt; Conditional Plot &gt; Box Plot</strong>, or from the conditional plots icon on the toolbar, as in Figure <a href="#fig:condboxplot">23</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:condboxplot"></span>
<img src="pics7b/44_condbox.png" alt="Conditional box plot option" width="10%" />
<p class="caption">
Figure 23: Conditional box plot option
</p>
</div>
<p>For this particular application, it is most effective to only condition on one dimension, either horizontal or vertical.
In the dialog shown in Figure <a href="#fig:condboxvars">24</a>, we selected the cluster variable <strong>CLa</strong> as the horizontal axis.
We also selected the <em>empty</em> category for the vertical axis (this ensures that conditioning only happens in the
horizontal dimension), and chose <strong>Litercy</strong> as the variable.</p>
<div class="figure" style="text-align: center"><span id="fig:condboxvars"></span>
<img src="pics7b/44_condbxsettings.png" alt="Conditional box plot variable selection" width="50%" />
<p class="caption">
Figure 24: Conditional box plot variable selection
</p>
</div>
<p>The first plot that results is not very useful, since it takes a quantile
distribution as the default for the horizontal axis. By right clicking, the options are brought up and we select
<strong>Horizontal Bins Breaks &gt; Unique Values</strong>. The result is as in Figure <a href="#fig:condboxlitracy">25</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:condboxlitracy"></span>
<img src="pics7b/44_literacybxplot.png" alt="Conditional box plot for literacy" width="70%" />
<p class="caption">
Figure 25: Conditional box plot for literacy
</p>
</div>
<p>The box plots use the same scale, so that comparisons are easy to make. Note that even though the box plot fences are drawn,
the actual range of the distribution in each cluster does not necessarily reach that far. In our example, the values
for clusters 1-3 are well within these bounds. Only for cluster 5 do we observe an outlier.</p>
<p>The plot clearly shows how
the median value is much higher for clusters 2 and 3 relative to the others. It also shows little overlap of the interquartile
range between the two groups (2 and 3, relative to 1-4-5). Conditional box plots can be a useful aide in the characterization
of the different clusters in that they provide more than just a summary measure, such as a mean or median, but portray
the whole distribution.</p>
</div>
<div id="aggregation-by-cluster" class="section level3 unnumbered" number="">
<h3>Aggregation by cluster</h3>
<p>With the clusters at hand, as defined for each observation by the category in the cluster
field, we can now compute aggregate values for the new clusters and even
create new layers for the dissolved units. We illustrate this
as a quick check on the population totals we imposed in the bounded cluster procedure.</p>
<p>There are two different ways to proceed. In one, we only create a new table, in the other, we end up with a new layers that has the original units spatially <em>dissolved</em> into the new clusters.</p>
<p>The simple aggregation is invoked from the <strong>Table</strong> as an option by right-clicking. This
brings up the list of options, from which we select <strong>Aggregate</strong>, as in
Figure <a href="#fig:tabagg">26</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:tabagg"></span>
<img src="pics7b/7_002_table_aggregate.png" alt="Table Aggregate option" width="20%" />
<p class="caption">
Figure 26: Table Aggregate option
</p>
</div>
<p>The following dialog, shown in Figure <a href="#fig:tabaggpop">27</a>, provides the specific
aggregation method, i.e., count, average, max, min, or <strong>Sum</strong>, the <strong>key</strong> on which
to aggregate and a selection of variable to aggregate. In our example, we use the
cluster field key, e.g., <strong>CLc</strong> (from the bounded clustering solution) and select only the population variable <strong>Pop1831</strong>,
which we will sum over the departments that make up each cluster. This can be readily
extended to multiple variables, as well as to different summary measures, such
as the average.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:tabaggpop"></span>
<img src="pics7b/7_079_aggregate_vars.png" alt="Aggregation of total population by cluster" width="40%" />
<p class="caption">
Figure 27: Aggregation of total population by cluster
</p>
</div>
<p>Pressing the <strong>Aggregate</strong> key brings up a dialog to select the file in which the new
results will be saved. For example, we can select a <strong>dbf</strong> format and specify the
file name. The contents of the new file are given in Figure <a href="#fig:clusterpop">28</a>,
with the total population for each cluster. Clearly, each cluster meets the minimum
requirement of 5179 that was specified.</p>
<div class="figure" style="text-align: center"><span id="fig:clusterpop"></span>
<img src="pics7b/000_dissolve_table.png" alt="Total population by cluster" width="30%" />
<p class="caption">
Figure 28: Total population by cluster
</p>
</div>
<p>The same procedure can be used to create new values for any variable, aggregated
to the new cluster scale. Note that since most variables in the Guerry data set are ratios, a simple sum would not be appropriate.</p>
<p>A second way to create new aggregate spatial units is to invoke the <strong>Dissolve</strong> functionality from the <strong>Tools</strong> menu as in
Figure <a href="#fig:clusterdissolve">29</a>. This will aggregate the variables in the same way as for the table, but also create a new spatial layer that <em>dissolves</em> the original units into their new aggregate clusters.</p>
<div class="figure" style="text-align: center"><span id="fig:clusterdissolve"></span>
<img src="pics7b/000_toolsdissolve.png" alt="Dissolve function" width="15%" />
<p class="caption">
Figure 29: Dissolve function
</p>
</div>
<p>The next interface is the same as for the aggregate functionality, shown in
Figure <a href="#fig:tabaggpop">27</a>. We again select <strong>CLc</strong> as the <em>key</em> to aggregate the spatial units and specify <strong>Pop1831</strong> as the variable to be summed into the new units. After saving the new layer, we can bring it back into GeoDa and create a unique values map of the population totals, as in Figure
<a href="#fig:dissolvepoptot">30</a>.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> The table associated with this layer is identical to the
one shown in Figure <a href="#fig:clusterpop">28</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:dissolvepoptot"></span>
<img src="pics7b/000_dissolvemap.png" alt="Map of clusters with population totals" width="60%" />
<p class="caption">
Figure 30: Map of clusters with population totals
</p>
</div>
</div>
</div>
<div id="clustering-with-dimension-reduction" class="section level2 unnumbered" number="">
<h2>Clustering with Dimension Reduction</h2>
<p>In practice, it is often more effective to carry out a clustering exercise after dimension reduction,
instead of using the full set of variables. This is typically based on principal components, but can
equally be computed from the MDS coordinates. Both approaches are illustrated below.</p>
<div id="pca" class="section level3 unnumbered" number="">
<h3>PCA</h3>
<p>Instead of specifying all six variables, we carry out k-means clustering for the first two principal components,
say the variables PC1 and PC2. Since the principal components are already standardized, we can set that option
to <strong>Raw</strong>. The results for k=5 are shown in Figure <a href="#fig:pcaclustermap">31</a>, with the coordinates in a two-dimensional
principal component scatter plot on the left, and the associated cluster map on the right. The clusters appear
as clear <em>regions</em> in the scatter plot, highlighting the (multivariate) similarity among those observations in the
same cluster.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:pcaclustermap"></span>
<img src="pics7b/33_pcaclustermap.png" alt="PCA plot and cluster map" width="80%" />
<p class="caption">
Figure 31: PCA plot and cluster map
</p>
</div>
<p>A summary with the cluster centers, the associated WSS and summary statistics is given in Figure <a href="#fig:pcaclusterresult">32</a>.
Strikingly, the between sum of squares to total sum of squares ratio is 0.755, by far the best cluster separation obtained for k=5 (and even better
than the six-variable result for k=12). This illustrates the potential gain in efficiency obtained by reducing the full six variable
dimensions to the principal components, which effectively summarize the multivariate characteristics of the data.</p>
<div class="figure" style="text-align: center"><span id="fig:pcaclusterresult"></span>
<img src="pics7b/33_pcaclusterresults.png" alt="PCA cluster summary" width="50%" />
<p class="caption">
Figure 32: PCA cluster summary
</p>
</div>
</div>
<div id="mds" class="section level3 unnumbered" number="">
<h3>MDS</h3>
<p>The same principle can be applied to the coordinates obtained from multidimensional scaling. While this is less used in practice,
it is in fact equivalent to using principal components. We have already seen that for classic metric scaling the two-dimensional plots of the two approaches
are equivalent, except for flipping the axes. This can also be seen from the results for k=5 shown in Figure <a href="#fig:mdsclustermap">33</a>.
The variables used in the clustering exercise are the MDS coordinates V1 and V2 (again, with the standardization option set to <strong>Raw</strong>).
The clusters are identical to those obtained for the first two principal components. The scatter plot is flipped, but otherwise contains
the same information as before.</p>
<div class="figure" style="text-align: center"><span id="fig:mdsclustermap"></span>
<img src="pics7b/33_mdsclustermap.png" alt="MDS plot and cluster map" width="80%" />
<p class="caption">
Figure 33: MDS plot and cluster map
</p>
</div>
<p>Similarly, the summary results are the same, as shown in Figure <a href="#fig:mdsclusterresult">34</a>, except that the signs of the cluster
centers for V2 are the opposite of those for PC2 in Figure <a href="#fig:pcaclusterresult">32</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:mdsclusterresult"></span>
<img src="pics7b/33_mdsclusterresults.png" alt="MDS cluster summary" width="50%" />
<p class="caption">
Figure 34: MDS cluster summary
</p>
</div>
</div>
</div>
<div id="appendix" class="section level2 unnumbered" number="">
<h2>Appendix</h2>
<div id="equivalence-of-euclidean-distances-and-sum-of-squared-errors-sse" class="section level3 unnumbered" number="">
<h3>Equivalence of Euclidean distances and Sum of Squared Errors (SSE)</h3>
<p>We start with the objective function as the within sum of squared distances. More precisely,
this is expressed as one half the sum of the squared Euclidean distances
for each cluster between all the pairs <span class="math inline">\(i-j\)</span> that form part of the cluster <span class="math inline">\(h\)</span>, i.e., for all <span class="math inline">\(i \in h\)</span>
and <span class="math inline">\(j \in h\)</span>:
<span class="math display">\[W = (1/2) \sum_{h=1}^k \sum_{i \in h} \sum_{j \in h} ||x_i - x_j||^2.\]</span>
To keep things simple, we take the <span class="math inline">\(x\)</span> as univariate, so we can ignore the more complex notation needed for higher order manipulations. This is without a loss of generality, since for multiple variables the Euclidean distance is simply the
sum of the distances for each of the variables.</p>
<p>A fundamental identity exploited to move from the
sum of all pairwised distances to the sum of squared deviations is that:
<span class="math display">\[(1/2) \sum_i \sum_j (x_i - x_j)^2 = n \sum_i (x_i - \bar{x})^2,\]</span>
where <span class="math inline">\(\bar{x}\)</span> is the mean of <span class="math inline">\(x\)</span>. This takes a little algebra to confirm.</p>
<p>For each <span class="math inline">\(i\)</span> in a given cluster <span class="math inline">\(h\)</span>, the sum over the <span class="math inline">\(j\)</span> in the cluster is:
<span class="math display">\[\sum_{j \in h} (x_i - x_j)^2 = \sum_{j \in h} (x_i^2 - 2x_ix_j + x_j^2),\]</span>
or (dropping the <span class="math inline">\(\in h\)</span> notation for simplicity),
<span class="math display">\[\sum_j x_i^2 - 2\sum_j x_i x_j + \sum_j x_j^2.\]</span></p>
<p>With <span class="math inline">\(n_h\)</span> observations in cluster <span class="math inline">\(h\)</span>, <span class="math inline">\(\sum_j x_i^2 = n_h x_i^2\)</span> (since <span class="math inline">\(x_i\)</span> does not contain the
index <span class="math inline">\(j\)</span> and thus is simply repeated <span class="math inline">\(n_h\)</span> times). Also, <span class="math inline">\(\sum_j x_j = n_h \bar{x}_h\)</span>,
where <span class="math inline">\(\bar{x}_h\)</span> is the mean of <span class="math inline">\(x\)</span> for cluster <span class="math inline">\(h\)</span> (<span class="math inline">\(\bar{x}_h = \sum_j x_j / n_h = \sum_i x_i / n_h\)</span>).
As a result, <span class="math inline">\(- 2 x_i \sum_j x_j = - 2 n_h x_i \bar{x}_h\)</span>, which yields:
<span class="math display">\[\sum_{j} (x_i - x_j)^2 = n_h x_i^2 - 2 n_h x_i \bar{x}_h + \sum_j x_j^2.\]</span>
Next, we proceed in the same way to compute the sum over all the <span class="math inline">\(i \in h\)</span>:
<span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = n_h \sum_i x_i^2 - 2 n_h \bar{x}_h \sum_i x_i + \sum_i \sum_j x_j^2\]</span>
Using the same approach as before, and since <span class="math inline">\(\sum_j x_j^2 = \sum_i x_i^2\)</span>, this becomes:
<span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = n_h \sum_i x_i^2 - 2 n_h^2 \bar{x}_h^2 + n_h \sum_i x_i^2 = 2 n_h \sum_i x_i^2 - 2 n_h^2 \bar{x}_h^2,\]</span>
or,
<span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = 2n_h^2 (\sum_i x_i^2 / n_h - \bar{x}_h^2).\]</span>
From the definition of the variance, we know that the following equality holds:
<span class="math display">\[\sigma^2 = (1/n_h) \sum_i (x_i - \bar{x}_h)^2 = \sum_i x_i^2 / n_h - \bar{x}_h^2.\]</span>
Therefore:
<span class="math display">\[\sum_i \sum_{j} (x_i - x_j)^2 = 2n_h^2 [(1/n_h) \sum_i (x_i - \bar{x}_h)^2] = 2n_h [ \sum_i (x_i - \bar{x}_h)^2 ],\]</span>
which gives the contribution of cluster <span class="math inline">\(h\)</span> to the objective function as:
<span class="math display">\[(1/2) \sum_i \sum_{j} (x_i - x_j)^2 = n_h [ \sum_i (x_i - \bar{x}_h)^2 ].\]</span></p>
<p>For all the clusters jointly, the objective becomes:
<span class="math display">\[\sum_{h=1}^k n_h \sum_{i \in h} (x_i - \bar{x}_h)^2.\]</span></p>
<p>In other words, minimizing the sum of (one half) of all squared distances is equivalent to minimizing
the sum of squared deviations from the mean in each cluster, the within sum of squared errors.</p>
</div>
<div id="k-means-worked-example" class="section level3 unnumbered" number="">
<h3>K-means worked example</h3>
<p>We now provide the details behind the k-means algorithm portrayed in
Figures <a href="#fig:kmexample">1</a> and <a href="#fig:kmsteps">2</a>. The coordinates of the seven points are given in the X and Y columns of Figure <a href="#fig:kmeansex1">35</a>. The column labeled as SSE shows the squared distance from each point to the center of the point cloud, computed as the average of the X and Y coordinates (X=5.714, Y=5.143).<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> The sum of the squared distances constitutes the total sum of squared errors, or TSS, which equals 62.286 in this example. This value does not change as we iterate, since it pertains to all the observations taken together and ignores the cluster allocations.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansex1"></span>
<img src="pics7b/01_coordinates.png" alt="Worked example - basic data" width="35%" />
<p class="caption">
Figure 35: Worked example - basic data
</p>
</div>
<p>The initialization consists of randomly picking two observations as starting points, one for each potential cluster. In this example, we take observations 4 and 7. Next, we compute the distance from each point to the two <em>seeds</em> and allocate each to the closest seed. The highlighted values in columns d_i4 and d_i7 in Figure <a href="#fig:kmeansd1">36</a> show how the first allocation consists of five observations in cluster 1 (1-5) and two observations in cluster 2 (6 and 7).</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansd1"></span>
<img src="pics7b/02_disttoseed.png" alt="Squared distance to seeds" width="25%" />
<p class="caption">
Figure 36: Squared distance to seeds
</p>
</div>
<p>Next, we compute a central point for each cluster as the average of the respective X and Y coordinates. The SSE follows as the squared distance between each observation in the cluster and the central point, as listed in
Figure <a href="#fig:kmeanstep1a">37</a>. The sum of the total SSE in each cluster is the <em>within</em> sum of squares, WSS = 30.9 in this first step. Consequently, the <em>between</em> sum of squares BSS = TSS - WSS = 62.3 - 30.9 = 31.4. The associated ratio BSS/TSS, which is an indicator of the quality of the cluster is 0.50.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanstep1a"></span>
<img src="pics7b/02_step1_centers.png" alt="Step 1 - Summary characteristics" width="80%" />
<p class="caption">
Figure 37: Step 1 - Summary characteristics
</p>
</div>
<p>Next, we take the new cluster center points and again allocate each observation to the closest center. As shown in Figure <a href="#fig:kmeansd2">38</a>, this results in observation 5 moving from cluster 1 to cluster 2, which now consists of three observations (cluster 1 has the remaining four).</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansd2"></span>
<img src="pics7b/02_dist_to_1.png" alt="Squared distance to Step 1 centers" width="25%" />
<p class="caption">
Figure 38: Squared distance to Step 1 centers
</p>
</div>
<p>We compute the new cluster centers and calculate the associated SSE. As suggested by Figure <a href="#fig:kmeanstep2a">39</a>, this results in a new WSS of 22.667, clearly an improvement of the objective function. The corresponding ratio of BSS/TSS becomes 0.64, also a clear improvement.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanstep2a"></span>
<img src="pics7b/02_step2_centers.png" alt="Step 2 - Summary characteristics" width="80%" />
<p class="caption">
Figure 39: Step 2 - Summary characteristics
</p>
</div>
<p>We repeat the process with the updated centers, which results in observation 4 moving from cluster 1 to cluster 2, as illustrated in Figure <a href="#fig:kmeansd3">40</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansd3"></span>
<img src="pics7b/02_dist_to_2.png" alt="Squared distance to Step 2 centers" width="25%" />
<p class="caption">
Figure 40: Squared distance to Step 2 centers
</p>
</div>
<p>Figure <a href="#fig:kmeanstep3a">41</a> shows the new cluster centers and associated SSE. The updated WSS is 15.333, yielding a ratio BSS/TSS of 0.75.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanstep3a"></span>
<img src="pics7b/02_step3_centers.png" alt="Step 3 - Summary characteristics" width="80%" />
<p class="caption">
Figure 41: Step 3 - Summary characteristics
</p>
</div>
<p>Finally, this latest allocation no longer results in a change, as shown
Figure <a href="#fig:kmeansd4">42</a>, so that we can conclude that we reached a local optimum.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeansd4"></span>
<img src="pics7b/02_dist_to_3.png" alt="Squared distance to Step 3 centers" width="25%" />
<p class="caption">
Figure 42: Squared distance to Step 3 centers
</p>
</div>
<p>Figure <a href="#fig:kmexgeoda">43</a> demonstrates that this is the same result as obtained by <code>GeoDa</code> with the default settings, but the <strong>Transformation</strong> set to <strong>Raw</strong> in order to use the actual coordinates.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:kmexgeoda"></span>
<img src="pics7b/02_geoda.png" alt="K-means results from GeoDa" width="55%" />
<p class="caption">
Figure 43: K-means results from GeoDa
</p>
</div>
<p><br></p>
</div>
</div>
<div id="references" class="section level2 unnumbered" number="">
<h2>References</h2>
<div id="refs" class="references hanging-indent">
<div id="ref-ArthurVassilvitskii:07">
<p>Arthur, David, and Sergei Vassilvitskii. 2007. “k-means++: The Advantages of Careful Seeding.” In <em>SODA 07, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</em>, edited by Harold Gabow, 1027–35. Philadelphia, PA: Society for Industrial and Applied Mathematics.</p>
</div>
<div id="ref-Dempsteretal:77">
<p>Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” <em>Journal of the Royal Statistical Society, Series B</em> 39: 1–38.</p>
</div>
<div id="ref-Everittetal:11">
<p>Everitt, Brian S., Sabine Landau, Morven Leese, and Daniel Stahl. 2011. <em>Cluster Analysis, 5th Edition</em>. New York, NY: John Wiley.</p>
</div>
<div id="ref-Hanetal:12">
<p>Han, Jiawei, Micheline Kamber, and Jian Pei. 2012. <em>Data Mining (Third Edition)</em>. Amsterdam: MorganKaufman.</p>
</div>
<div id="ref-HartiganWong:79">
<p>Hartigan, J. A., and M. A. Wong. 1979. “Algorithm AS 136: A k-means Clustering Algorithm.” <em>Applied Statistics</em> 28: 100–108.</p>
</div>
<div id="ref-Hartigan:72">
<p>Hartigan, John A. 1972. “Direct Clustering of a Data Matrix.” <em>Journal of the American Statistical Association</em> 67: 123–29.</p>
</div>
<div id="ref-Hartigan:75">
<p>———. 1975. <em>Clustering Algorithms</em>. New York, NY: John Wiley.</p>
</div>
<div id="ref-Hastieetal:09">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning (2nd Edition)</em>. New York, NY: Springer.</p>
</div>
<div id="ref-deHoonetal:17">
<p>Hoon, Michiel de, Seiya Imoto, and Satoru Miyano. 2017. “The C Clustering Library.” Tokyo, Japan: The University of Tokyo, Institute of Medical Science, Human Genome Center.</p>
</div>
<div id="ref-Jain:10">
<p>Jain, Anil K. 2010. “Data Clustering: 50 Years Beyond K-Means.” <em>Pattern Recognition Letters</em> 31 (8): 651–66.</p>
</div>
<div id="ref-JainDubes:88">
<p>Jain, Anil K., and Richard C. Dubes. 1988. <em>Algorithms for Clustering Data</em>. Englewood Cliffs, NJ: Prentice Hall.</p>
</div>
<div id="ref-Jamesetal:13">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning, with Applications in R</em>. New York, NY: Springer-Verlag.</p>
</div>
<div id="ref-KaufmanRousseeuw:05">
<p>Kaufman, L., and P. Rousseeuw. 2005. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. New York, NY: John Wiley.</p>
</div>
<div id="ref-Lloyd:82">
<p>Lloyd, Stuart P. 1982. “Least Squares Quantization in PCM.” <em>IEEE Transactions on Information Theory</em> 28: 129–36.</p>
</div>
<div id="ref-PadilhaCampello:17">
<p>Padilha, Victor A., and Ricardo J. G. B. Campello. 2017. “A Systematic Comparative Evaluation of Biclustering Techniques.” <em>BMC Bioinformatics</em> 18: 55. <a href="https://doi.org/10.1186/s12859-017-1487-1">https://doi.org/10.1186/s12859-017-1487-1</a>.</p>
</div>
<div id="ref-Tanayetal:04">
<p>Tanay, Amos, Roded Sharan, and Ron Shamir. 2004. “Biclustering Algorithms: A Survey.” In <em>Handbook of Computational Molecular Biology</em>, edited by Srinivas Aluru, 26–21–17. Boca Raton, FL: Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-Tibshiranietal:01">
<p>Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” <em>Journal of the Royal Statistical Society, Series B</em> 63: 411–23.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu" class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>While we discuss dimension reduction and clustering separately, so-called <em>biclustering</em> techniques group both variables and observations simultaneously. While old, going back to an article by <span class="citation">Hartigan (<a href="#ref-Hartigan:72" role="doc-biblioref">1972</a>)</span>, these techniques have gained a lot of interest more recently in the field of gene expression analysis. For overviews, see, e.g., <span class="citation">Tanay, Sharan, and Shamir (<a href="#ref-Tanayetal:04" role="doc-biblioref">2004</a>)</span> and <span class="citation">Padilha and Campello (<a href="#ref-PadilhaCampello:17" role="doc-biblioref">2017</a>)</span>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>The discussion in this section is loosely based on part of
the presentation in <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-Hastieetal:09" role="doc-biblioref">2009</a>)</span> Chapter 14.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Since each pair is counted twice, the total sum is divided by 2. While this seems arbitrary at this point, it helps later on when we establish the equivalence between the Euclidean distances and the sum of squared errors.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>As shown in <span class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12" role="doc-biblioref">2012</a>)</span>, p. 505, the k-means algorithm can be considered to be a special case of the so-called EM (expectation-maximization) algorithm of <span class="citation">Dempster, Laird, and Rubin (<a href="#ref-Dempsteretal:77" role="doc-biblioref">1977</a>)</span>. The expectation step consists of allocating each observation to its nearest cluster center, and the maximization step is the recalculation
of those cluster centers for each new layout.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>The drop-down list goes from 2 to 85,
which may be insufficient in <em>big data</em> settings. Hence <code>GeoDa</code> now offers the option to enter
a value directly.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>The Z standardization subtracts the mean and divides by the
standard deviation. Alternative standardizations are to use the mean absolute
deviation, MAD, the range adjust or range standardization methods.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>In the current implementation, the same summary method needs to
be applied to all the variables.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>The categories have been adjusted to keep the same color pattern as with the cluster map in Figure <a href="#fig:minboundmap">19</a>.<a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>For a similar logic, see for example Chapter 2 in <span class="citation">Everitt et al. (<a href="#ref-Everittetal:11" role="doc-biblioref">2011</a>)</span>, where a <em>visual inspection</em> of two-dimensional
scatter plots of principal components is illustrated as way to identify clusters.<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Each squared distance is <span class="math inline">\((x_i - \bar{x})^2 + (y_i - \bar{y})^2\)</span>.<a href="#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>This is accomplished by loading a csv file with the coordinates as the data input in <code>GeoDa</code>.<a href="#fnref12" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<footer class="site-footer">
  <span class="site-footer-owner"><a href="https://github.com/lixun910/geoda">GeoDa</a> is maintained by <a href="#">lixun910</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
</footer>

</section>


<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
