<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="author" content="Luc Anselin" />


  <title>Dimension Reduction Methods (2)</title>

  <link href="lab7ab_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
  <script src="lab7ab_files/highlightjs-9.12.0/highlight.js"></script>
  <title>GeoDa on Github</title>

  <style>
    * {
      margin: 0;
      padding: 0;
    }

    .shadowfilter {
      -webkit-filter: drop-shadow(12px 12px 7px rgba(0, 0, 0, 0.5));
      filter: url(shadow.svg#drop-shadow);
    }

    .intro1 {
      margin-left: -45px;
    }
  </style>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css"
    media="screen">
  <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
  <style>
    ul {
      padding-left: 30px;
    }

    figcaption {
      top: .70em;
      left: .35em;
      bottom: auto !important;
      right: auto !important;
    }
  </style>

  <style>
    h1 {
      text-align: center;
    }

    h3.subtitle {
      text-align: center;
    }

    h4.author {
      text-align: center;
    }

    h4.date {
      text-align: center;
    }

    p.caption {
      font-size: 12px;
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LC0QJ53WFS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-LC0QJ53WFS');
  </script>
  <!-- End Google tag -->

  <style type="text/css">
    code {
      white-space: pre;
    }
  </style>
  <script type="text/javascript">
    if (window.hljs) {
      hljs.configure({ languages: [] });
      hljs.initHighlightingOnLoad();
      if (document.readyState && document.readyState === "complete") {
        window.setTimeout(function () { hljs.initHighlighting(); }, 0);
      }
    }
  </script>








</head>

<body>


  <section class="page-header">
    <h1 class="project-name">GeoDa</h1>
    <h2 class="project-tagline">An Introduction to Spatial Data Science</h2>
    <a href="https://geodacenter.github.io/index.html" class="btn">Homepage</a>
    <a href="https://geodacenter.github.io/download.html" class="btn">Download</a>
    <a href="https://github.com/GeoDaCenter/geoda/" class="btn">View on GitHub</a>
    <a href="https://spatial.uchicago.edu/sample-data" target="_blank" class="btn">Data</a>
    <a href="https://geodacenter.github.io/documentation.html" class="btn">Documentation</a>
    <a href="https://geodacenter.github.io/support.html" class="btn">Support</a>
    <a href="https://geodacenter.github.io/index-cn.html" class="btn">中文</a>
  </section>

  <section class="main-content">


    <h1 class="title toc-ignore">Dimension Reduction Methods (2)</h1>
    <h3 class="subtitle">Distance Preserving Methods</h3>
    <h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
    <h4 class="date">06/28/2020 (latest update)</h4>


    <div id="TOC">
      <ul>
        <li><a href="#introduction">Introduction</a>
          <ul>
            <li><a href="#objectives">Objectives</a>
              <ul>
                <li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#multidimensional-scaling-mds">Multidimensional Scaling (MDS)</a>
          <ul>
            <li><a href="#principle">Principle</a></li>
            <li><a href="#mathematical-details">Mathematical details</a>
              <ul>
                <li><a href="#classic-metric-scaling">Classic metric scaling</a></li>
                <li><a href="#smacof">SMACOF</a></li>
              </ul>
            </li>
            <li><a href="#implementation">Implementation</a>
              <ul>
                <li><a href="#default-settings">Default settings</a></li>
                <li><a href="#saving-the-mds-coordinates">Saving the MDS coordinates</a></li>
                <li><a href="#mds-scatter-plot">MDS scatter plot</a></li>
                <li><a href="#d-mds">3D MDS</a></li>
                <li><a href="#power-approximation">Power approximation</a></li>
                <li><a href="#smacof-1">SMACOF</a></li>
                <li><a href="#categorical-variable">Categorical variable</a></li>
              </ul>
            </li>
            <li><a href="#interpretation">Interpretation</a></li>
          </ul>
        </li>
        <li><a href="#t-sne">t-SNE</a>
          <ul>
            <li><a href="#principle-1">Principle</a></li>
            <li><a href="#implementation-1">Implementation</a>
              <ul>
                <li><a href="#default-settings-1">Default settings</a></li>
                <li><a href="#animation">Animation</a></li>
                <li><a href="#categorical-variable-1">Categorical variable</a></li>
                <li><a href="#tuning-the-optimization">Tuning the optimization</a></li>
              </ul>
            </li>
            <li><a href="#interpretation-1">Interpretation</a>
              <ul>
                <li><a href="#visualizing-a-categorical-variable">Visualizing a categorical variable</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#attribute-and-locational-similarity">Attribute and Locational Similarity</a>
          <ul>
            <li><a href="#linking-mds-scatter-plot-and-map">Linking MDS scatter plot and map</a></li>
            <li><a href="#spatial-weights-from-mds-scatter-plot">Spatial weights from MDS scatter plot</a>
              <ul>
                <li><a href="#matching-attribute-and-geographic-neighbors">Matching attribute and geographic
                    neighbors</a></li>
                <li><a href="#comparing-the-neighbor-structure-of-different-scaling-algorithms">Comparing the neighbor
                    structure of different scaling algorithms</a></li>
              </ul>
            </li>
            <li><a href="#local-neighbor-match-test-based-on-mds-neighbors">Local neighbor match test based on MDS
                neighbors</a></li>
          </ul>
        </li>
        <li><a href="#appendix">Appendix</a>
          <ul>
            <li><a href="#connection-between-a-squared-distance-matrix-and-the-gram-matrix">Connection between a squared
                distance matrix and the Gram matrix</a>
              <ul>
                <li><a href="#double-centering-the-squared-distance-matrix">Double centering the squared distance
                    matrix</a></li>
                <li><a href="#finding-coordinates-from-the-gram-matrix">Finding coordinates from the Gram matrix</a>
                </li>
              </ul>
            </li>
            <li><a href="#power-iteration-method-to-obtain-eigenvalueseigenvectors">Power iteration method to obtain
                eigenvalues/eigenvectors</a></li>
            <li><a href="#iterative-majorization-of-the-stress-function">Iterative majorization of the stress
                function</a></li>
            <li><a href="#barnes-hut-optimization-of-t-sne">Barnes-Hut optimization of t-SNE</a></li>
          </ul>
        </li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <p><br></p>
    <div id="introduction" class="section level2 unnumbered">
      <h2>Introduction</h2>
      <p>In this chapter, we continue our review of ways to reduce the variable dimension of a problem, but now focus on
        so-called <em>distance preserving</em>
        methods, i.e., techniques that attempt to represent, or, more precisely,
        <em>embed</em> data points from a high-dimensional attribute space in a lower dimensional space (typically 2D or
        3D) for easy visualization. This representation is such that the relative distances between the data points are
        preserved as much as possible. We focus on two methods, the classic
        <em>multidimensional scaling</em> (MDS) and the more recently developed <em>t-SNE</em>, a variant of a principle
        referred to as <em>stochastic neighbor embedding</em>. While MDS is a linear technique, t-SNE may be
        more appropriate in the case when the relationships among the variables may be nonlinear. Also, MDS tends to
        assure that points that are far apart in high-dimensional space are also far apart in the lower-dimensional
        space, whereas t-SNE is more attuned at preserving the relative location of close points.
      </p>
      <p>For each method, we first present some basic concepts and the implementation in <code>GeoDa</code>. We then
        consider more
        specifically a
        <em>spatialized</em> approach, where we exploit geovisualization, linking and brushing to explore the connection
        between neighbors in multi-attribute space and neighbors in geographic space.
      </p>
      <p>To illustrate these techniques, we continue to use the Guerry data set on
        moral statistics in 1830 France, which comes pre-installed with <code>GeoDa</code>.</p>
      <div id="objectives" class="section level3 unnumbered">
        <h3>Objectives</h3>
        <ul>
          <li>
            <p>Understand the mathematics behind classic metric multidimensional scaling (MDS)</p>
          </li>
          <li>
            <p>Understand the principle behind the SMACOF algorithm</p>
          </li>
          <li>
            <p>Carry out multidimensional scaling for a set of variables</p>
          </li>
          <li>
            <p>Gain insight into the various options used in MDS analysis</p>
          </li>
          <li>
            <p>Interpret the results of MDS</p>
          </li>
          <li>
            <p>Understand the mathematical principles behind t-SNE</p>
          </li>
          <li>
            <p>Carry out t-SNE to visualize multi-attribute data</p>
          </li>
          <li>
            <p>Understand the role of the various t-SNE optimization parameters</p>
          </li>
          <li>
            <p>Interpret the results of t-SNE</p>
          </li>
          <li>
            <p>Compare closeness in attribute space to closeness in geographic space</p>
          </li>
          <li>
            <p>Carry out local neighbor match test using MDS neighbors</p>
          </li>
        </ul>
        <div id="geoda-functions-covered" class="section level4 unnumbered">
          <h4>GeoDa functions covered</h4>
          <ul>
            <li>Clusters &gt; MDS
              <ul>
                <li>select variables</li>
                <li>MDS methods</li>
                <li>MDS parameters</li>
                <li>saving MDS results</li>
                <li>spatial weights from MDS results</li>
              </ul>
            </li>
            <li>Clusters &gt; t-SNE
              <ul>
                <li>select variables</li>
                <li>t-SNE optimization parameters</li>
                <li>saving t-SNE results</li>
                <li>spatial weights from t-SNE results</li>
              </ul>
            </li>
            <li>Weights Manager
              <ul>
                <li>weights intersection</li>
              </ul>
            </li>
            <li>Space &gt; Local Neighbor Match Test</li>
          </ul>
          <p><br></p>
        </div>
      </div>
    </div>
    <div id="multidimensional-scaling-mds" class="section level2 unnumbered">
      <h2>Multidimensional Scaling (MDS)</h2>
      <div id="principle" class="section level3 unnumbered">
        <h3>Principle</h3>
        <p>Multidimensional Scaling or MDS is a classic multivariate approach designed to portray the <em>embedding</em>
          of a high-dimensional
          data cloud in a lower dimension, typically 2D or 3D. The technique goes back to pioneering work on so-called
          metric scaling by Torgerson <span class="citation">(Torgerson <a href="#ref-Torgerson:52">1952</a>, <a
              href="#ref-Torgerson:58">1958</a>)</span>, and its extension to non-metric scaling by
          Shepard and Kruskal <span class="citation">(Shepard <a href="#ref-Shepard:62a">1962</a><a
              href="#ref-Shepard:62a">a</a>, <a href="#ref-Shepard:62b">1962</a><a href="#ref-Shepard:62b">b</a>;
            Kruskal <a href="#ref-Kruskal:64">1964</a>)</span>. A very nice overview of
          the principles and historical evolution
          of MDS as well as a more complete list of the classic references can be found in <span class="citation">Mead
            (<a href="#ref-Mead:92">1992</a>)</span>.</p>
        <p>MDS is based on the notion of distance between
          observation points in multiattribute space. For <span class="math inline">\(k\)</span> variables, the
          <em>Euclidean</em> distance
          between observations <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> in
          <span class="math inline">\(k\)</span>-dimensional space is
          <span class="math display">\[d_{ij} = || x_i - x_j|| = [\sum_{h=1}^k (x_{ih} - x_{jh})^2]^{1/2},\]</span>
          the square root of the sum of the squared differences between two data points over each variable dimension. In
          this expression, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are
          <span class="math inline">\(k\)</span>-dimensional column vectors, with one value for each of the
          <span class="math inline">\(k\)</span> variables (dimensions). In practice, it turns out that working with the
          squared distance is mathematically more convenient, and it does not affect the properties of the method. In
          order to keep the notation straight, we will use <span class="math inline">\(d_{ij}^2\)</span> for the squared
          distance.
        </p>
        <p>The squared distance can also be expressed as the inner product of the difference between two vectors:
          <span class="math display">\[d_{ij}^2 = (x_i - x_j)&#39;(x_i - x_j),\]</span>
        </p>
        <p>with <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> represented as
          <span class="math inline">\(k \times 1\)</span> column vectors. It is important to keep the dimensions
          straight, since the data points are typically contained in an <span class="math inline">\(n \times k\)</span>
          matrix <span class="math inline">\(X\)</span>, where each observation corresponds to a row of the matrix.
          However, in order to obtain the sum of squared differences as a inner product, each observation (on the <span
            class="math inline">\(k\)</span> variables) must be represented as a column vector.</p>
        <p>An alternative to Euclidean distance is a <em>Manhattan</em> (city block) distance, which is less susceptible
          to outliers, but does not lend itself to being expressed as an inner product of two vectors:
          <span class="math display">\[d_{ij} = \sum_{h=1}^k |x_{ih} - x_{jh}|,\]</span>
          the sum of the absolute differences in each dimension.
        </p>
        <p>The objective of MDS is to find the coordinates of data
          points <span class="math inline">\(z_1, z_2, \dots, z_n\)</span> in 2D or 3D that mimic the distance in
          multiattribute
          space as closely as possible. This can be thought of as minimizing a <em>stress function</em>, <span
            class="math inline">\(S(z)\)</span>:
          <span class="math display">\[S(z) = \sum_i \sum_j (d_{ij} - ||z_i - z_j||)^2.\]</span>
          In other words, a set of coordinates in a lower dimension (<span class="math inline">\(z_i\)</span>, for
          <span class="math inline">\(i = 1, \dots, n\)</span>) are found such that the distances between the resulting
          pairs of points ( <span class="math inline">\(||z_i - z_j||\)</span> ) are as close as possible to their
          pair-wise distances in high-dimensional
          multi-attribute space ( <span class="math inline">\(d_{ij}\)</span> ). Note that, due to the use of a squared
          difference, this objective functions penalizes large differentials more. Hence a tendency of MDS to represent
          points that are far apart rather well, but maybe with less attention to points that are closer together.
        </p>
      </div>
      <div id="mathematical-details" class="section level3 unnumbered">
        <h3>Mathematical details</h3>
        <div id="classic-metric-scaling" class="section level4 unnumbered">
          <h4>Classic metric scaling</h4>
          <p>Classic metric MDS, one of the two methods implemented in <code>GeoDa</code>, approaches the optimization
            problem indirectly by focusing on the
            cross product between the actual vectors of observations <span class="math inline">\(x_i\)</span> and <span
              class="math inline">\(x_j\)</span> rather than between their difference, i.e., the cross-product between
            each pair of rows in the
            observation matrix <span class="math inline">\(X\)</span>. The values for all these cross-products are
            contained in the matrix expression <span class="math inline">\(XX&#39;\)</span>. Note that
            this matrix, often referred to as a <em>Gram</em> matrix, is of dimension <span class="math inline">\(n
              \times n\)</span> (and not <span class="math inline">\(k \times k\)</span> as is the case for the more
            familiar <span class="math inline">\(X&#39;X\)</span> used in PCA).</p>
          <p>If the actual observation matrix <span class="math inline">\(X\)</span> is available, then the eigenvalue
            decomposition of the Gram matrix provides the solution
            to MDS. In the same notation as used for PCA (but now pertaining to the matrix <span
              class="math inline">\(XX&#39;\)</span>):
            <span class="math display">\[XX&#39; = VGV&#39;,\]</span>
            where V is the matrix with eigenvectors (each of dimension <span class="math inline">\(n \times 1\)</span>)
            and <span class="math inline">\(G\)</span> is a <span class="math inline">\(n \times n\)</span> diagonal
            matrix that
            contains the eigenvalues on the diagonal. It turns out that the new coordinates are found
            as:
            <span class="math display">\[Z = VG^{1/2},\]</span>
            i.e., each eigenvector (for <span class="math inline">\(n\)</span> observations) is multiplied by the square
            root of the matching eigenvalue,
            with the eigenvalues ordered in decreasing order.
            Typically, only the first two or three columns of <span class="math inline">\(VG^{1/2}\)</span> are used.
            This also implies that only the first few (largest)
            eigenvalues need to be computed, rather than the full set of <span class="math inline">\(n\)</span> (see the
            <a href="#appendix">Appendix</a> for details).
          </p>
          <p>More generally, MDS only requires the specification of a dissimilarity or squared distance matrix <span
              class="math inline">\(D^2\)</span>, not necessarily the
            full set of observations <span class="math inline">\(X\)</span>. As it turns out, the Gram matrix can be
            obtained from <span class="math inline">\(D^2\)</span> by double centering:
            <span class="math display">\[XX&#39; = - \frac{1}{2} (I - M)D^2(I - M),\]</span>
            where <span class="math inline">\(M\)</span> is the <span class="math inline">\(n \times n\)</span> matrix
            <span class="math inline">\((1/n)\iota \iota&#39;\)</span> and <span class="math inline">\(\iota\)</span> is
            a <span class="math inline">\(n \times 1\)</span> vector of ones (so, <span class="math inline">\(M\)</span>
            is
            an <span class="math inline">\(n \times n\)</span> matrix containing 1/n in every cell). The
            matrix <span class="math inline">\((I - M)\)</span> converts values into deviations from the mean. In the
            literature, the joint pre- and post-multiplication
            by <span class="math inline">\((I - M)\)</span> is referred to as
            <em>double centering</em>.
          </p>
          <p>Given this equivalence, we don’t actually need the matrix <span class="math inline">\(X\)</span> (in order
            to construct <span class="math inline">\(XX&#39;\)</span>) since the eigenvalue decomposition can be applied
            directly to the double centered
            squared distance matrix (see the <a href="#appendix">Appendix</a>). A more formal explanation of the
            mathematical properties can be found in <span class="citation">Borg and Groenen (<a
                href="#ref-BorgGroenen:05">2005</a>)</span>, Chapter 7, and
            <span class="citation">Lee and Verleysen (<a href="#ref-LeeVerleysen:07">2007</a>)</span>, pp. 74-78.
          </p>
        </div>
        <div id="smacof" class="section level4 unnumbered">
          <h4>SMACOF</h4>
          <p>SMACOF stands for “scaling by majorizing a complex function,” and was initially suggested by
            <span class="citation">de Leeuw (<a href="#ref-deLeeuw:77">1977</a>)</span> as an alternative to the
            gradient descent method that was typically employed
            to minimize the MDS stress function. It was motivated in part because the eigenvalue approach outlined
            above for classic metric MDS does not work unless the distance function is Euclidean. In many
            early applications of MDS in psychometrics this was not the case.
          </p>
          <p>The idea behind the <em>iterative majorization</em> method is actually fairly straightforward, but its
            implementation
            in this particular case is quite complex.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> In
            essence, a <em>majorizing function</em> is found that is much simpler than the original function, and, for a
            minimization problem, is always <em>above</em> the actual function. With a function <span
              class="math inline">\(f(x)\)</span>, a majorizing function <span class="math inline">\(g(x,z)\)</span> is
            such that for a fixed point <span class="math inline">\(z\)</span> the two functions are equal, such that
            <span class="math inline">\(f(z) = g(z,z)\)</span> (<span class="math inline">\(z\)</span> is called the
            <em>supporting point</em>). The auxiliary function should be easy to minimize (easier than <span
              class="math inline">\(f(x)\)</span>) and always dominate the original function, such that
            <span class="math inline">\(f(x) \leq g(x,z)\)</span>. This leads to the so-called <em>sandwich
              inequality</em> (coined as such by de Leeuw):
            <span class="math display">\[f(x^*) \leq g(x^*,z) \leq g(z,z) = f(z),\]</span>
            where <span class="math inline">\(x^*\)</span> is the value that minimizes the function <span
              class="math inline">\(g\)</span>.
          </p>
          <p>The <em>iterative</em> part stands for the way in which one proceeds. One starts with a value <span
              class="math inline">\(x_0\)</span>, such that <span class="math inline">\(g (x,x_0) = f(x_0)\)</span>. For
            example, consider a parabola that sits above a complex nonlinear function and touches it at <span
              class="math inline">\(x_0\)</span>, as shown
            in Figure <a href="#fig:smacofig">1</a>. We can then easily find the minimum for the parabola, say at <span
              class="math inline">\(x_1\)</span>. Next, we move a new parabola so that it touches our function at <span
              class="math inline">\(x_1\)</span>, with again
            <span class="math inline">\(g(x,x_1) = f(x_1)\)</span>. In the following step, we find the minimum for the
            new parabola at <span class="math inline">\(x_2\)</span> and keep proceeding in this iterative fashion until
            the difference between <span class="math inline">\(x_k\)</span> and <span
              class="math inline">\(x_{k-1}\)</span> for two consecutive steps is less than a critical value (the
            convergence criterion). At that point we consider the minimum of our parabola to be the same as the minimum
            for the function <span class="math inline">\(f(x)\)</span>, or, close
            enough, given our convergence criterion.
          </p>
          <div class="figure" style="text-align: center"><span id="fig:smacofig"></span>
            <img src="pics7a/99_majorization_fig.png"
              alt="Iterative majorization (source: Borg and Groenen, 2005, p. 180) " width="50%" />
            <p class="caption">
              Figure 1: Iterative majorization (source: Borg and Groenen, 2005, p. 180)
            </p>
          </div>
          <p>In general, the stress function for a <em>solution</em> <span class="math inline">\(Z\)</span> to the MDS
            problem can be written as:
            <span class="math display">\[S(Z) = \sum_{i &lt; j} (\delta_{ij} - d_{ij}(Z))^2 = \sum_{i&lt;j}
              \delta_{ij}^2 + \sum_{i&lt;j} d_{ij}^2(Z) - 2 \sum_{i&lt;j} \delta_{ij}d_{ij}(Z),\]</span>
            where <span class="math inline">\(\delta_{ij}\)</span> is the distance between <span
              class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> for the original
            configuration,
            and <span class="math inline">\(d_{ij}(Z)\)</span> is the corresponding distance for a proposed solution
            <span class="math inline">\(Z\)</span>. Note that <span class="math inline">\(\delta_{ij}\)</span> can be
            based on any distance metric or dissimilarity measure, but <span class="math inline">\(d_{ij}(Z)\)</span> is
            a Euclidean distance. In the stress function, the first term is a constant, since it does not change with
            the values for the coordinates in <span class="math inline">\(Z\)</span>. The second term is a sum of
            squared distances between pairs of points in <span class="math inline">\(Z\)</span>, and the third term is a
            weighted sum of the pairwise distances (each weighted by the initial distances).<a href="#fn3"
              class="footnote-ref" id="fnref3"><sup>3</sup></a> We need to find a set of coordinates <span
              class="math inline">\(Z\)</span> that minimizes <span class="math inline">\(S(Z)\)</span>.
          </p>
          <p>The upshot of the iterative majorization is the so-called <em>Guttman transform</em>, which expresses an
            updated
            solution <span class="math inline">\(Z\)</span> as a function of a tentative solution to the majorizing
            condition, <span class="math inline">\(Y\)</span>, through the following equality:
            <span class="math display">\[Z = (1/n)B(Y)Y,\]</span>
            where the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(B(Y)\)</span>
            has off-diagonal elements <span class="math inline">\(B_{ij} = - \delta_{ij} / d_{ij}(Y)\)</span>, and
            diagonal elements <span class="math inline">\(B_{ii} = - \sum_{j, j\neq i} B_{ij}\)</span>. Note that this
            negative of a sum of negatives turns out to be positive. Basically, the diagonal elements equal the sum of
            the absolute values of all the column/row elements, so that rows and columns sum to zero, i.e., the matrix
            <span class="math inline">\(B\)</span> is double centered (details are given in the section on <a
              href="#iterative-majorization-of-the-stress-function">Iterative majorization of the stress function</a> in
            the Appendix).
          </p>
          <p>In practice, this means that the majorization algorithm proceeds through a number of simple steps:</p>
          <ul>
            <li>
              <p>start by picking a set of (random) starting values for <span class="math inline">\(Z\)</span> and set
                <span class="math inline">\(Y\)</span> to these values</p>
            </li>
            <li>
              <p>compute the stress function for the current value of <span class="math inline">\(Z\)</span></p>
            </li>
            <li>
              <p>find a new value for <span class="math inline">\(Z\)</span> by means of the Guttman transform, using
                the computed distances included in <span class="math inline">\(B(Y)\)</span>, based on the current value
                of <span class="math inline">\(Y\)</span></p>
            </li>
            <li>
              <p>update the stress function and check its change; stop when the change is smaller than a pre-set
                difference</p>
            </li>
            <li>
              <p>if no convergence yet, set <span class="math inline">\(Y\)</span> to the new value of <span
                  class="math inline">\(Z\)</span> and proceed with the update, etc.</p>
            </li>
          </ul>
        </div>
      </div>
      <div id="implementation" class="section level3 unnumbered">
        <h3>Implementation</h3>
        <div id="default-settings" class="section level4 unnumbered">
          <h4>Default settings</h4>
          <p>The MDS functionality in GeoDa is invoked from the <strong>Clusters</strong> toolbar icon, or from the
            main menu, as <strong>Clusters &gt; MDS</strong>. It is the second item in the dimension reduction
            submenu, as shown in Figure <a href="#fig:mdsmenu">2</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsmenu"></span>
            <img src="pics7a/2_695_MDS_menu.png" alt="MDS menu option" width="10%" />
            <p class="caption">
              Figure 2: MDS menu option
            </p>
          </div>
          <p>This brings up the <strong>MDS Settings</strong> panel through which the variables are selected
            and various computational parameters are set. We continue with the same six variables
            as used previously in the PCA example, i.e., <strong>Crm_prs</strong>, <strong>Crm_prp</strong>,
            <strong>Litercy</strong>, <strong>Donatns</strong>, <strong>Infants</strong>,
            and <strong>Suicids</strong>. For now, we leave all options to their default settings,
            shown in Figure <a href="#fig:mdsvars">3</a>. The default method is <strong>classic metric</strong> with the
            variables <strong>standardized</strong>. Since this method only works for Euclidean distances, the
            <strong>Distance Function</strong> option is not available. Importantly, the default number of dimensions is
            set to <strong>2</strong> (see below for <a href="#d-mds">3D MDS</a>).</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsvars"></span>
            <img src="pics7a/2_696_MDS_variables.png" alt="MDS input variable selection" width="40%" />
            <p class="caption">
              Figure 3: MDS input variable selection
            </p>
          </div>
        </div>
        <div id="saving-the-mds-coordinates" class="section level4 unnumbered">
          <h4>Saving the MDS coordinates</h4>
          <p>After clicking the <strong>Run</strong> button, a small dialog is brought up to select the variable
            names for the new MDS coordinates, in Figure <a href="#fig:mdscoords">4</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdscoords"></span>
            <img src="pics7a/2_697_MDS_saveresults.png" alt="MDS result variable selection" width="20%" />
            <p class="caption">
              Figure 4: MDS result variable selection
            </p>
          </div>
          <p>As mentioned, <code>GeoDa</code> offers both the standard 2D MDS as well as a 3D MDS. The default used here
            is 2D, with the new variables V1 and V2 checked (the coordinates in the new space). In a typical
            application, the default variable <em>names</em> of V1 and V2 should be changed to a more appropriate
            choice.</p>
          <p>The coordinates are added to the data table, as illustrated for the first five observations in Figure <a
              href="#fig:mdsintable">5</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsintable"></span>
            <img src="pics7a/88_mdstable.png" alt="MDS coordinates in data table" width="30%" />
            <p class="caption">
              Figure 5: MDS coordinates in data table
            </p>
          </div>
          <p>With some additional computations, we can verify how these results are obtained. First, we need to create
            the matrix <span class="math inline">\(XX&#39;\)</span> from the standardized values for the six variables.
            Next, we compute the eigenvalues and eigenvectors for this <span class="math inline">\(85 \times 85\)</span>
            matrix.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The results are summarized in Figure
            <a href="#fig:mdscalcs">6</a>.</p>
          <p>The top row shows the first two eigenvalues, with their square roots listed immediately below. The
            eigenvector elements are also given for the first five observations (out of 85). The columns V1 and V2 give
            the corresponding MDS coordinates, calculated by multiplying the square root of the eigenvalue with the
            matching elements of the eigenvector (see the mathematical derivation in <a
              href="#classic-metric-scaling">Classic metric scaling</a>). The results are the same as those reported in
            Figure <a href="#fig:mdsintable">5</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdscalcs"></span>
            <img src="pics7a/88_mdscalculations.png" alt="MDS coordinate calculations" width="60%" />
            <p class="caption">
              Figure 6: MDS coordinate calculations
            </p>
          </div>
        </div>
        <div id="mds-scatter-plot" class="section level4 unnumbered">
          <h4>MDS scatter plot</h4>
          <p>After the variable selection, <strong>OK</strong> generates a two-dimensional scatter plot with the
            observations, illustrated in Figure <a href="#fig:mdsscat1">7</a>.
            In this scatter plot, points that are close together in two dimensions should also be close together in the
            six-dimensional attribute space. In addition, two new variables with the coordinate values
            will be added to the data table (as usual, to make this addition permanent, we need to save
            the table). Below the scatter plot are two lines with <strong>statistics</strong>. They can be removed by
            turning the option <strong>View &gt; Statistics</strong> off (the default is on).</p>
          <p>The statistics list the variables included in the analysis and two rough measures of fit. One is the value
            of the stress function for the solution, in our example, this is 0.343 (a lower value indicates a better
            fit). The other is the rank correlation between the distances in multi-attribute space and the distances in
            the lower-dimensional space, here 0.825 (a higher value suggests a stronger correlation between the relative
            ranks of the distances). The method used (<strong>classic metric</strong>) is included in the window top
            banner.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsscat1"></span>
            <img src="pics7a/2_698_MDS_plot.png" alt="MDS plot" width="50%" />
            <p class="caption">
              Figure 7: MDS plot
            </p>
          </div>
          <p>To illustrate the mathematical similarity between PCA and classic metric MDS, consider the scatter plot of
            the first two principal components (using the same six variables) next to the MDS plot (here shown without
            the statistics), as
            in Figure <a href="#fig:mdspcscat1">8</a>. Both plots have the same shape, except for being flipped around
            the x-axis.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdspcscat1"></span>
            <img src="pics7a/22_pca_mds_plots.png" alt="PCA and MDS scatter plots" width="100%" />
            <p class="caption">
              Figure 8: PCA and MDS scatter plots
            </p>
          </div>
          <p>This is further highlighted by selecting the upper-left quadrant in the PCA plot. The matching points in
            the MDS plot are in the lower-left quadrant
            in Figure <a href="#fig:mdspcscat2">9</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdspcscat2"></span>
            <img src="pics7a/22_pca_mds_plots_selected.png" alt="Selected points in PCA and MDS scatter plots"
              width="100%" />
            <p class="caption">
              Figure 9: Selected points in PCA and MDS scatter plots
            </p>
          </div>
          <p>Recall that the signs of the elements of the eigenvectors depend on the computational approach, so that the
            relative positioning is really what counts. Consequently, points (observations) that are close in MDS space,
            will also be close in PCA space, and vice versa.</p>
        </div>
        <div id="d-mds" class="section level4 unnumbered">
          <h4>3D MDS</h4>
          <p>When the <strong># of Dimensions</strong> option is set to <strong>3</strong>, the results of MDS are
            visualized in a 3-dimensional scatterplot cube, as in Figure <a href="#fig:3dmdspcscat1">10</a>. Both the
            stress value (0.196) as well as the rank correlation (0.931) are superior to the two-dimensional solution.
            This is to be expected, since less dimension reduction is involved.<a href="#fn5" class="footnote-ref"
              id="fnref5"><sup>5</sup></a></p>
          <p>Points that are close in the 2D MDS scatter plot are not necessarily close in 3D. In Figure <a
              href="#fig:3dmdspcscat1">10</a>, this seems to be the case, since the three close points selected in the
            2D graph on the right also look close in 3D
            (the points highlighted in yellow).</p>
          <div class="figure" style="text-align: center"><span id="fig:3dmdspcscat1"></span>
            <img src="pics7a/22_3dmds_scat3.png" alt="Selected points in 3D and 2D MDS scatter plots (a)" width="90%" />
            <p class="caption">
              Figure 10: Selected points in 3D and 2D MDS scatter plots (a)
            </p>
          </div>
          <p>However, this is not necessarily the case, and it is important to take advantage of the rotation of the 3D
            scatterplot cube to get a better sense of the location of the points in three dimensions. For example, the
            two points selected in the 2D plot in Figure <a href="#fig:3dmdspcscat2">11</a> turn out not to be that
            close in 3D, after some rotation of the cube.</p>
          <div class="figure" style="text-align: center"><span id="fig:3dmdspcscat2"></span>
            <img src="pics7a/22_3dmds_scat2.png" alt="Selected points in 3D and 2D MDS scatter plots (b)" width="90%" />
            <p class="caption">
              Figure 11: Selected points in 3D and 2D MDS scatter plots (b)
            </p>
          </div>
          <p>In our typical applications, we will focus on the 2D visualization.</p>
        </div>
        <div id="power-approximation" class="section level4 unnumbered">
          <h4>Power approximation</h4>
          <p>The <strong>Parameters</strong> panel in the MDS Settings dialog allows for a number of options to be
            set. The <strong>Transformation</strong> options are the same as for PCA, with the default set to
            <strong>Standardize (Z)</strong>, but <strong>Standardize (MAD)</strong>, <strong>Raw</strong>,
            <strong>Demean</strong>, <strong>Range Adjust</strong> and <strong>Range Standardize</strong> are available
            as well. Best practice is to use the
            variables in standardized form, either the traditional z-transformation, or a more robust alternative (MAD
            or range standardize).
          </p>
          <p>An option that is particularly useful in larger data sets is to use a <strong>Power Iteration</strong> to
            approximate the first few largest eigenvalues needed for the MDS algorithm. Since the first two or three
            eigenvalue/eigenvector pairs are all that is needed for the classic metric implementation, this provides
            considerable computational advantages in large data sets. Recall that the matrix for which the
            eigenvalues/eigenvectors need to be obtained is of dimension <span class="math inline">\(n \times
              n\)</span>. For large <span class="math inline">\(n\)</span>, the standard eigenvalue calculation will
            tend to break down.</p>
          <p>The <strong>Power Iteration</strong> option is selected by checking the box in the
            <strong>Parameters</strong> panel, as in
            Figure <a href="#fig:poweroption">12</a>. This also activates the <strong>Maximum # of Iterations</strong>
            option. The default is set to 1000, which should be fine in most situations. However, if needed,
            it can be adjusted by entering a different value in the corresponding box.</p>
          <div class="figure" style="text-align: center"><span id="fig:poweroption"></span>
            <img src="pics7a/3_709_poweroption.png" alt="MDS power iteration option" width="35%" />
            <p class="caption">
              Figure 12: MDS power iteration option
            </p>
          </div>
          <p>At first sight, the result in Figure <a href="#fig:powerscat">13</a> may seem different from the standard
            output in Figure <a href="#fig:mdsscat1">7</a>, but this is due
            to the indeterminacy of the signs of the eigenvectors. The stress value and rank correlation statistics are
            identical to the default solution. This is not unexpected, since <span class="math inline">\(n = 85\)</span>
            in our example, so that the power approximation is actually not needed.</p>
          <div class="figure" style="text-align: center"><span id="fig:powerscat"></span>
            <img src="pics7a/2_703_power.png" alt="MDS plot (power approximation)" width="50%" />
            <p class="caption">
              Figure 13: MDS plot (power approximation)
            </p>
          </div>
          <p>Closer inspection of the two graphs suggests that the axes are flipped. We can clearly see
            this by selecting the points in the upper right quadrant in the power graph and locating them in the other,
            as in Figure <a href="#fig:mdsflip">14</a>. The relative position
            of the points is the same in the two graphs, but they are on opposite sides of the y-axis.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsflip"></span>
            <img src="pics7a/2_704_powercomparison.png" alt="Computation algorithm comparison" width="100%" />
            <p class="caption">
              Figure 14: Computation algorithm comparison
            </p>
          </div>
        </div>
        <div id="smacof-1" class="section level4 unnumbered">
          <h4>SMACOF</h4>
          <p>The <strong>Method</strong> option provides two approaches to minimize the stress function. The default is
            <strong>classic metric</strong>, but the <strong>smacof</strong> method is provided as an alternative. When
            the latter is selected, as in Figure <a href="#fig:smacof">15</a>,
            the <strong>Maximum # of iterations</strong> and the <strong>Convergence Criterion</strong> can be changed
            as well (those options are not available to the default <strong>classic metric</strong> method). The default
            setting is for <strong>1000</strong> iterations and a convergence criterion of <strong>0.000001</strong>.
            These should be fine for most applications, but they may need to be adjusted if the minimization is not
            accomplished by the maximum number of iterations.</p>
          <div class="figure" style="text-align: center"><span id="fig:smacof"></span>
            <img src="pics7a/33_smacof_option.png" alt="SMACOF method" width="35%" />
            <p class="caption">
              Figure 15: SMACOF method
            </p>
          </div>
          <p>As before, after selecting variable names for the two dimensions, a scatter plot is provided (it lists the
            method in the banner). For our example, the results are as in Figure <a href="#fig:smacofplot">16</a>, with
            the statistics listed at the bottom.</p>
          <div class="figure" style="text-align: center"><span id="fig:smacofplot"></span>
            <img src="pics7a/33_smacof_plot.png" alt="MDS plot using smacof method" width="50%" />
            <p class="caption">
              Figure 16: MDS plot using smacof method
            </p>
          </div>
          <p>Relative to the results for the classic metric method, the <strong>stress value</strong> is poorer, at
            0.487, but the <strong>rank correlation</strong> is better, at 0.905. It is also shown that convergence (for
            the default criterion) is reached after 424 iterations. If the result would show 1000/1000 iterations, that
            would indicate that convergence has not been reached. This means that the default number of iterations would
            need to be increased. Alternatively, the convergence criterion could be relaxed, but that is not
            recommended.</p>
          <p>We can compare the relative position of the points obtained by means of each method through linking and
            brushing, in the usual way. For example, in
            Figure <a href="#fig:smacofclassic">17</a> some close points in the SMACOF solution are shown to be somewhat
            farther apart in the classic metric solution.</p>
          <div class="figure" style="text-align: center"><span id="fig:smacofclassic"></span>
            <img src="pics7a/33_smacof_link2_mds.png" alt="Selected points in SMACOF and classic metric scatter plots"
              width="100%" />
            <p class="caption">
              Figure 17: Selected points in SMACOF and classic metric scatter plots
            </p>
          </div>
          <p>With the <strong>smacof</strong> method selected, it is now possible to choose an alternative distance
            metric for the original points, such as the <strong>Manhattan</strong> block distance. This computes
            absolute
            differences instead of squared differences. The option is shown in Figure <a href="#fig:manhattan">18</a>.
            The effect is to lessen the impact of outliers,
            or large distances in the original high dimensional space.</p>
          <div class="figure" style="text-align: center"><span id="fig:manhattan"></span>
            <img src="pics7a/33_smacof_manhattan.png" alt="SMACOF Manhattan distance function" width="35%" />
            <p class="caption">
              Figure 18: SMACOF Manhattan distance function
            </p>
          </div>
          <p>The result that follows from a Manhattan distance metric is quite different from the
            default Euclidean SMACOF plot. As seen in Figure <a href="#fig:mdsscat2">19</a>, the scale of the
            coordinates is different. Note that the range of values for V1sm and V2sm is much smaller than for the
            metric plot, so even if the points look farther apart, they are actually quite
            close on the scale of Figure <a href="#fig:smacofplot">16</a>. However,
            the identification of <em>close</em> observations can also differ quite a bit between the two plots. This
            can be further explored using linking and brushing (not shown).</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsscat2"></span>
            <img src="pics7a/33_smacof_manhattan_plot.png" alt="SMACOF plot for Manhattan distance" width="50%" />
            <p class="caption">
              Figure 19: SMACOF plot for Manhattan distance
            </p>
          </div>
          <p>Note that in terms of fit, the stress value of 0.317 is better than for SMACOF with Euclidean distance, but
            the rank correlation of 0.786 is worse. Convergence is obtained after 626 iterations, slightly more than for
            the Euclidean distance option.</p>
          <p>The moral of the story is that the MDS scatter plots should not be interpreted in a mechanical way, but
            careful linking and brushing should explore the trade-offs between the different options.</p>
        </div>
        <div id="categorical-variable" class="section level4 unnumbered">
          <h4>Categorical variable</h4>
          <p>The MDS interface includes an option to visualize the position of data points that correspond to a
            particular category. One of the main uses of embedding methods is to discover hidden structure in
            high-dimensional data, such as particular groupings. The <strong>Category Variable</strong> option provides
            a way to visualize how the points that correspond to pre-defined groups in the data position themselves in
            the lower dimensional embedding. For example, in
            Figure <a href="#fig:mdscatvar">20</a>, the <strong>Category Variable</strong> was set to
            <strong>Region</strong>, five broad regions of France, as defined in the Guerry data set (see also the
            left-hand panel of Figure <a href="#fig:tsneregion">36</a>).</p>
          <div class="figure" style="text-align: center"><span id="fig:mdscatvar"></span>
            <img src="pics7a/44_tsne_catvariable.png" alt="MDS categorical variable selection" width="35%" />
            <p class="caption">
              Figure 20: MDS categorical variable selection
            </p>
          </div>
          <p>As a result, after the variable names are entered, the resulting MDS scatter plot designates each point
            with a color corresponding to its category, as in
            Figure <a href="#fig:mdscatplot">21</a>. This primarily for visualization during the exploratory process. An
            alternative way to assess the way the point locations might correspond to different groups is given in the
            section on <a href="#visualizing-a-categorical-variable">Visualizing a categorical variable</a>, discussed
            in the
            context of t-SNE, but equally applicable to MDS scatter plots.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdscatplot"></span>
            <img src="pics7a/77_mdswithcats.png" alt="MDS scatter plot with categorical variables" width="50%" />
            <p class="caption">
              Figure 21: MDS scatter plot with categorical variables
            </p>
          </div>
        </div>
      </div>
      <div id="interpretation" class="section level3 unnumbered">
        <h3>Interpretation</h3>
        <p>In a general sense, the multidimensional scaling method essentially projects points from a high-dimensional
          multivariate attribute space onto a two or three-dimensional space. To get a more precise idea of how this
          process operates, we can link points that are close in the MDS scatter plot to the matching lines in a
          parallel coordinate plot. Data points that are close in multidimensional variable space should be represented
          by lines that are close in the PCP.</p>
        <p>In Figure <a href="#fig:mdspcp">22</a>, we selected two points that are close in the MDS scatter plot and
          assess the matching lines in the six-variable PCP. Since the MDS works on standardized variables, we have
          expressed the PCP in those dimensions as well by means of the <strong>Data &gt; View Standardized
            Data</strong> option. While the lines track each other closely, the match is not perfect, and is better on
          some dimensions than on others. For example, for <strong>Crm_prp</strong> and <strong>Suicids</strong>, the
          values are near identical, whereas for <strong>Infants</strong> the gap between the two values is quite large.
        </p>
        <div class="figure" style="text-align: center"><span id="fig:mdspcp"></span>
          <img src="pics7a/3_708_MDS_PCP.png" alt="MDS and PCP, close points" width="100%" />
          <p class="caption">
            Figure 22: MDS and PCP, close points
          </p>
        </div>
        <p>To provide some context, Figure <a href="#fig:mdspcpa">23</a> provides an example of two points that are far
          from each other in the MDS plot (in the top and bottom of the left hand panel). The gaps between the PCP lines
          are generally much farther from each other, although for <strong>Litercy</strong> and
          <strong>Infants</strong>, they are quite close.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdspcpa"></span>
          <img src="pics7a/3_708_MDS_PCPa.png" alt="MDS and PCP, remote points" width="100%" />
          <p class="caption">
            Figure 23: MDS and PCP, remote points
          </p>
        </div>
        <p>For a proper interpretation, this needs to be examined more carefully, for example,
          by brushing the MDS scatter plot, or, alternatively, by brushing the PCP and examining
          the relative locations of the matching points in the MDS scatter plot.</p>
      </div>
    </div>
    <div id="t-sne" class="section level2 unnumbered">
      <h2>t-SNE</h2>
      <div id="principle-1" class="section level3 unnumbered">
        <h3>Principle</h3>
        <p>A drawback of a linear dimension reduction technique like MDS is that it tends to do less well to keep the
          low-dimensional representation of similar data points close together. t-SNE is an alternative technique that
          addresses this aspect. It was originally outlined in <span class="citation">van der Maaten and Hinton (<a
              href="#ref-vanderMaatenHinton:08">2008</a>)</span> as a refinement of the <em>stochastic neighbor
            embedding</em> (SNE) idea formulated by <span class="citation">Hinton and Roweis (<a
              href="#ref-HintonRoweis:03">2003</a>)</span>.</p>
        <p>Instead of basing the alignment between high and low-dimensional representations on a stress function that
          includes the actual distances, the problem is reformulated as one of matching two different probability
          distributions. These distributions are conditional probabilities that represent similarities between data
          points. More precisely, a conditional probability for the original high-dimensional data of <span
            class="math inline">\(p (j | i)\)</span> expresses the probability that <span
            class="math inline">\(j\)</span> is picked as a neighbor of <span class="math inline">\(i\)</span>. In other
          words, the distance between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>
          is converted into a probability measure using a <em>kernel</em> centered on <span
            class="math inline">\(i\)</span>. The kernel is a symmetric distribution such that the probability decreases
          with distance. For example, using a normal (Gaussian) distribution centered at <span
            class="math inline">\(i\)</span> with a variance of <span class="math inline">\(\sigma_i^2\)</span> would
          yield the conditional distribution as:
          <span class="math display">\[ p_{j|i} = \frac{exp(-d(x_i,x_j)^2/\sigma_i^2)}{\sum_{h \neq i}
            exp(-d(x_i,x_h)^2/\sigma_i^2) },\]</span>
          with <span class="math inline">\(d(x_i, x_j) = ||x_i - x_j||\)</span>, i.e., the Euclidean distance between
          <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in high-dimensional space,
          and, by convention, <span class="math inline">\(p_{i|i} = 0\)</span>. The variance <span
            class="math inline">\(\sigma_i^2\)</span> determines the <em>bandwidth</em> of the kernel and is chosen
          optimally for each point, such that a target <em>perplexity</em> is reached.
        </p>
        <p>Perplexity is a concept that originates in information theory. It is related to the Shannon entropy of a
          distribution, <span class="math inline">\(H = - \sum p_i ln(p_i)\)</span>, such that perplexity <span
            class="math inline">\(= 2^{H}\)</span>. While a complex concept, the perplexity intuitively relates to a
          smooth measure of the effective number of neighbors (fewer neighbors relates to a smaller perplexity). In
          regions with dense points, the variance <span class="math inline">\(\sigma_i^2\)</span> should be smaller,
          whereas in areas with sparse points, it should be larger to reach the same number of neighbors (something
          similar in spirit to the adaptive bandwidth to select the <span class="math inline">\(k\)</span> nearest
          neighbors).</p>
        <p>In t-SNE (as opposed to the original SNE), the conditional probabilities are turned into a joint probability
          <span class="math inline">\(P\)</span> as:
          <span class="math display">\[p_{ij} = \frac{p(j|i) + p(i | j)}{2n},\]</span>
          where <span class="math inline">\(n\)</span> is the number of observations.
        </p>
        <p>The counterpart of the distribution <span class="math inline">\(P\)</span> for the high-dimensional points is
          a distribution <span class="math inline">\(Q\)</span> for the points in a lower dimension. The innovation
          behind the <em>t</em>-SNE is that it uses a thick tailed normalized Student-t kernel with a single degree of
          freedom to avoid some issues associated with the original SNE implementation.<a href="#fn6"
            class="footnote-ref" id="fnref6"><sup>6</sup></a> The associated distribution for points <span
            class="math inline">\(z_i\)</span> in the low-dimensional space is:
          <span class="math display">\[q_{ij} = \frac{(1 + ||z_i - z_j||^2)^{-1}}{\sum_{h \neq l} (1 + ||z_h -
            z_l||^2)^{-1}}.\]</span>
          The measure of <em>fit</em> between the original and embedded distribution is based on information-theoretic
          concepts, i.e., the <em>Kullback-Leibler</em> divergence between the two distributions (smaller is a better
          fit):
          <span class="math display">\[C = KLIC(P || Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
            = \sum_{i \neq j} [ p_{ij} \log p_{ij} - p_{ij} \log q_{ij} ] .\]</span>
          In essence, this boils down to trying to match pairs with a high <span class="math inline">\(p_{ij}\)</span>
          (nearby points in high dimension) to pairs with a high <span class="math inline">\(q_{ij}\)</span> (nearby
          points in the embedded space).
        </p>
        <p>In order to minimize the cost function, we need its gradient. This is the first partial derivative of the
          function <span class="math inline">\(C\)</span> with respect to the coordinates <span
            class="math inline">\(z_i\)</span>. It has the (complex) form:
          <span class="math display">\[\frac{\partial C}{\partial z_i} = 4 \sum_{j \neq i}(p_{ij} - q_{ij})(1 + ||z_i -
            z_j ||^2)^{-1} (z_i - z_j).\]</span>
          This simplifies a bit, if we set <span class="math inline">\(U = \sum_{h \neq l} (1 + ||z_h -
            z_l||^2)^{-1}\)</span>. Therefore, since
          <span class="math inline">\(q_{ij} = (1 + ||z_i - z_j ||^2)^{-1} / U\)</span>, the term
          <span class="math inline">\((1 + ||z_i - z_j ||^2)^{-1}\)</span> can be replaced by <span
            class="math inline">\(q_{ij}U\)</span>, so that
          the gradient becomes:
          <span class="math display">\[\frac{\partial C}{\partial z_i} = 4 \sum_{j \neq i}(p_{ij} - q_{ij})q_{ij}U (z_i
            - z_j).\]</span>
          The optimization involves a complex gradient search that is adjusted with a <em>learning rate</em> and a
          <em>momentum term</em> to speed up the process. At each iteration <span class="math inline">\(t\)</span> the
          value of <span class="math inline">\(z_i^t\)</span> is updated as a function of its value at the previous
          iteration, <span class="math inline">\(z_i^{t-1}\)</span>, a multiplier of the gradient (<span
            class="math inline">\(\partial C / \partial z_i\)</span>) represented by the <em>learning rate</em> <span
            class="math inline">\(\eta\)</span>, and a term that takes into account the change at the previous iteration
          step, the so-called <em>momentum</em> <span class="math inline">\(\alpha(t)\)</span> (indexed by <span
            class="math inline">\(t\)</span> since it is typically adjusted during the optimization
          process):
          <span class="math display">\[z_i^t = z_i^{t-1} + \eta \frac{\partial C}{\partial z_i} + \alpha(t)(z_i^{t-1} -
            z_i^{t-2}).\]</span>
          The learning rate and momentum are parameters to be specified. In addition, the momentum is typically changed
          after a number of iterations, which is another parameter that should be set (see <a
            href="#implementation-1">Implementation</a>).
        </p>
        <p>The most recent implementation of this optimization process uses advanced algorithms that speed up the
          process and allow t-SNE to scale to very large data sets, as outlined
          in <span class="citation">van der Maaten (<a href="#ref-vanderMaaten:14">2014</a>)</span> (some details are
          provided in the Appendix section on <a href="#barnes-hut-optimization-of-t-sne">Barnes-Hut optimization of
            t-SNE</a>). The algorithm is rather finicky and may require some trial and error to find a stable solution.
          An excellent illustration of the sensitivity of the algorithm to various tuning parameters can be found in
          <span class="citation">Wattenberg, Viégas, and Johnson (<a href="#ref-Wattenbergetal:16">2016</a>)</span> (<a
            href="https://distill.pub/2016/misread-tsne/" class="uri">https://distill.pub/2016/misread-tsne/</a>).</p>
      </div>
      <div id="implementation-1" class="section level3 unnumbered">
        <h3>Implementation</h3>
        <div id="default-settings-1" class="section level4 unnumbered">
          <h4>Default settings</h4>
          <p>The t-SNE functionality is the third option in the dimension reduction section of the cluster toolbar,
            shown in Figure <a href="#fig:tsnemenu">24</a>. Alternatively, it can be invoked as <strong>Clusters &gt;
              t-SNE</strong> from the menu.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsnemenu"></span>
            <img src="pics7a/44_tsne.png" alt="t-SNE menu option" width="10%" />
            <p class="caption">
              Figure 24: t-SNE menu option
            </p>
          </div>
          <p>This brings up the main t-SNE interface, in Figure <a href="#fig:tsnevars">25</a>. In the left-hand panel
            is the list of available variables as well as several parameters to fine tune the algorithm. We continue
            with the same six variables as in the previous examples.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsnevars"></span>
            <img src="pics7a/44_tsnevariables.png" alt="t-SNE variable selection" width="80%" />
            <p class="caption">
              Figure 25: t-SNE variable selection
            </p>
          </div>
          <p>The right-hand panel consists of a space at the top for the two-dimensional scatterplot with the results
            shown as the iterations proceed. The bottom panel gives further quantitative information on the quality of
            every 50 iterations, with a value for the <em>error</em> (i.e., the gradient function), which gives an
            indication of how the process is converging to a stable state.</p>
          <p>In-between the two result panes is a small control panel to adjust the speed of the animation, as well as
            allowing to pause and resume as the iterations proceed.</p>
          <p>With all options left at their default values, pressing <strong>Run</strong> yields the full set of 5000
            iterations, resulting in the scatter plot in Figure <a href="#fig:tsnefulliter">26</a>. The bottom pane
            indicates a <strong>final cost</strong> value of 0.241751 and shows that its value has been hovering around
            0.241 for the last 100 iterations.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
          <div class="figure" style="text-align: center"><span id="fig:tsnefulliter"></span>
            <img src="pics7a/44_tsne_fulliteration.png" alt="t-SNE results - full iteration" width="50%" />
            <p class="caption">
              Figure 26: t-SNE results - full iteration
            </p>
          </div>
          <p>At this point, selecting <strong>Save</strong> will bring up the customary interface to select variable
            names for the two dimensions (as in Figure <a href="#fig:mdscoords">4</a>). With these specified, the
            scatter plot is drawn, as in
            Figure <a href="#fig:tsnedefplot">27</a>. As before, this lists the method in the banner, and contains some
            summary statistics at the bottom of the graph. These consist of the <strong>final cost</strong>, the
            corresponding <strong>rank correlation</strong> and the number of <strong>iterations</strong>. Since the
            t-SNE optimization algorithm is not based on a specific stopping criterion, unless the process is somehow
            paused
            (see <a href="#animation">Animation</a>), the number of iterations will always correspond to what is
            specified in the options panel. However, this does not mean that it necessarily has resulted in a stable
            optimum. In practice, one therefore needs to pay close attention to the point pattern of the iterations as
            they proceed.</p>
          <p>The <strong>final cost</strong> is not comparable to the objective function for MDS, since it is not based
            on a stress function. On the other hand, the rank correlation is a rough global measure of the
            correspondence between the relative distances and remains a valid measure. The result of 0.726 is lower than
            for MDS, but in the same general ball-park as SMACOF with Manhattan distance (see Figure <a
              href="#fig:mdsscat2">19</a>). However, it should be kept in mind that t-SNE is geared to optimize
            <em>local</em> distances, so a global performance measure is not fully appropriate.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsnedefplot"></span>
            <img src="pics7a/44_tsne_defaultplot.png" alt="t-SNE default result plot" width="50%" />
            <p class="caption">
              Figure 27: t-SNE default result plot
            </p>
          </div>
        </div>
        <div id="animation" class="section level4 unnumbered">
          <h4>Animation</h4>
          <p>As mentioned, with the default settings, the optimization runs its full course for the number of iterations
            specified in the options. The <strong>Speed</strong> button below the graph on the right-hand side allows
            for the process to be slowed down, by moving the button to the left. This provides a very detailed view of
            the way the optimization proceeds, gets trapped in local optima and then manages to move out of them. In
            addition, the pause button (<strong>||</strong>) can temporarily halt the process. The resume button
            (<strong>&gt;</strong>) continues the optimization. The iteration count is given below the graph and in the
            bottom panel the progression of the gradient minimization can be followed.</p>
          <p>Note that the pause button is different from the <strong>Stop</strong> button below the options in the
            left-hand panel. The latter ends the iterations and freezes the result, without the option to resume.
            However once frozen, one can move back through the sequence of iterations by moving the
            <strong>Iteration</strong> button to the left.</p>
          <p>For example, in Figure <a href="#fig:tsneiter129">28</a>, the speed was set to about half the default and
            the iterations were paused at 129. The gradient (error) is still quite large: compare 48.5 to 0.241 at the
            final stage.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsneiter129"></span>
            <img src="pics7a/44_tsne_iter1.png" alt="t-SNE results - animation stop at 129" width="50%" />
            <p class="caption">
              Figure 28: t-SNE results - animation stop at 129
            </p>
          </div>
          <p>As the iterations proceed further, the point cloud constantly changes until it starts to attain a more
            stable form. For example, in Figure <a href="#fig:tsneiter662">29</a>, the plot is shown with the process
            stopped after 662 iterations. The error of 0.245 at iteration 650 is very similar to the result for a full
            set of 5000 iterations. Basically, after 650-700 iterations, the process has stabilized around a particular
            configuration with only minor changes.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsneiter662"></span>
            <img src="pics7a/44_tsne_iter500.png" alt="t-SNE results - animation stop at 662" width="50%" />
            <p class="caption">
              Figure 29: t-SNE results - animation stop at 662
            </p>
          </div>
          <p>The corresponding final scatter plot in
            Figure <a href="#fig:tsneiter662plot">30</a> shows a rank correlation of 0.726, the same as for the full set
            of iterations.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsneiter662plot"></span>
            <img src="pics7a/44_tsne_iter500plot.png" alt="t-SNE result plot at 662" width="50%" />
            <p class="caption">
              Figure 30: t-SNE result plot at 662
            </p>
          </div>
        </div>
        <div id="categorical-variable-1" class="section level4 unnumbered">
          <h4>Categorical variable</h4>
          <p>The t-SNE interface includes the same option as MDS to visualize the position of data points that
            correspond to a particular category,
            as in Figure <a href="#fig:mdscatvar">20</a>. However, for t-SNE, this does not only apply to the final
            scatter plot (similar
            to its MDS counterpart in Figure <a href="#fig:mdscatplot">21</a>), but also during the iterations shown in
            the animation panel.</p>
          <p>Using the same selection as for MDS, the iteration plot shows how the region categories move around as the
            process converges,
            e.g., as in Figure <a href="#fig:tsnecatplot">31</a>. Here as well, some more flexibility can be obtained
            through the use of the <strong>bubble chart</strong>,
            as illustrated below
            in the section on <a href="#visualizing-a-categorical-variable">Visualizing a categorical variable</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsnecatplot"></span>
            <img src="pics7a/44_tsne_catresults.png" alt="t-SNE iterations with categorical variables" width="50%" />
            <p class="caption">
              Figure 31: t-SNE iterations with categorical variables
            </p>
          </div>
        </div>
        <div id="tuning-the-optimization" class="section level4 unnumbered">
          <h4>Tuning the optimization</h4>
          <p>The number of options to tune the optimization algorithm may seem a bit overwhelming. However, in many
            instances, the default settings will be fine, although some experimenting is always a good idea.</p>
          <p>To illustrate the effect of some of the parameters, we will change <strong>Theta</strong>, the
            <strong>Perplexity</strong> and the <strong>Iteration Switch</strong> for the <strong>Momentum</strong> (see
            the technical discussion for details on the meaning and interpretation).</p>
          <p>The Barnes-Hut algorithm implemented for t-SNE sets the <strong>Theta</strong> parameter to 0.5 by default.
            This is the criterion that determines the extent of simplification for the <span
              class="math inline">\(q_{ij}\)</span> measures in the gradient equation. This parameter is primarily
            intended to allow the algorithm to scale up to large data sets. In smaller data sets, like in our example,
            we may want to set this to zero, which avoids the simplification. The result is as in Figure <a
              href="#fig:tsnetheta">32</a>. There are slight differences with the default result, but overall the
            performance is somewhat worse. The final cost is 0.312 (compared to 0.241) and the rank correlation is
            inferior as well, 0.682 relative to 0.726.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsnetheta"></span>
            <img src="pics7a/44_tsne_theta.png" alt="t-SNE result plot with theta = 0" width="50%" />
            <p class="caption">
              Figure 32: t-SNE result plot with theta = 0
            </p>
          </div>
          <p>The second parameter to adjust is the <strong>Perplexity</strong> (with <strong>Theta</strong> set back to
            its default value). This is a rough proxy for the number of neighbors and is by default set to its maximum
            (<span class="math inline">\(n/3 = 28\)</span>). The results for a value of 15 are given in Figure <a
              href="#fig:tsneperp">33</a>. Again, the results are much worse than for the default setting, with a final
            cost of 0.449 and a rank correlation of 0.537.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsneperp"></span>
            <img src="pics7a/44_tsne_perp.png" alt="t-SNE result plot with perplexity = 15" width="50%" />
            <p class="caption">
              Figure 33: t-SNE result plot with perplexity = 15
            </p>
          </div>
          <p>The final adjustment is to the <strong>Iteration Switch</strong> for the <strong>Momentum</strong>
            parameter, which decides at which point in the iterations the <strong>Momentum</strong> moves from 0.5 to
            0.8. The default value is 250. In
            Figure <a href="#fig:tsnemom">34</a>, the result is shown for a value of 100. Again, the results are worse
            than for the default settings, with a final cost of 0.293 and a rank correlation of 0.718.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsnemom"></span>
            <img src="pics7a/44_tsne_momentum.png" alt="t-SNE result plot with momentum iteration switch=100"
              width="50%" />
            <p class="caption">
              Figure 34: t-SNE result plot with momentum iteration switch=100
            </p>
          </div>
          <p>Further experimentation with the various parameters can provide deep insight into the dynamics of this
            complex algorithm, although the specifics will vary from case to case. Also, as argued in <span
              class="citation">van der Maaten (<a href="#ref-vanderMaaten:14">2014</a>)</span>, the default settings are
            an excellent starting point. .</p>
        </div>
      </div>
      <div id="interpretation-1" class="section level3 unnumbered">
        <h3>Interpretation</h3>
        <p>The interpretation of the results of t-SNE is the same as for the standard MDS plots. Linking and brushing
          between subsets of points in the scatter plot and PCP graphs will provide insight into the particulars of the
          observation groupings.</p>
        <p>As mentioned, a common interest is to assess the extent to which certain pre-specified <em>clusters</em> are
          reflected in the scatter plot, both for MDS as well as for t-SNE. This is visualized when a <strong>Category
            Variable</strong> is specified. In addition, a more flexible visualization of various categories
          (pre-defined groupings or clusters) can be obtained by leveraging the functionality of the <strong>bubble
            plot</strong>.</p>
        <div id="visualizing-a-categorical-variable" class="section level4 unnumbered">
          <h4>Visualizing a categorical variable</h4>
          <p>To illustrate this feature, we will use the <strong>Region</strong> variable in the Guerry data set. The
            pre-specified groupings or clusters should be represented by an integer unique value or categorical
            variable, but that is not the case for <strong>Region</strong>, which in the original data set is given by a
            letter code. A quick way to turn the letter code into a numerical value is to create a unique value map (as
            shown in the left-hand panel of Figure <a href="#fig:tsneregion">36</a>) and save the categories to a new
            variable, say <strong>regno</strong>.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> In
            addition, to get the bubble chart to work properly, we need a variable that is set to a constant.<a
              href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
          <p>We invoke the chart by means of <strong>Explore &gt; Bubble Chart</strong> from the menu, or from the
            <strong>Bubble Chart</strong> toolbar icon. In the interface, shown in
            Figure <a href="#fig:tsnebubble">35</a>, we specify the first coordinate as <strong>X-Axis</strong> (here,
            <strong>V1tsne</strong>) and the second coordinate at <strong>Y-Axis</strong> (<strong>V2tsne</strong>).
            Next, we set the <strong>Bubble Size</strong> to the <strong>constant</strong> we just created, and
            <strong>Standard Deviation Color</strong> to <strong>regno</strong>.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsnebubble"></span>
            <img src="pics7a/44_cat_bubblechart.png" alt="t-SNE bubble chart interface" width="50%" />
            <p class="caption">
              Figure 35: t-SNE bubble chart interface
            </p>
          </div>
          <p>The default bubble plot color is for a standard deviational classification, but we can change that by right
            clicking and setting <strong>Classification Themes &gt; Unique Values</strong>. Finally, we use the
            <strong>Adjust Bubble Size</strong> option to change the size of the circles to something meaningful. The
            result is as in the
            right-hand panel of
            Figure <a href="#fig:tsneregion">36</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:tsneregion"></span>
            <img src="pics7a/44_tsne_regionsmap.png" alt="t-SNE points by region" width="100%" />
            <p class="caption">
              Figure 36: t-SNE points by region
            </p>
          </div>
          <p>The match between the locations in the t-SNE plot and the five main regions is far from perfect, although
            the locations in the north and east tend to group in the lower-right quadrant, whereas the south and central
            ones are more in the upper half of the chart. Nevertheless, this illustrates the mechanism through which
            different cluster or regional definitions can be mapped to the corresponding locations in a MDS scatter
            plot. We pursue this in more detail in the discussion of attribute and locational similarity that follows.
          </p>
        </div>
      </div>
    </div>
    <div id="attribute-and-locational-similarity" class="section level2 unnumbered">
      <h2>Attribute and Locational Similarity</h2>
      <p>The points in the MDS scatter plot can be viewed as <em>locations</em> in a two-dimensional attribute space.
        This is an example of the use of geographical concepts (location, distance) in contexts where the space is
        non-geographical. It also allows us to investigate and visualize the tension between <em>attribute
          similarity</em> and <em>locational similarity</em>, two core components underlying the notion of spatial
        autocorrelation. These two concepts are also central to the various clustering techniques that we consider in
        later chapters.</p>
      <p>An explicit <em>spatial</em> perspective is introduced by linking the MDS scatter plot with various map views
        of the data. In addition, it is possible to exploit the location of the points in attribute space to construct
        spatial weights based on neighboring locations. These weights can then be compared to their geographical
        counterparts to discover overlap. Finally, we can construct an alternative version of the local neighbor match
        test using the distances in an MDS plot instead of the full k-nearest neighbor distances. We discuss each of
        these prespectives in turn. For further technical details, see <span class="citation">Anselin and Li (<a
            href="#ref-AnselinLi:20">2020</a>)</span>.</p>
      <div id="linking-mds-scatter-plot-and-map" class="section level3 unnumbered">
        <h3>Linking MDS scatter plot and map</h3>
        <p>To illustrate the extent to which <em>neighbors in attribute space</em> correspond to <em>neighbors in
            geographic space</em>, we link a selection of close points in the MDS scatter plot to a quartile map of the
          first principal component. Clearly, this can easily be extended to all kinds of maps of the variables
          involved, including co-location maps that pertain to multiple variables, cluster maps, etc.</p>
        <p>In Figure <a href="#fig:mdsmap1">37</a>, we selected nine points that are close in attribute space in the
          right-hand panel of an MDS scatter plot using the classic metric approach for our six variables. We connect
          this to a quartile map of the first principal component (using the default SVD setting), and assess the extent
          to which the locations match. All points in the graph are in the upper quartile for PC1, and many, but not
          all, are also closely located in geographic space. We can explore this further by brushing the scatter plot to
          systematically evaluate the extent of the match between attribute and locational similarity for any number of
          variables. In addition, we can apply this principle to a link between locations in any of the univariate or
          multivariate cluster maps and the MDS scatter plot.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdsmap1"></span>
          <img src="pics7a/55_link_mdspc1.png" alt="Neighbors in MDS space" width="100%" />
          <p class="caption">
            Figure 37: Neighbors in MDS space
          </p>
        </div>
        <p>Alternatively, we can <em>brush</em> the map and identify the matching observations in the
          MDS scatter plot, as in Figure <a href="#fig:mdsmap2">38</a>. This reverses the logic and assesses the extent
          to which neighbors
          in geographic space are also neighbors in attribute space. In this instance in our
          example, the result is mixed, with some evidence of a match between the two concepts for selected points, but
          not for others.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdsmap2"></span>
          <img src="pics7a/66_linkmapmds.png" alt="Brushing the map and MDS" width="100%" />
          <p class="caption">
            Figure 38: Brushing the map and MDS
          </p>
        </div>
        <p>With an active spatial weights matrix, we can explore this more precisely by using the <strong>Connectivity
            &gt; Show Selection and Neighbors</strong> in the map. This will highlight the corresponding points in the
          MDS scatter plot.</p>
      </div>
      <div id="spatial-weights-from-mds-scatter-plot" class="section level3 unnumbered">
        <h3>Spatial weights from MDS scatter plot</h3>
        <p>The points in the MDS scatter plot can be viewed as <em>locations</em> in an embedded attribute space. As
          such, they can be <em>mapped</em>. In such a point map, the neighbor structure among the points can be
          exploited to create spatial weights, in exactly the same way as for geographic points (e.g., distance bands,
          k-nearest neighbors, contiguity from Thiessen polygons). Conceptually, such spatial weights are similar to the
          distance weights created from multiple variables, but they are based on inter-observation distances from an
          embedded high-dimensional object in two dimensions. While this involves some loss of information, the
          associated two-dimensional visualization is highly intuitive.</p>
        <p>In <code>GeoDa</code>, there are three ways to create spatial weights from points in a MDS scatter plot. One
          is to explicitly create a point layer using the MDS coordinates (for example, <strong>V1</strong> and
          <strong>V2</strong>), by means of <strong>Tools &gt; Shape &gt; Points from Table</strong>. Once the point
          layer is in place, the standard spatial weights functionality can be invoked.</p>
        <p>A second way pertains only to distance weights. It again uses the <strong>Weights Manager</strong>, but with
          the <strong>Variables</strong> option for <strong>Distance Weight</strong>. For example, in Figure <a
            href="#fig:mdsweights2">39</a>, this is illustrated for k-nearest neighbor weights with <span
            class="math inline">\(k = 6\)</span> and the t-SNE coordinates as the variables. Since these coordinates are
          already normalized in a way, we can safely use the <strong>Raw</strong> setting for standardization.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdsweights2"></span>
          <img src="pics7a/66_tsne_k6.png" alt="Create weights using MDS coordinates" width="35%" />
          <p class="caption">
            Figure 39: Create weights using MDS coordinates
          </p>
        </div>
        <p>A final method applies directly to the MDS scatterplot. As shown in Figure <a href="#fig:mdsweights">40</a>,
          one of the options available is to <strong>Create Weights</strong>.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdsweights"></span>
          <img src="pics7a/4_createwts.png" alt="Create weights from MDS scatter plot" width="25%" />
          <p class="caption">
            Figure 40: Create weights from MDS scatter plot
          </p>
        </div>
        <p>This brings up the standard <strong>Weights File Creation</strong> interface, as shown in
          Figure <a href="#fig:mdsqueen">41</a>. The example again illustrates k-nearest neighbor weights with <span
            class="math inline">\(k=6\)</span>. The difference with the previous approach is that the MDS coordinates do
          not need to be specified, but are taken directly from the MDS scatter plot where the option is invoked. In
          addition, it is also possible to create contiguity weights from the Thiessen polygons associated with the
          scatter plot points.</p>
        <div class="figure" style="text-align: center"><span id="fig:mdsqueen"></span>
          <img src="pics7a/66_newweightsfrommds.png" alt="Queen weights from MDS scatter plot" width="35%" />
          <p class="caption">
            Figure 41: Queen weights from MDS scatter plot
          </p>
        </div>
        <p>Once created, the weights file appears in the <strong>Weights Manager</strong> and its properties are listed,
          in the same way as for other weights.</p>
        <div id="matching-attribute-and-geographic-neighbors" class="section level4 unnumbered">
          <h4>Matching attribute and geographic neighbors</h4>
          <p>We can now investigate the extent to which the k-nearest neighbors in geographic space match neighbors in
            attribute space using the <strong>Intersection</strong> functionality in the <strong>Weights
              Manager</strong> (Figure <a href="#fig:mdsintersect">42</a>).</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsintersect"></span>
            <img src="pics7a/66_weightsintersection.png" alt="Intersection knn weights" width="35%" />
            <p class="caption">
              Figure 42: Intersection knn weights
            </p>
          </div>
          <p>This creates a <strong>gal</strong> weights file that lists for each observation the neighbors that are
            shared between the two k-nearest neighbor files. As is to be expected, the resulting file is much sparser
            than the original weights. In the <strong>Histogram</strong>, we can see the distribution of the number of
            matches. We can then <strong>Save Connectivity to Table</strong> to create a variable that reflects this
            cardinality. Such a variable then lends itself to a <strong>Unique Values Map</strong>, as in Figure <a
              href="#fig:mdsgalconn1">43</a>. Even though the associated colors are not ideal (they can easily be
            changed), this identifies five departments that have four out of the six nearest neighbors in common between
            the two weights.</p>
          <div class="figure" style="text-align: center"><span id="fig:mdsgalconn1"></span>
            <img src="pics7a/66_tsne_graph.png" alt="Intersection knn weights for geography and MDS t-SNE"
              width="80%" />
            <p class="caption">
              Figure 43: Intersection knn weights for geography and MDS t-SNE
            </p>
          </div>
          <p>When the connectivity graph is superimposed on the map, the location of the overlap becomes very clear. The
            red lines on the map correspond with matches between the two types of k-nearest neighbors.</p>
          <p>The presence of several sets of interconnected vertices suggest the presence of <em>multivariate
              clustering</em>, albeit without a measure of significance. The local neighbor match test is based on the
            same principle, but also provides a measure of significance (as well as a better colored unique values map).
          </p>
        </div>
        <div id="comparing-the-neighbor-structure-of-different-scaling-algorithms" class="section level4 unnumbered">
          <h4>Comparing the neighbor structure of different scaling algorithms</h4>
          <p>We can use the <strong>Intersection</strong> functionality for spatial weights to compare the nearest
            neighbor structures implied by classic metric MDS,
            SMACOF and t-SNE in terms of how closely they match the geographical nearest neighbor relations. We just
            illustrated the use of the connectivity graph and the neighbor cardinality to visualize the similarity
            between two types of weights. In addition, we can also
            interpret the summary characteristics in the <strong>Weights Manager</strong> to get an assessment of the
            degree of agreement.</p>
          <p>For example, we compare the percentage non-zero to the standard result, which is 7.06% for k=6 in our
            example. For the classic
            metric MDS weights, this percentage is 1.59%, for SMACOF it is 1.72% and for t-SNE 1.97%. Compared to the
            maximum achievable, the
            relevant percentages are 22.5% for MDS, 24.4% for SMACOF and 27.9% for t-SNE. We call this metric the
            <em>common coverage percentage</em>.
            The results confirm the superiority of t-SNE in terms of
            capturing <em>local</em> structure, relative to the two MDS methods, which are more geared towards
            respecting large distances.</p>
          <p>We can also compare the common coverage percentage between the three scaling algorithms:
            for metric MDS-SMACOF this is 55.2%, for MDS-t-SNE 41.5%, and for SMACOF-t-SNE 45.6%. This further
            highlights that t-SNE captures
            a different type of neighbor relation than the classic MDS methods. The specific locations of overlap can be
            explored
            further using the connectivity graph as shown above.</p>
        </div>
      </div>
      <div id="local-neighbor-match-test-based-on-mds-neighbors" class="section level3 unnumbered">
        <h3>Local neighbor match test based on MDS neighbors</h3>
        <p>A final, more formal approach to check the similarity between neighbors in attribute space and neighbors in
          geographic space is as a special case of the
          local neighbor match test. Instead of using the k-nearest neighbor relation for the high-dimensional space, we
          can use the same relation in the two- or three-dimensional MDS space.</p>
        <p>In the interface for <strong>Space &gt; Local Neighbor Match Test</strong>, we select the MDS coordinates as
          the variables. For example, in Figure <a href="#fig:tsneknn">44</a>, the t-SNE coordinates are selected to
          create k-nearest neighbor weights for <span class="math inline">\(k=6\)</span>.</p>
        <div class="figure" style="text-align: center"><span id="fig:tsneknn"></span>
          <img src="pics7a/55_link_tsnematch1.png" alt="K-nearest neigbhors weights of t-SNE coordinates" width="40%" />
          <p class="caption">
            Figure 44: K-nearest neigbhors weights of t-SNE coordinates
          </p>
        </div>
        <p>Clicking on <strong>Run</strong> gives the option to save the cardinality of the weights intersection between
          the attribute knn weights and the geographical knn weights, as well as the associated probability of finding
          that many neighbors in common. Next, the associated unique value map is brought up that shows the
          cardinalities of the match for each location, with the corresponding connectivity graph superimposed, as in
          Figure <a href="#fig:tsnematchtest">45</a>.
          Note that this is identical to the map shown in Figure <a href="#fig:mdsgalconn1">43</a>,
          except for the different colors.</p>
        <p>The <strong>cpval</strong> column in the data table reveals that locations with four matches have an
          associated <span class="math inline">\(p\)</span>-value of 0.00011. This map can then be compared to the
          results found for the multi-attribute k-nearest distances.<a href="#fn10" class="footnote-ref"
            id="fnref10"><sup>10</sup></a></p>
        <div class="figure" style="text-align: center"><span id="fig:tsnematchtest"></span>
          <img src="pics7a/55_link_tsnematch2.png" alt="Local neighbor match test for t-SNE coordinates" width="80%" />
          <p class="caption">
            Figure 45: Local neighbor match test for t-SNE coordinates
          </p>
        </div>
      </div>
    </div>
    <div id="appendix" class="section level2 unnumbered">
      <h2>Appendix</h2>
      <div id="connection-between-a-squared-distance-matrix-and-the-gram-matrix" class="section level3 unnumbered">
        <h3>Connection between a squared distance matrix and the Gram matrix</h3>
        <p>The reason why the solution to the metric MDS problem can be found in a fairly straightforward analytic way
          has to do with the interesting relationship between the squared distance matrix and the Gram matrix for the
          observations, the matrix <span class="math inline">\(XX&#39;\)</span>. Note that this only holds for Euclidean
          distances. As a result, the Manhattan distance option is not available for the classic metric MDS.</p>
        <p>The squared Euclidean distance between two points <span class="math inline">\(i\)</span> and <span
            class="math inline">\(j\)</span> in <span class="math inline">\(k\)</span>-dimensional space is:
          <span class="math display">\[d^2_{ij} = \sum_{h=1}^k (x_{ih} - x_{jh})^2,\]</span>
          where <span class="math inline">\(x_{ih}\)</span> corresponds to the <span class="math inline">\(h\)</span>-th
          value in the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(X\)</span> (i.e.,
          the <span class="math inline">\(i\)</span>-th observation), and <span class="math inline">\(x_{jh}\)</span> is
          the same for the <span class="math inline">\(j\)</span>-th row.
          After working out the individual terms of the squared difference, this becomes:
          <span class="math display">\[d^2_{ij} = \sum_{h=1}^k (x_{ih}^2 + x_{jh}^2 - 2 x_{ih}x_{jh}).\]</span>
          The full matrix <span class="math inline">\(D^2\)</span> for all pairwise distances between observations for a
          given variable (column) <span class="math inline">\(h\)</span> simply contains the corresponding term in each
          position <span class="math inline">\(i-j\)</span>. For the diagonal terms, this boils down to
          <span class="math inline">\(x_{ih}^2 + x_{ih}^2 - 2 x_{ih}x_{ih} = 0\)</span>, as desired. Each off-diagonal
          element is <span class="math inline">\(x_{ih}^2 + x_{jh}^2 - 2 x_{ih}x_{jh}\)</span>. The full matrix <span
            class="math inline">\(D\)</span> for a given variable/column <span class="math inline">\(h\)</span> can be
          expressed as the sum of three matrices. The first matrix has <span class="math inline">\(x_{ih}^2\)</span> in
          each row <span class="math inline">\(i\)</span> (so <span class="math inline">\(x_{1h}^2\)</span> in the first
          row, <span class="math inline">\(x_{2h}^2\)</span> in the second row, etc.). The second matrix has <span
            class="math inline">\(x_{ih}^2\)</span> in each column <span class="math inline">\(i\)</span> (so <span
            class="math inline">\(x_{1h}^2\)</span> in the first column, <span class="math inline">\(x_{2h}^2\)</span>
          in the second column, etc.). The third matrix has all the cross-products, or <span
            class="math inline">\(x_hx_h&#39;\)</span>. Note that the diagonal of the matrix <span
            class="math inline">\(x_h x_h&#39;\)</span> is made up of the squares, <span
            class="math inline">\(x_{ih}^2\)</span>. So, with this diagonal expressed as a vector <span
            class="math inline">\(c_h\)</span>, it turns out that the squared distance matrix between all pairs of
          observations for a given variable <span class="math inline">\(h\)</span> is:
          <span class="math display">\[D^2_h = c_{h} \iota&#39; + \iota c_{h}&#39; - 2 x_{h} x_{h}&#39;,\]</span>
          where <span class="math inline">\(\iota\)</span> is a <span class="math inline">\(n \times 1\)</span> vector
          of ones.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> As a result, the full <span
            class="math inline">\(n \times n\)</span> squared distance matrix consists of the sum of the individual
          squared distance matrices over all variables/columns:
          <span class="math display">\[D^2 = c \iota&#39; + \iota c&#39; - 2 XX&#39;,\]</span>
          since <span class="math inline">\(\sum_{h = 1}^k x_h x_h&#39; = XX&#39;\)</span>, and with each element of
          <span class="math inline">\(c = \sum_{h=1}^k x_{ih}^2\)</span>, for <span class="math inline">\(i = 1, \dots,
            n\)</span>.
        </p>
        <div id="double-centering-the-squared-distance-matrix" class="section level4 unnumbered">
          <h4>Double centering the squared distance matrix</h4>
          <p>An important result in metric MDS is that the Gram matrix can be obtained from <span
              class="math inline">\(D^2\)</span> by double centering:
            <span class="math display">\[XX&#39; = - \frac{1}{2} (I - M)D^2(I - M),\]</span>
            where <span class="math inline">\((I - M)\)</span> is as above.
            With some tedious matrix algebra, we can show that this actually holds. Since <span
              class="math inline">\(X\)</span> is standardized, <span class="math inline">\(X&#39; \iota = 0\)</span>
            and <span class="math inline">\(\iota&#39; X = 0\)</span> (the sum of deviations from the mean is zero).
            Also, <span class="math inline">\(\iota &#39; \iota = n\)</span>.
          </p>
          <p>First, we post-multiply <span class="math inline">\(D^2\)</span> by <span class="math inline">\((I -
              M)\)</span>. This yields:
            <span class="math display">\[c \iota&#39; + \iota c&#39; - 2 XX&#39; - (1/n) c \iota&#39; \iota \iota&#39;
              - (1/n) \iota c&#39; \iota \iota&#39; - (2/n) XX&#39;\iota \iota&#39;,\]</span>
            Recall that <span class="math inline">\(c\)</span> contains the diagonal elements of <span
              class="math inline">\(XX&#39;\)</span> so that <span class="math inline">\(c &#39; \iota\)</span> is the
            sum of the diagonal elements, or the trace of <span class="math inline">\(XX&#39;\)</span>, say <span
              class="math inline">\(t\)</span>, a scalar.
            Moreover, with a standardized matrix <span class="math inline">\(X\)</span> as we use here, <span
              class="math inline">\(t = n\)</span> (since the variance for each variable equals 1), although in the end,
            that does not matter to obtain the correct result. Using the other properties from above, the expression
            becomes:
            <span class="math display">\[c \iota&#39; + \iota c&#39; - 2 XX&#39; - (n/n) c \iota&#39;
              - (t/n) \iota \iota&#39; - 0 = \iota c&#39; - 2 XX&#39;
              - (t/n) \iota \iota&#39;.\]</span>
            Now, pre-multiplying this by <span class="math inline">\((I - M)\)</span> gives:
            <span class="math display">\[ \iota c&#39; - 2 XX&#39;
              - (t/n) \iota \iota&#39;
              - (1/n) \iota \iota&#39; \iota c&#39; + (2/n) \iota \iota&#39; XX&#39;
              + (t/n^2) \iota \iota&#39; \iota \iota&#39;,\]</span>
            or, again using the trace property:
            <span class="math display">\[\iota c&#39; - 2 XX&#39;
              - (t/n) \iota \iota&#39;
              - (n/n) \iota c&#39; + 0
              + (t/n) \iota \iota&#39;,\]</span>
            and removing the terms that cancel out:
            <span class="math display">\[ - 2 XX&#39;,\]</span>
            the desired result.
          </p>
        </div>
        <div id="finding-coordinates-from-the-gram-matrix" class="section level4 unnumbered">
          <h4>Finding coordinates from the Gram matrix</h4>
          <p>While we have established the equivalence between a doubly-centered squared distance matrix and the matrix
            <span class="math inline">\(XX&#39;\)</span>, that does not give us coordinates for <span
              class="math inline">\(X\)</span> that would yield he proper distances. Assuming that we only have <span
              class="math inline">\(XX&#39;\)</span>, what would such coordinates be?</p>
          <p>Here, we resort again to the eigenvalue decomposition of <span class="math inline">\(XX&#39;\)</span>,
            which is a square and symmetric <span class="math inline">\(n \times n\)</span> matrix. The decomposition
            is:
            <span class="math display">\[XX&#39; = VGV&#39;,\]</span>
            with the all matrices of dimension <span class="math inline">\(n \times n\)</span>, <span
              class="math inline">\(G\)</span> a diagonal matrix with the eigenvalues on the diagonal, and <span
              class="math inline">\(V\)</span> a matrix with the eigenvectors as the columns. Since the matrix is
            symmetric, all the eigenvalues are positive, so that their square roots exist. We can thus write the
            decomposition as:
            <span class="math display">\[XX&#39; = VG^{1/2}G^{1/2}V&#39; = (VG^{1/2})(VG^{1/2})&#39;.\]</span>
            In other words, the matrix <span class="math inline">\(VG^{1/2}\)</span> can play the role of <span
              class="math inline">\(X\)</span>. It is not the same as <span class="math inline">\(X\)</span>, in fact it
            is expressed as coordinates in a different (rotated) axis system, but they yield the same distances, so they
            have the same <em>relative</em> positions. In practice, to express this in lower dimensions, we only select
            the first and second (and sometimes third) columns of <span class="math inline">\(VG^{1/2}\)</span>, so we
            only need the two/three largest eigenvalues and corresponding eigenvectors. They can be readily computed by
            means of the power iteration method.
          </p>
        </div>
      </div>
      <div id="power-iteration-method-to-obtain-eigenvalueseigenvectors" class="section level3 unnumbered">
        <h3>Power iteration method to obtain eigenvalues/eigenvectors</h3>
        <p>The power method is an iterative approach to obtain the eigenvector associated with the largest eigenvalue of
          a matrix <span class="math inline">\(A\)</span>. It is found from an iteration that starts with an arbitrary
          unit vector, say <span class="math inline">\(x_0\)</span>,<a href="#fn12" class="footnote-ref"
            id="fnref12"><sup>12</sup></a> and iterates by applying higher and higher powers to the transformation <span
            class="math inline">\(x_{k} = A x_{k-1} = A^k x_{0}\)</span>. So, starting with an arbitrary unit initial
          vector <span class="math inline">\(x_0\)</span>, we get <span class="math inline">\(x_1 = Ax_0\)</span>, and
          then subsequently <span class="math inline">\(x_2 = Ax_1 = A^2x_0\)</span>, and so on until convergence (i.e.,
          the difference between <span class="math inline">\(x_k\)</span> and <span
            class="math inline">\(x_{k-1}\)</span> is less than a pre-defined threshold). The end value, say <span
            class="math inline">\(x_k\)</span>, is a good approximation to the dominant eigenvector of <span
            class="math inline">\(A\)</span>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
        <p>The associated eigenvalue is found from the so-called <em>Rayleigh</em> quotient:
          <span class="math display">\[\lambda = \frac{x_k&#39; A x_k}{x_k &#39; x_k}.\]</span>
          The second largest eigenvector is found by applying the same procedure to the matrix <span
            class="math inline">\(B\)</span>:
          <span class="math display">\[B = A - \lambda x_k x_k&#39;,\]</span>
          using the largest eigenvalue and eigenvector just computed. The idea can be applied to obtain more
          eigenvalues/eigenvectors, although typically this method is only used to compute a few of the largest ones.
          The procedure can also be applied to the
          inverse matrix <span class="math inline">\(A^{-1}\)</span> to obtain the smallest eigenvalues.
        </p>
      </div>
      <div id="iterative-majorization-of-the-stress-function" class="section level3 unnumbered">
        <h3>Iterative majorization of the stress function</h3>
        <p>The discussion here is largely based on the materials in <span class="citation">Borg and Groenen (<a
              href="#ref-BorgGroenen:05">2005</a>)</span>, Chapter 8, except that the notation is slightly different, to
          keep it consistent with the rest of the notes.</p>
        <p>The objective of the iterative majorization of the stress function is to find a simpler function that bounds
          the stress function from above (so, the stress function is always smaller), and to iterate until its minimum
          and the minimum of the stress function coincide.</p>
        <p>The first step in this is to turn the stress function into a matrix expression, as a function of the solution
          matrix <span class="math inline">\(Z\)</span>. It consists of the three parts given in the main text:
          <span class="math display">\[S(Z) = \sum_{i&lt;j} \delta_{ij}^2 + \sum_{i&lt;j} d_{ij}^2(Z) - 2 \sum_{i&lt;j}
            \delta_{ij}d_{ij}(Z)\]</span>
          One way to write the difference between observation pairs <span class="math inline">\(i-j\)</span> for
          coordinate <span class="math inline">\(h\)</span> in <span class="math inline">\(Z\)</span> (i.e., the values
          contained in column <span class="math inline">\(h\)</span>), or <span class="math inline">\(z_{ih}-
            z_{jh}\)</span>, is by considering an <span class="math inline">\(n \times 1\)</span> vector <span
            class="math inline">\(e_i\)</span> to pick out element <span class="math inline">\(i\)</span> and a vector
          <span class="math inline">\(e_j\)</span> to pick out element <span class="math inline">\(j\)</span>. These
          vectors are respectively the <span class="math inline">\(i-th\)</span> and <span
            class="math inline">\(j-th\)</span> column of an identity matrix. Then, <span class="math inline">\((e_i -
            e_j)&#39;z_{*h}\)</span> yields a <span class="math inline">\(1 \times n\)</span> vector
          with <span class="math inline">\(z_{ih} - z_{jh}\)</span>. To obstain the squared distance between <span
            class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> for a given variable/column
          <span class="math inline">\(h\)</span> we thus use <span class="math inline">\(z_{*h}&#39;(e_i - e_j)(e_i -
            e_j)&#39;z_{*h}\)</span>. In the MDS literature, the matrix <span class="math inline">\((e_i - e_j)(e_i -
            e_j)&#39; = A_{ij}\)</span>, a <span class="math inline">\(n \times n\)</span> matrix with <span
            class="math inline">\(a_{ii} = a_{jj} = 1\)</span> and <span class="math inline">\(a_{ij} = a_{ji} =
            -1\)</span>. Considering all column dimensions of <span class="math inline">\(Z\)</span> jointly then gives
          the squared distance between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>
          as <span class="math inline">\(d^2_{ij}(Z) = \sum_{h=1}^k z_h&#39;A_{ij}z_h = tr Z&#39;A_{ij}Z\)</span>, with
          <span class="math inline">\(tr\)</span> as the trace operator.
          Summing this over all the pairs (without double counting) gives
          <span class="math display">\[\sum_{i&lt;j} d_{ij}^2(Z) = tr(Z&#39; \sum_{i &lt;j}A_{ij}Z) =
            tr(Z&#39;VZ),\]</span>
          with the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(V = \sum_{i
            &lt;j}A_{ij}\)</span>, a row and column centered marix (i.e., each row and each column sums to zero), with
          <span class="math inline">\(n - 1\)</span> on the diagonals and <span class="math inline">\(-1\)</span> in all
          other positions. Given the row and column centering, this matrix is singular.
        </p>
        <p>The third term, <span class="math inline">\(- 2 \sum_{i&lt;j} \delta_{ij}d_{ij}(Z)\)</span> is where the
          majorization comes into play. Consider a new set of coordinates, <span class="math inline">\(Y\)</span>, which
          will be critical in setting up the majorization function. The “trick” is to take advantage of the so-called
          <em>Cauchy-Schwarz inequality</em>:
          <span class="math display">\[ \sum_m z_my_m \leq (\sum_m z_m^2)^{1/2} (\sum_m y_m^2)^{1/2},\]</span>
          i.e., the sum of the cross-products of all values <span class="math inline">\(m\)</span> of <span
            class="math inline">\(z\)</span> and <span class="math inline">\(y\)</span> is less or equal to the product
          of the norms for each of the variables (the square root of the sums of squares, a measure of <em>length</em>
          for a vector).
        </p>
        <p>If we take <span class="math inline">\(z\)</span> as <span class="math inline">\((z_i - z_j)\)</span> and
          <span class="math inline">\(y\)</span> as <span class="math inline">\((y_i - y_j)\)</span>, for the <span
            class="math inline">\(i\)</span>-th and <span class="math inline">\(j\)</span>-th rows of respectively <span
            class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span>, with <span
            class="math inline">\(i \neq j\)</span>, then we have:
          <span class="math display">\[ \sum_m (z_{im} - z_{jm})(y_{im} - y_{jm}) \leq [\sum_m (z_{im} -
            z_{jm})^2]^{1/2}[\sum_m (y_{im} - y_{jm})^2]^{1/2},\]</span>
          where the right hand side corresponds to <span class="math inline">\(d_{ij}(Z).d_{ij}(Y)\)</span>, i.e., the
          product of the distance between <span class="math inline">\(i\)</span> and <span
            class="math inline">\(j\)</span> for <span class="math inline">\(Z\)</span> and <span
            class="math inline">\(Y\)</span>. For <span class="math inline">\(z = y\)</span>, equality results, but in
          all other instances the right hand side dominates the left-hand side. After moving around some terms, we end
          up with:
          <span class="math display">\[ - d_{ij}(Z) \leq - \frac{\sum_m (z_{im} - z_{jm})(y_{im} -
            y_{jm})}{d_{ij}(Y)}.\]</span>
          Using the same expression for <span class="math inline">\(A_{ij}\)</span> as above, this can also be written
          as:
          <span class="math display">\[- d_{ij}(Z) \leq - tr(Z&#39;A_{ij}Y) / d_{ij}(Y),\]</span>
          or:
          <span class="math display">\[- d_{ij}(Z) \leq - tr[Z&#39;(A_{ij}/d_{ij}(Y))Y],\]</span>
          since <span class="math inline">\(d_{ij}(Y)\)</span> is just a scalar. Extending this logic, we obtain:
          <span class="math display">\[- \delta_{ij} d_{ij}(Z) \leq - tr[Z&#39;(\delta_{ij}/d_{ij}(Y))A_{ij}Y],\]</span>
          where <span class="math inline">\(\delta_{ij}/d_{ij}(Y)\)</span> is just a scalar multiple of the four
          non-zero entries in <span class="math inline">\(A_{ij}\)</span>, based on the
          distance between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> computed
          for the elements of <span class="math inline">\(Y\)</span>.
        </p>
        <p>We now get the counterpart of the matrix <span class="math inline">\(V\)</span> above from summing the <span
            class="math inline">\(A_{ij}\)</span> over all the relevant pairs, each multiplied by <span
            class="math inline">\(\delta_{ij}/d_{ij}(Y)\)</span>. Note that this ratio ends up in positions <span
            class="math inline">\(i,i\)</span> and <span class="math inline">\(j,j\)</span>, and its negative in <span
            class="math inline">\(i,j\)</span> and <span class="math inline">\(j,i\)</span>. Consequently:
          <span class="math display">\[\sum_{i &lt; j} (\delta_{ij}/d_{ij}(Y))A_{ij} = B(Y),\]</span>
          with off-diagonal elements <span class="math inline">\(B_{ij} = - \delta_{ij} / d_{ij}(Y)\)</span>, and
          diagonal elements <span class="math inline">\(B_{ii} = - \sum_{j, j\neq i} B_{ij}\)</span>.
        </p>
        <p>The <em>majorization condition</em> for the third term can then be written as:
          <span class="math display">\[ - tr[Z&#39;B(Z)Z] \leq - tr[Z&#39;B(Y)Y].\]</span>
        </p>
        <p>The majorization of the complete stress function follows as:
          <span class="math display">\[ S(Z) = \sum_{i&lt;j} \delta_{ij}^2 + tr(Z&#39;VZ) - 2tr[Z&#39;B(Z)Z]
            \leq \sum_{i&lt;j} \delta_{ij}^2 + tr(Z&#39;VZ) - 2tr[Z&#39;B(Y)Y].\]</span>
          It is minimized with respect to <span class="math inline">\(Z\)</span> for <span class="math inline">\(2VZ -
            2B(Y)Y = 0\)</span>.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>
        </p>
        <p>As it turns out, <span class="math inline">\(V\)</span> is singular (due to the double centering), so that
          the usual solution <span class="math inline">\(Z = V^{-1}B(Y)Y\)</span> doesn’t work. Instead, we have to
          resort to a generalized or Moore-Penrose inverse <span class="math inline">\(V^+\)</span>. For double-centered
          matrices, this takes the special form <span class="math inline">\(V^+ = (V + \iota \iota&#39;)^{-1} - n^{-2}
            \iota \iota&#39;\)</span>, with <span class="math inline">\(\iota\)</span> as a vector of ones.<a
            href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> The second term doesn’t matter, since <span
            class="math inline">\(\iota&#39;B(Y) = 0\)</span>, due to the double centering. For the case we consider,
          where we do not use weights, the first term simplifies to <span class="math inline">\(V^+ = (1/n)(I -
            (1/n)\iota \iota&#39;)\)</span> so that the solution becomes:
          <span class="math display">\[Z = (1/n) B(Y)Y,\]</span>
          the so-called <em>Guttman transform</em>.
        </p>
        <p>In the <code>GeoDa</code> implementation of SMACOF, the initial values for each column of <span
            class="math inline">\(Y\)</span> are set as standard normal random variates (using the default seed set in
          the global <strong>Preferences</strong>).
          <br>
        </p>
      </div>
      <div id="barnes-hut-optimization-of-t-sne" class="section level3 unnumbered">
        <h3>Barnes-Hut optimization of t-SNE</h3>
        <p>The implementation of t-SNE in <code>GeoDa</code> is adopted from the tree-based algorithms outlined in <span
            class="citation">van der Maaten (<a href="#ref-vanderMaaten:14">2014</a>)</span>, themselves based on a
          principle proposed by <span class="citation">Barnes and Hut (<a href="#ref-BarnesHut:86">1986</a>)</span>,
          hence the reference to Barnes-Hut optimization. The van der Maaten approach is based on two simplifications
          that allow the algorithms to scale to very large data sets. One strategy is to parse the distribution <span
            class="math inline">\(P\)</span> and eliminate all the probabilities <span
            class="math inline">\(p_{ij}\)</span> that are <em>too small</em> in some sense. The other strategy is to do
          something similar to the probabilities <span class="math inline">\(q_{ij}\)</span> in the sense that the value
          for several individual points that are <em>close</em> together is represented by a central point (hence
          reducing the number of individual <span class="math inline">\(q_{ij}\)</span> that need to be evaluated). This
          is the Barnes-Hut aspect.</p>
        <p>The t-SNE gradient is separated into two parts (in the same notation as before):
          <span class="math display">\[\frac{\partial C}{\partial z_i} = 4 [\sum_{j \neq i} p_{ij}q_{ij}U(z_i - z_j) -
            \sum_{j \neq i} q_{ij}^2 U (z_i - z_j)],\]</span>
          where the first part (the cross-products) represents an <em>attractive</em> force and the second part (the
          squared probabilities)
          represents a <em>repulsive</em> force. The simplification of the <span class="math inline">\(p_{ij}\)</span>
          targets the first part (converting many products to a value of zero), the simplification of <span
            class="math inline">\(q_{ij}\)</span> addresses the second part (reducing the number of expressions that
          need to be evaluated).
        </p>
        <p>In order to simplify the <span class="math inline">\(p_{ij}\)</span> we consider those pairs that are more
          than <span class="math inline">\(3u\)</span> apart (where <span class="math inline">\(u\)</span> is the
          perplexity, i.e., roughly the target number of neighbors). For the points <span
            class="math inline">\(j\)</span> that are farther away than this critical distance, the probability <span
            class="math inline">\(p_{ij}\)</span> is set to zero. The rationale for this is that the value of <span
            class="math inline">\(p_{ij}\)</span> for those pairs that do not meet the closeness criterion is likely to
          be very small and hence can be ignored (i.e., their <span class="math inline">\(p_{ij}\)</span> is set to
          <span class="math inline">\(0\)</span>). As a result of the choice of the distance criterion, the value for
          perplexity set as an option can be at most <span class="math inline">\(n/3\)</span>.<a href="#fn16"
            class="footnote-ref" id="fnref16"><sup>16</sup></a> The parsing of <span
            class="math inline">\(p_{ij}\)</span> is implemented by means of a so-called <em>vantage-point tree</em> (VP
          tree). In essence, this tree divides the space into those points that are closer than a given distance to a
          reference point (the <em>vantage-point</em>) and those that are further away. By applying this recursively,
          the data get partitioned into smaller and smaller entities such that neighbors in the tree are likely to be
          neighbors in space <span class="citation">(see Yianilos <a href="#ref-Yianilos:93">1993</a> for technical
            details)</span>. The VP tree is used to carry out a nearest neighbor search so that all <span
            class="math inline">\(j\)</span> that are not in the nearest neighbor set for a given <span
            class="math inline">\(i\)</span> result in <span class="math inline">\(p_{ij} = 0\)</span>. Note that the
          <span class="math inline">\(p_{ij}\)</span> do not change during the optimization, since they pertain to the
          high-dimensional space, so this calculation only needs to be done once.</p>
        <p>The Barnes-Hut algorithm deals with the layout of the points <span class="math inline">\(z\)</span> at each
          iteration. First, a quadtree is created for the current layout.<a href="#fn17" class="footnote-ref"
            id="fnref17"><sup>17</sup></a> The tree is traversed (depth-first) and at each node a decision is made
          whether the points in that cell can be effectively represented by their center. The logic behind this is that
          the distance from <span class="math inline">\(i\)</span> to all the points <span
            class="math inline">\(j\)</span> in the cell will be very similar, so rather than computing each individual
          <span class="math inline">\(q_{ij}^2U(z_i - z_j)\)</span> for all the <span class="math inline">\(j\)</span>
          in the cell, they are replaced by
          <span class="math inline">\(n_c q^2_{ic}(1 + ||z_i - z_c ||^2)^{-1} (z_i - z_c)\)</span>, where <span
            class="math inline">\(n_c\)</span> is the number of points in the cell and <span
            class="math inline">\(z_c\)</span> are the coordinates of a central (representative) point. The denser the
          points are in a given cell, the greater the computational gain.<a href="#fn18" class="footnote-ref"
            id="fnref18"><sup>18</sup></a>
        </p>
        <p>A critical decision is the selection of those cells for which the simplification is appropriate. This is set
          by the <span class="math inline">\(\theta\)</span> parameter to the optimization program
          (<strong>Theta</strong> in Figure <a href="#fig:tsnevars">25</a>), such that:
          <span class="math display">\[\frac{r_c}{||z_i - z_c||} &lt; \theta,\]</span>
          where <span class="math inline">\(r_c\)</span> is the length of the diagonal of the cell square. In other
          words, the cell is summarized when the diagonal of the cell is less than <span
            class="math inline">\(\theta\)</span> times the distance between a point <span
            class="math inline">\(i\)</span> and the center of the cell. With <span
            class="math inline">\(\theta\)</span> set to zero, there is no simplification going on and all the <span
            class="math inline">\(q_{ij}\)</span> are computed at each iteration.
        </p>
        <p>The software implementation of this algorithm in <code>GeoDa</code> is based on the <a
            href="https://github.com/lvdmaaten/bhtsne">C++ code by van der Maaten</a>.</p>
      </div>
    </div>
    <div id="references" class="section level2 unnumbered">
      <h2>References</h2>
      <div id="refs" class="references">
        <div id="ref-AnselinLi:20">
          <p>Anselin, Luc, and Xun Li. 2020. “Tobler’s Law in a Multivariate World.” <em>Geographical Analysis</em>. <a
              href="https://doi.org/10.111/gean.12237">https://doi.org/10.111/gean.12237</a>.</p>
        </div>
        <div id="ref-BanerjeeRoy:14">
          <p>Banerjee, Sudipto, and Anindya Roy. 2014. <em>Linear Algebra and Matrix Analysis for Statistics</em>. Boca
            Raton: Chapman &amp; Hall/CRC.</p>
        </div>
        <div id="ref-BarnesHut:86">
          <p>Barnes, J., and P. Hut. 1986. “A Hierarchical O(N log N) Force-Calculation Algorithm.” <em>Nature</em> 324
            (4): 446–49.</p>
        </div>
        <div id="ref-BorgGroenen:05">
          <p>Borg, Ingwer, and Patrick J.F. Groenen. 2005. <em>Modern Multidimensional Scaling, Theory and Applications
              (2nd Ed)</em>. New York, NY: Springer.</p>
        </div>
        <div id="ref-deLeeuw:77">
          <p>de Leeuw, Jan. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In <em>Recent
              Developments in Statistics</em>, edited by J.R. Barra, F. Brodeau, G. Romier, and B. van Cutsem, 133–45.
            Amsterdam: North Holland.</p>
        </div>
        <div id="ref-deLeeuwMair:09">
          <p>de Leeuw, Jan, and Patrick Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.”
            <em>Journal of Statistical Software</em> 21 (3).</p>
        </div>
        <div id="ref-HintonRoweis:03">
          <p>Hinton, Geoffrey E., and Sam T. Roweis. 2003. “Stochastic Neighbor Embedding.” In <em>Advances in Neural
              Information Processing Systems 15 (NIPS 2002)</em>, edited by S. Becker, S. Thun, and K. Obermayer,
            833–40. Vancouver, BC: NIPS.</p>
        </div>
        <div id="ref-Kruskal:64">
          <p>Kruskal, Joseph. 1964. “Multidimensional Scaling by Optimizing Goodness of Fit to a Non-Metric Hypothesis.”
            <em>Psychometrika</em> 29: 1–17.</p>
        </div>
        <div id="ref-LeeVerleysen:07">
          <p>Lee, John A., and Michel Verleysen. 2007. <em>Nonlinear Dimensionality Reduction</em>. New York, NY:
            Springer-Verlag.</p>
        </div>
        <div id="ref-Mead:92">
          <p>Mead, A. 1992. “Review of the Development of Multidimensional Scaling Methods.” <em>Journal of the Royal
              Statistical Society. Series D (the Statistician)</em> 41: 27–39.</p>
        </div>
        <div id="ref-RaoMitra:71">
          <p>Rao, C., and S.Mitra. 1971. <em>Generalized Inverse of a Matrix and Its Applications</em>. New York, NY:
            John Wiley.</p>
        </div>
        <div id="ref-Shepard:62a">
          <p>Shepard, Roger N. 1962a. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance
            Function I.” <em>Psychometrika</em> 27: 125–40.</p>
        </div>
        <div id="ref-Shepard:62b">
          <p>———. 1962b. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function II.”
            <em>Psychometrika</em> 27: 219–46.</p>
        </div>
        <div id="ref-Torgerson:52">
          <p>Torgerson, Warren S. 1952. “Multidimensional Scaling, I: Theory and Method.” <em>Psychometrika</em> 17:
            401–19.</p>
        </div>
        <div id="ref-Torgerson:58">
          <p>———. 1958. <em>Theory and Methods of Scaling</em>. New York, NY: John Wiley.</p>
        </div>
        <div id="ref-vanderMaaten:14">
          <p>van der Maaten, Laurens. 2014. “Accelerating t-SNE Using Tree-Based Algorithms.” <em>Journal of Machine
              Learning Research</em> 15: 1–21.</p>
        </div>
        <div id="ref-vanderMaatenHinton:08">
          <p>van der Maaten, Laurens, and Geoffrey Hinton. 2008. “Visualizing Data Using t-SNE.” <em>Journal of Machine
              Learning Research</em> 9: 2579–2605.</p>
        </div>
        <div id="ref-Wattenbergetal:16">
          <p>Wattenberg, Martin, Fernanda Viégas, and Ian Johnson. 2016. “How to Use t-SNE Effectively.”
            <em>Distill</em>. <a
              href="https://doi.org/10.23915/distill.00002">https://doi.org/10.23915/distill.00002</a>.</p>
        </div>
        <div id="ref-Yianilos:93">
          <p>Yianilos, Peter N. 1993. “Data Structures and Algorithms for Nearest Neighbor Search in General Metric
            Spaces.” In <em>Proceedings of the ACM-SIAM Symposium on Discrete Algorithms</em>, 311–21. Philadelphia, PA:
            SIAM.</p>
        </div>
      </div>
    </div>
    <div class="footnotes">
      <hr />
      <ol>
        <li id="fn1">
          <p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu"
              class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩</a></p>
        </li>
        <li id="fn2">
          <p>For an extensive discussion, see <span class="citation">Borg and Groenen (<a
                href="#ref-BorgGroenen:05">2005</a>)</span>, Chapter 8. A software implementation is outlined in <span
              class="citation">de Leeuw and Mair (<a href="#ref-deLeeuwMair:09">2009</a>)</span>.<a href="#fnref2"
              class="footnote-back">↩</a></p>
        </li>
        <li id="fn3">
          <p>In a more general exposition, as in <span class="citation">Borg and Groenen (<a
                href="#ref-BorgGroenen:05">2005</a>)</span> and <span class="citation">de Leeuw and Mair (<a
                href="#ref-deLeeuwMair:09">2009</a>)</span>, the distances can be weighted, primarily to deal with
            missing values, but we will ignore that aspect here.<a href="#fnref3" class="footnote-back">↩</a></p>
        </li>
        <li id="fn4">
          <p>To find the actual values (not reported separately in <code>GeoDa</code>), a routine such as the
            <code>eigen</code> function in <code>R</code> can be employed. However, keep in mind that the signs of the
            elements of the eigenvector may differ between routines. In the illustration given here, the actual values
            used by <code>GeoDa</code> for the default setting are reported.<a href="#fnref4"
              class="footnote-back">↩</a></p>
        </li>
        <li id="fn5">
          <p>Unlike what is the case for the 2D MDS scatter plot, the statistics can currently not be turned off in
            3D.<a href="#fnref5" class="footnote-back">↩</a></p>
        </li>
        <li id="fn6">
          <p>For technical details related to the so-called <em>crowding</em> problem, see the discussion in
            <span class="citation">van der Maaten and Hinton (<a href="#ref-vanderMaatenHinton:08">2008</a>)</span>.<a
              href="#fnref6" class="footnote-back">↩</a>
          </p>
        </li>
        <li id="fn7">
          <p>Due to the Barnes-Hut approximation, the final cost value does not equal the value for the
            <strong>error</strong> at the last iteration. As mentioned in the discussion of
            the <a href="#barnes-hut-optimization-of-t-sne">Barnes-Hut optimization of t-SNE</a>, when
            <strong>Theta</strong> is not zero, several values of <span class="math inline">\(q_{ij}\)</span> are
            replaced by the center of the corresponding quadtree block. The <strong>final cost</strong> reported is
            computed with the values for all <span class="math inline">\(i-j\)</span> pairs included in the calculation,
            and does not use the simplified <span class="math inline">\(q_{ij}\)</span>, whereas the
            <strong>error</strong> reported for each iteration uses the simplified version.<a href="#fnref7"
              class="footnote-back">↩</a></p>
        </li>
        <li id="fn8">
          <p>The specific steps involved are invoked by selecting <strong>Classification Themes &gt; Unique
              Values</strong> and choosing <strong>Region</strong> as the variable. Then <strong>Save
              Categories</strong> will save integer values to a new variable.<a href="#fnref8"
              class="footnote-back">↩</a></p>
        </li>
        <li id="fn9">
          <p>To accomplish this, we use the <strong>Calculator</strong> with the <strong>Univariate &gt; Assign</strong>
            option and set the constant to 1, as an integer (the actual value doesn’t matter, since we will be adjusting
            the size of the bubble).<a href="#fnref9" class="footnote-back">↩</a></p>
        </li>
        <li id="fn10">
          <p>See <span class="citation">Anselin and Li (<a href="#ref-AnselinLi:20">2020</a>)</span> for further
            examples.<a href="#fnref10" class="footnote-back">↩</a></p>
        </li>
        <li id="fn11">
          <p>Post-multiplying the vector <span class="math inline">\(c\)</span> with a row vector of ones puts the same
            element in each row, pre-multiplying its transpose by a column vector of ones puts the same element in each
            column.<a href="#fnref11" class="footnote-back">↩</a></p>
        </li>
        <li id="fn12">
          <p><span class="math inline">\(x_0\)</span> is typically a vector of ones divided by the square root of the
            dimension. So, if the dimension is <span class="math inline">\(n\)</span>, the value would be <span
              class="math inline">\(1/\sqrt{n}\)</span>.<a href="#fnref12" class="footnote-back">↩</a></p>
        </li>
        <li id="fn13">
          <p>For a formal proof, see, e.g., <span class="citation">Banerjee and Roy (<a
                href="#ref-BanerjeeRoy:14">2014</a>)</span>, pp. 353-354.<a href="#fnref13" class="footnote-back">↩</a>
          </p>
        </li>
        <li id="fn14">
          <p>This exploits some results from matrix partial derivatives, namely that
            <span class="math inline">\(\partial Z&#39;VZ / \partial Z = VZ\)</span> and <span
              class="math inline">\(\partial Z&#39;BY / \partial Z = BY\)</span>.<a href="#fnref14"
              class="footnote-back">↩</a>
          </p>
        </li>
        <li id="fn15">
          <p>The formal result is based on
            <span class="citation">Rao and S.Mitra (<a href="#ref-RaoMitra:71">1971</a>)</span>, pp. 181-182.<a
              href="#fnref15" class="footnote-back">↩</a>
          </p>
        </li>
        <li id="fn16">
          <p>In <span class="citation">van der Maaten and Hinton (<a href="#ref-vanderMaatenHinton:08">2008</a>)</span>
            a range for perplexity between 5 and 50 was suggested, but in smaller data sets, this is not possible under
            the Barnes-Hut approach. For example, in the Guerry example, with <span class="math inline">\(n=85\)</span>,
            the maximum value for perplexity would be <span class="math inline">\(85/3 = 28\)</span>, which is much less
            than 50.<a href="#fnref16" class="footnote-back">↩</a></p>
        </li>
        <li id="fn17">
          <p>A quadtree is a tree data structure that recursively partitions the space into squares as long as there are
            points in the resulting space. For example, the bounding box of the points is first divided into four
            squares. If one of those is empty, that becomes an endpoint in the tree (a leaf). For those squares with
            points in them, the process is repeated, again stopping whenever a square is empty. This structure allows
            for very fast searching of the tree. In three diminsions, the counterpart (using cubes) is called an
            octree.<a href="#fnref17" class="footnote-back">↩</a></p>
        </li>
        <li id="fn18">
          <p>For further technical details, see <span class="citation">van der Maaten (<a
                href="#ref-vanderMaaten:14">2014</a>)</span>.<a href="#fnref18" class="footnote-back">↩</a></p>
        </li>
      </ol>
    </div>

    <footer class="site-footer">
      <span class="site-footer-owner"><a href="https://github.com/lixun910/geoda">GeoDa</a> is maintained by <a
          href="#">lixun910</a>.</span>
      <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>
        using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a
          href="https://twitter.com/jasonlong">Jason Long</a>.</span>
    </footer>

  </section>


  <!-- code folding -->


  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>

</html>