<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="author" content="Luc Anselin" />


  <title>Spatial Clustering (2)</title>

  <script src="lab9c_files/header-attrs-2.3/header-attrs.js"></script>
  <link href="lab9c_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
  <script src="lab9c_files/highlightjs-9.12.0/highlight.js"></script>
  <title>GeoDa on Github</title>

  <style>
    * {
      margin: 0;
      padding: 0;
    }

    .shadowfilter {
      -webkit-filter: drop-shadow(12px 12px 7px rgba(0, 0, 0, 0.5));
      filter: url(shadow.svg#drop-shadow);
    }

    .intro1 {
      margin-left: -45px;
    }
  </style>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css"
    media="screen">
  <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
  <style>
    ul {
      padding-left: 30px;
    }

    figcaption {
      top: .70em;
      left: .35em;
      bottom: auto !important;
      right: auto !important;
    }
  </style>

  <style>
    h1 {
      text-align: center;
    }

    h3.subtitle {
      text-align: center;
    }

    h4.author {
      text-align: center;
    }

    h4.date {
      text-align: center;
    }

    p.caption {
      font-size: 12px;
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LC0QJ53WFS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-LC0QJ53WFS');
  </script>
  <!-- End Google tag -->

  <style type="text/css">
    code {
      white-space: pre;
    }
  </style>
  <script type="text/javascript">
    if (window.hljs) {
      hljs.configure({ languages: [] });
      hljs.initHighlightingOnLoad();
      if (document.readyState && document.readyState === "complete") {
        window.setTimeout(function () { hljs.initHighlighting(); }, 0);
      }
    }
  </script>








</head>

<body>


  <section class="main-content">


    <h1 class="title toc-ignore">Spatial Clustering (2)</h1>
    <h3 class="subtitle">Spatially Constrained Clustering - Hierarchical Methods</h3>
    <h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
    <h4 class="date">11/21/2020 (latest update)</h4>


    <div id="TOC">
      <ul>
        <li><a href="#introduction">Introduction</a>
          <ul>
            <li><a href="#objectives">Objectives</a>
              <ul>
                <li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
              </ul>
            </li>
            <li><a href="#preliminaries">Preliminaries</a></li>
          </ul>
        </li>
        <li><a href="#spatially-constrained-hierarchical-clustering-schc">Spatially Constrained Hierarchical Clustering
            (SCHC)</a>
          <ul>
            <li><a href="#principle">Principle</a></li>
            <li><a href="#implementation">Implementation</a>
              <ul>
                <li><a href="#wards-linkage">Ward’s linkage</a></li>
                <li><a href="#single-linkage">Single linkage</a></li>
                <li><a href="#complete-linkage">Complete linkage</a></li>
                <li><a href="#average-linkage">Average linkage</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#skater">SKATER</a>
          <ul>
            <li><a href="#principle-1">Principle</a></li>
            <li><a href="#implementation-1">Implementation</a>
              <ul>
                <li><a href="#saving-the-minimum-spanning-tree">Saving the minimum spanning tree</a></li>
                <li><a href="#setting-a-minimum-cluster-size">Setting a minimum cluster size</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#redcap">REDCAP</a>
          <ul>
            <li><a href="#principle-2">Principle</a></li>
            <li><a href="#implementation-2">Implementation</a>
              <ul>
                <li><a href="#full-order-wards-linkage">Full-Order Ward’s linkage</a></li>
                <li><a href="#full-order-average-linkage">Full-Order Average linkage</a></li>
                <li><a href="#full-order-complete-linkage">Full-Order Complete linkage</a></li>
                <li><a href="#full-order-single-linkage">Full-Order Single linkage</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#assessment">Assessment</a></li>
        <li><a href="#appendix">Appendix</a>
          <ul>
            <li><a href="#arizona-counties-sample-data-set">Arizona counties sample data set</a></li>
            <li><a href="#schc-complete-linkage-worked-example">SCHC Complete Linkage worked example</a></li>
            <li><a href="#skater-worked-example">SKATER worked example</a></li>
            <li><a href="#redcap-worked-example">REDCAP worked example</a></li>
          </ul>
        </li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <p><br></p>
    <div id="introduction" class="section level2 unnumbered" number="">
      <h2>Introduction</h2>
      <p>We now move our focus to methods that impose contiguity as a <em>hard</em> constraint in a clustering
        procedure.
        Such methods are known under a number of different terms, including <em>zonation</em>, <em>districting</em>,
        <em>regionalization</em>, <em>spatially constrained clustering</em>,
        and the <em>p-region problem</em>. They are concerned with dividing
        an original set of <em>n</em> spatial units
        into <em>p</em> internally connected regions that maximize within similarity
        <span class="citation">(for recent reviews, see, e.g., Murray and Grubesic <a href="#ref-MurrayGrubesic:02"
            role="doc-biblioref">2002</a>; Duque, Ramos, and Suriñach <a href="#ref-Duqueetal:07"
            role="doc-biblioref">2007</a>; Duque, Church, and Middleton <a href="#ref-Duqueatal:11a"
            role="doc-biblioref">2011</a>)</span>.
      </p>
      <p>In the previous chapter, we covered approaches that impose a <em>soft</em> constraint, in the form of a
        trade-off between
        attribute similarity and spatial similarity.
        In the methods considered in the current and next chapter, the contiguity
        is a strict constraint, in that clusters can only consist of entities that are geographically connected.
        As a result, in
        some cases the resulting attribute similarity may be of poor quality, when dissimilar units are grouped together
        primarily due to the contiguity constraint.</p>
      <p>We consider three sets of methods. We start by introducing spatial constraints into an agglomerative
        hierarchical
        clustering procedure, following the approach reviewed in <span class="citation">Murtagh (<a
            href="#ref-Murtagh:85" role="doc-biblioref">1985</a>)</span> and <span class="citation">Gordon (<a
            href="#ref-Gordon:96" role="doc-biblioref">1996</a>)</span>, among others. Next, we outline two common
        algorithms,
        i.e., <em>SKATER</em> <span class="citation">(Assunção et al. <a href="#ref-Assuncaoetal:06"
            role="doc-biblioref">2006</a>)</span> and <em>REDCAP</em> <span class="citation">(Guo <a href="#ref-Guo:08"
            role="doc-biblioref">2008</a>; Guo and Wang <a href="#ref-GuoWang:11" role="doc-biblioref">2011</a>)</span>.
        SKATER stands for
        <em>Spatial `K’luster Analysis by Tree Edge Removal</em>, and obtains regionalization through a graph
        partitioning approach. REDCAP stands for
        <em>REgionalization with Dynamically Constrained Agglomerative clustering and Partitioning</em>, and consists of
        a family
        of six hierarchical regionalization methods.
      </p>
      <p>As before, the methods considered here share many of the same options with previously discussed techniques as
        implemented
        in <code>GeoDa</code>. Common options will not be considered, but the focus will be on aspects that are specific
        to the spatial
        perspective.</p>
      <p>To illustrate these methods, we continue to use the Guerry data set.</p>
      <div id="objectives" class="section level3 unnumbered" number="">
        <h3>Objectives</h3>
        <ul>
          <li>
            <p>Understand how imposing spatial constraints affects hierarchical agglomerative clustering</p>
          </li>
          <li>
            <p>Understand the tree partioning method underlying the SKATER algorithm</p>
          </li>
          <li>
            <p>Identify contiguous clusters by means of the SKATER algorithm</p>
          </li>
          <li>
            <p>Understand the connection between SCHC, SKATER and the REDCAP family of methods</p>
          </li>
          <li>
            <p>Identify contiguous clusters by means of the REDCAP algorithm</p>
          </li>
        </ul>
        <div id="geoda-functions-covered" class="section level4 unnumbered" number="">
          <h4>GeoDa functions covered</h4>
          <ul>
            <li>Clusters &gt; SCHC</li>
            <li>Clusters &gt; skater
              <ul>
                <li>set minimum bound variable</li>
                <li>set minimum cluster size</li>
              </ul>
            </li>
            <li>Clusters &gt; redcap</li>
          </ul>
          <p><br></p>
        </div>
      </div>
      <div id="preliminaries" class="section level3 unnumbered" number="">
        <h3>Preliminaries</h3>
        <p>We continue to use the Guerry data set and also need a spatial weights matrix in the form
          of queen contiguity.</p>
      </div>
    </div>
    <div id="spatially-constrained-hierarchical-clustering-schc" class="section level2 unnumbered" number="">
      <h2>Spatially Constrained Hierarchical Clustering (SCHC)</h2>
      <div id="principle" class="section level3 unnumbered" number="">
        <h3>Principle</h3>
        <p>Spatially constrained hierarchical clustering is a special form of constrained clustering, where the
          constraint is based on contiguity (common borders). We have earlier seen how a minimum size constraint
          can be imposed on classic clustering algorithms. The spatial constraint is more complex in that
          it directly affects the way in which elemental units can be merged into larger entities.</p>
        <p>The idea of including contiguity constraints into agglomerative hierarchical clustering goes back
          a long time, with early overviews of the principles involved in <span class="citation">Lankford (<a
              href="#ref-Lankford:69" role="doc-biblioref">1969</a>)</span>, <span class="citation">Murtagh (<a
              href="#ref-Murtagh:85" role="doc-biblioref">1985</a>)</span> and
          <span class="citation">Gordon (<a href="#ref-Gordon:96" role="doc-biblioref">1996</a>)</span>. Recent software
          implementations can be found in <span class="citation">Guo (<a href="#ref-Guo:09"
              role="doc-biblioref">2009</a>)</span> and <span class="citation">Recchia (<a href="#ref-Recchia:10"
              role="doc-biblioref">2010</a>)</span>.
        </p>
        <p>The clustering logic is identical to that of unconstrained hierarchical clustering, and the same
          expressions are used for linkage and updating formulas, i.e., single linkage, complete linkage,
          average linkage, and Ward’s method (we refer to the relevant chapter for details). The only difference is that
          now a contiguity constraint is
          imposed.</p>
        <p>More specifically, two entities <span class="math inline">\(i\)</span> and <span
            class="math inline">\(j\)</span> are merged when the dissimilarity measure <span
            class="math inline">\(d_{ij}\)</span> is the smallest among
          all pairs of entities, subject to <span class="math inline">\(w_{ij} = 1\)</span> (i.e., a non-zero spatial
          weight). In other words, merger is only carried out
          when the corresponding entry in the spatial weights (contiguity) matrix is non-zero. Another way
          to phrase this is that the minimum of the dissimilarity measure is only searched for those pairs of
          observations
          that are contiguous.</p>
        <p>As
          before, the dissimilarity measure is updated for the newly merged unit using the appropriate formula.
          In addition, the weights matrix needs to be updated to reflect the contiguity structure
          of the newly merged units.</p>
        <p>Consider this more closely. In the first step, the weights matrix is of dimension <span
            class="math inline">\(n \times n\)</span>. If
          observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are merged into
          a new entity, say <span class="math inline">\(A\)</span>, then the resulting matrix will be of
          dimension <span class="math inline">\((n - 1) \times (n-1)\)</span> and the two original rows/columns for
          <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> will be replaced
          by a new row/column for <span class="math inline">\(A\)</span>. For the new matrix, the row elements <span
            class="math inline">\(w_{Ah} = 1\)</span> if either <span class="math inline">\(w_{ih} = 1\)</span>
          <em>or</em> <span class="math inline">\(w_{jh} = 1\)</span>
          (or both are non-zero),
          and similarly for the column elements.</p>
        <p>The next step thus consists of a new dissimilarity matrix and new contiguity matrix, both of
          dimension <span class="math inline">\((n - 1) \times (n-1)\)</span>. At this point the process repeats itself.
          As for other hierarchical
          clustering methods, the end result is a single cluster that contains all the observations. The process
          of merging consecutive entities is graphically represented in an <em>dendrogram</em>, as before. A fully
          worked example is included in the <a href="#appendix">Appendix</a>.</p>
        <p>One potential complication is so-called inversion, when the dissimilarity criterion for the
          newly merged unit with respect to remaining observations is <em>better</em> than for the merged
          units themselves. This link reversal occurs when <span class="math inline">\(d_{i \cup j, k} \lt
            d_{ij}\)</span>. It is only avoided
          in the complete linkage case <span class="citation">(for details, see Murtagh <a href="#ref-Murtagh:85"
              role="doc-biblioref">1985</a>; Gordon <a href="#ref-Gordon:96" role="doc-biblioref">1996</a>)</span>. This
          problem is primarily one
          of interpretation and does not preclude the clustering methods from being applied.</p>
      </div>
      <div id="implementation" class="section level3 unnumbered" number="">
        <h3>Implementation</h3>
        <p>Spatially constrained hierarchical clustering is invoked as the first item in the corresponding group
          in the list associated with the <strong>Clusters</strong> item on the toolbar, as in Figure <a
            href="#fig:schc">1</a>, or from the
          menu, as <strong>Clusters &gt; SCHC</strong>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schc"></span>
          <img src="pics9c/00_schc.png" alt="SCHC cluster option" width="10%" />
          <p class="caption">
            Figure 1: SCHC cluster option
          </p>
        </div>
        <div id="wards-linkage" class="section level4 unnumbered" number="">
          <h4>Ward’s linkage</h4>
          <p>The menu brings up the usual cluster variable interface with the same general structure as for the
            unconstrained
            hierarchical clustering. The variable selection is carried out in the left-hand panel of Figure <a
              href="#fig:schcvars">2</a>.
            We choose the same six variables as before, <strong>Crm_prs</strong>, <strong>Crm_prp</strong>,
            <strong>Litercy</strong>, <strong>Donatns</strong>, <strong>Infants</strong> and <strong>Suicids</strong>.
            We use the variables in z-standardized form. In addition, we need to select a <strong>Spatial
              Weight</strong>, here set to queen
            contiguity in the <strong>guerry_85_q</strong> file. The default methods is <strong>Ward’s-linkage</strong>.
            As before, we also set the variable to which to save the cluster indicator, here <strong>CLw</strong>.</p>
          <div class="figure" style="text-align: center"><span id="fig:schcvars"></span>
            <img src="pics9c/44_schc_variables.png" alt="SCHC variable selection and initial run" width="60%" />
            <p class="caption">
              Figure 2: SCHC variable selection and initial run
            </p>
          </div>
          <p>The algorithm proceeds in two steps. Note that the <strong>Number of Clusters</strong> initially is set to
            2, which
            we change to <strong>6</strong>. Clicking on <strong>Run</strong>
            generates the dendrogram, shown in the right-hand panel of Figure <a href="#fig:schcvars">2</a>. We can move
            the <em>cut</em> line (dashed red line) in the dendrogram to the desired number of clusters if it is
            different from
            the <strong>Number of Clusters</strong> in the interface. At this point, selecting <strong>Save/Show
              Map</strong> generates the cluster map shown in Figure <a href="#fig:schcwardmap">3</a>.</p>
          <p>We notice two well-balanced clusters of 27 departments, one with 21, one smaller one with 8 observations,
            and two singletons. As required, all clusters consist of contiguous spatial units.</p>
          <div class="figure" style="text-align: center"><span id="fig:schcwardmap"></span>
            <img src="pics9c/44_schc_ward_map.png" alt="SCHC Ward method cluster map, k=6" width="60%" />
            <p class="caption">
              Figure 3: SCHC Ward method cluster map, k=6
            </p>
          </div>
          <p>The <strong>Summary</strong> button in the right-hand panel of Figure <a href="#fig:schcvars">2</a> brings
            up the usual summary
            characteristics, shown in Figure <a href="#fig:schcwardsummary">4</a>. The usual descriptors provide the
            cluster centers
            (in the original units), the within-cluster sum of squares for each cluster as well as the overall within
            and
            between cluster sum or squares. The ratio of between to total sum of squares amounts to 0.461812.</p>
          <div class="figure" style="text-align: center"><span id="fig:schcwardsummary"></span>
            <img src="pics9c/44_schc_ward_summary.png" alt="SCHC Ward method cluster characteristics, k=6"
              width="40%" />
            <p class="caption">
              Figure 4: SCHC Ward method cluster characteristics, k=6
            </p>
          </div>
          <p>In order to put this into perspective, we show the results of the unconstrained hierarchical clustering
            routine
            with Ward’s linkage in Figure <a href="#fig:hierwardresults">5</a>. The spatial pattern is quite different,
            with the larger
            clusters from Figure <a href="#fig:schcwardmap">3</a> split into the elements of several smaller clusters.
            There are no singletons,
            and the clusters range from 4 to 25 spatial units. Because the algorithm is unconstrained, the results for
            the
            ratio of between to total sum of squares is much better, at 0.532476. The difference with the constrained
            version
            gives a measure of the price to pay in order to achieve contiguity.</p>
          <div class="figure" style="text-align: center"><span id="fig:hierwardresults"></span>
            <img src="pics9c/00_hierarchical_ward_6.png" alt="Ward method unconstrained hierarchical clustering, k=6"
              width="80%" />
            <p class="caption">
              Figure 5: Ward method unconstrained hierarchical clustering, k=6
            </p>
          </div>
        </div>
        <div id="single-linkage" class="section level4 unnumbered" number="">
          <h4>Single linkage</h4>
          <p>As in the unconstrained case, the Ward linkage methods is the default because it tends to yield compact
            clusters. As we saw earlier, the other linkage tend to result in a lot of unbalanced clusters and yield
            several singletons. The same is the case in the spatially constrained algorithms.</p>
          <p>For completeness sake, we provide the results for the other three linkage methods, single linkage first.
          </p>
          <p>The cluster map and summary characteristics for single linkage are shown in Figure <a
              href="#fig:schcsingle">6</a>. We
            obtain one large cluster with 80 observations and five singletons. The between to total ratio is 0.236071.
            These
            results are pretty dismal relative to the Ward linkage method.</p>
          <div class="figure" style="text-align: center"><span id="fig:schcsingle"></span>
            <img src="pics9c/00_schc_single.png" alt="SCHC single linkage clustering, k=6" width="80%" />
            <p class="caption">
              Figure 6: SCHC single linkage clustering, k=6
            </p>
          </div>
        </div>
        <div id="complete-linkage" class="section level4 unnumbered" number="">
          <h4>Complete linkage</h4>
          <p>As shown in Figure <a href="#fig:schccomplete">7</a>, the results for complete linkage are slightly better,
            with three non-singleton
            clusters and between to total ratio of 0.306119.</p>
          <div class="figure" style="text-align: center"><span id="fig:schccomplete"></span>
            <img src="pics9c/00_schc_complete.png" alt="SCHC complete linkage clustering, k=6" width="80%" />
            <p class="caption">
              Figure 7: SCHC complete linkage clustering, k=6
            </p>
          </div>
        </div>
        <div id="average-linkage" class="section level4 unnumbered" number="">
          <h4>Average linkage</h4>
          <p>Finally, average linkage, shown in Figure <a href="#fig:schcavg">8</a>, again yields five singleton
            clusters, with
            a low between to total ratio of 0.246467, only slightly better than single linkage.</p>
          <div class="figure" style="text-align: center"><span id="fig:schcavg"></span>
            <img src="pics9c/00_schc_average.png" alt="SCHC average linkage clustering, k=6" width="80%" />
            <p class="caption">
              Figure 8: SCHC average linkage clustering, k=6
            </p>
          </div>
        </div>
      </div>
    </div>
    <div id="skater" class="section level2 unnumbered" number="">
      <h2>SKATER</h2>
      <div id="principle-1" class="section level3 unnumbered" number="">
        <h3>Principle</h3>
        <p>The SKATER algorithm introduced
          by <span class="citation">Assunção et al. (<a href="#ref-Assuncaoetal:06"
              role="doc-biblioref">2006</a>)</span> is based on the optimal pruning of a minimum spanning tree that
          reflects
          the contiguity structure among the observations.<a href="#fn2" class="footnote-ref"
            id="fnref2"><sup>2</sup></a></p>
        <p>The point of departure is a dissimilarity matrix that only contains weights for contiguous observations.
          In other words, we consider the <em>distance</em> <span class="math inline">\(d_{ij}\)</span> between <span
            class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, but only for those pairs
          where <span class="math inline">\(w_{ij} = 1\)</span> (contiguous). This matrix is represented as a graph with
          the observations as nodes
          and the contiguity relations as edges.</p>
        <p>The full graph is reduced to a minimum spanning tree (MST), i.e., such that there is a path
          that connects all observations (nodes),
          but each is only visited once. In other words, the n nodes are connected by n-1 edges, such that the overall
          between-node dissimilarity is minimized. This yields a starting sum of squared deviations or SSD as <span
            class="math inline">\(\sum_i (x_i - \bar{x})^2\)</span>,
          where <span class="math inline">\(\bar{x}\)</span> is the overall mean.</p>
        <p>The objective is to reduce the overall SSD by maximizing the between SSD, or, alternatively, minimizing the
          sum of within SSD. The MST is <em>pruned</em> by selecting the edge whose removal increases the objective
          function (between group dissimilarity) the most. To accomplish this, each potential split is evaluated in
          terms of its contribution
          to the objective function.</p>
        <p>More precisely, for each tree T, we consider <span class="math inline">\(\mbox{SSD}_T - (\mbox{SSD}_a +
            \mbox{SSD}_b)\)</span>, where
          <span class="math inline">\(\mbox{SSD}_a, \mbox{SSD}_b\)</span> are the contributions of each subtree. The
          contribution is computed by first
          calculating the average for that subtree and then obtaining the sum of squared deviations.<a href="#fn3"
            class="footnote-ref" id="fnref3"><sup>3</sup></a> We select the cut in the subtree where the difference
          <span class="math inline">\(\mbox{SSD}_T - (\mbox{SSD}_a + \mbox{SSD}_b)\)</span> is the largest.<a
            href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>
        </p>
        <p>At this point, we repeat the process for the new set of subtrees to select an optimal cut. We continue
          until we have reached the desired number of clusters (k). A fully worked out example is given in
          the <a href="#appendix">Appendix</a>.</p>
        <p>Like SCHC, this is a hierarchical clustering method, but here the approach is divisive instead of
          agglomerative. In other words, it starts with a single cluster, and finds the optimal split into
          subclusters until the value of k is satisfied. Because of this hierarchical nature,
          once the tree is cut at one point, all subsequent cuts are limited to the resulting subtrees.
          In other words, once an observation ends up in a pruned branch of the tree, it cannot switch
          back to a previous branch. This is sometimes viewed as a limitation of this algorithm.</p>
        <p>In addition, the contiguity constraint is based on the original configurations and does not take
          into account new neighbor relations that follow from the combination of different observations into
          clusters, as was the case for SCHC.</p>
      </div>
      <div id="implementation-1" class="section level3 unnumbered" number="">
        <h3>Implementation</h3>
        <p>In <code>GeoDa</code>, the SKATER algorithm is invoked as the second item in the hierarchical group on the
          <strong>Clusters</strong> toolbar icon
          (Figure <a href="#fig:schc">1</a>), or from the
          main menu as <strong>Clusters &gt; skater</strong>.</p>
        <p>Selecting this option brings up the <strong>Skater Settings</strong> dialog, shown in Figure <a
            href="#fig:skatervars">9</a>.
          This interface has essentially the
          same structure as in all other clustering approaches.
          The panel provides a way to select the variables, the number of clusters and different
          options to determine the clusters. As for SCHC, we again have
          a <strong>Weights</strong> drop down list, where the contiguity weights must be specified. The SKATER
          algorithm does not work without a spatial weights file.</p>
        <p>The <strong>Distance Function</strong> and <strong>Transformation</strong> options work in the same manner as
          for classic clustering. At the bottom of the dialog, we specify the <strong>Field</strong> or
          variable name where the classification will be saved.
          We consider the <strong>Minimum Bound</strong> and <strong>Min Region Size</strong> options below.</p>
        <p>We continue with the Guerry example data set and select the same six variables as before:
          <strong>Crm_prs</strong>, <strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>,
          <strong>Infants</strong>,
          and <strong>Suicids</strong>. The spatial weights are based on queen contiguity
          (<strong>guerry_85_q</strong>). We initially
          set the number of clusters to <strong>4</strong>.
        </p>
        <div class="figure" style="text-align: center"><span id="fig:skatervars"></span>
          <img src="pics9c/33_skatervars.png" alt="SKATER cluster settings" width="30%" />
          <p class="caption">
            Figure 9: SKATER cluster settings
          </p>
        </div>
        <p>Clicking on <strong>Run</strong> brings up the SKATER cluster map as a separate window and
          lists the cluster characteristics in the <strong>Summary</strong> panel. We only give the map for now, as
          Figure <a href="#fig:skatermap4">10</a>, and consider the summary for k=6 below.</p>
        <div class="figure" style="text-align: center"><span id="fig:skatermap4"></span>
          <img src="pics9c/33_skater4_map.png" alt="SKATER cluster map (k=4)" width="60%" />
          <p class="caption">
            Figure 10: SKATER cluster map (k=4)
          </p>
        </div>
        <p>The spatial clusters generated by SKATER follow the minimum spanning tree closely and tend
          to reflect the hierarchical nature of the pruning. In this example, the country initially gets divided
          into three largely horizontal regions, of which the top one gets split into an eastern and a
          western part.</p>
        <p>The hierarchical nature of the SKATER algorithm can be further illustrated by increasing the
          number of clusters to <strong>6</strong> for easier comparison with SCHC (all other options remain the same).
          The resulting cluster map in Figure <a href="#fig:skatermap6">11</a> shows
          how the previous regions 3 and 4 each get split into an eastern and a western part.</p>
        <div class="figure" style="text-align: center"><span id="fig:skatermap6"></span>
          <img src="pics9c/33_skater6_map.png" alt="SKATER cluster map (k=6)" width="60%" />
          <p class="caption">
            Figure 11: SKATER cluster map (k=6)
          </p>
        </div>
        <p>The summary in Figure <a href="#fig:skater6summary">12</a> reveals a ratio of between sum or squares to total
          sum of squares of about 0.420. This is inferior
          to the result we obtained for SCHC for Ward’s method (about 0.462), but better than for the other linkage
          functions.</p>
        <div class="figure" style="text-align: center"><span id="fig:skater6summary"></span>
          <img src="pics9c/33_skater6_summary.png" alt="SKATER cluster summary (k=6)" width="40%" />
          <p class="caption">
            Figure 12: SKATER cluster summary (k=6)
          </p>
        </div>
        <div id="saving-the-minimum-spanning-tree" class="section level4 unnumbered" number="">
          <h4>Saving the minimum spanning tree</h4>
          <p>Near the bottom of the settings dialog is an option to save the minimum spanning tree. As
            shown in Figure <a href="#fig:mstoptions">13</a>, either the complete MST can be saved, with the
            corresponding box checked, or the MST that corresponds to the final result (box unchecked,
            the default).</p>
          <div class="figure" style="text-align: center"><span id="fig:mstoptions"></span>
            <img src="pics9c/33_save_spanning_tree.png" alt="Minimum spanning tree options" width="35%" />
            <p class="caption">
              Figure 13: Minimum spanning tree options
            </p>
          </div>
          <p>The MST is saved as a <strong>gwt</strong> weights file and is loaded into the <strong>Weights
              Manager</strong> upon creation.
            This allows the graph to be displayed using the <strong>Connectivity &gt; Show Graph</strong> option in the
            cluster map
            (or any map). In Figure <a href="#fig:skatermst">14</a>, the complete MST is shown.<a href="#fn5"
              class="footnote-ref" id="fnref5"><sup>5</sup></a> We clearly see the edges that correspond
            with the transition between clusters.</p>
          <div class="figure" style="text-align: center"><span id="fig:skatermst"></span>
            <img src="pics9c/33_skater_mst_all.png" alt="Complete minimum spanning tree" width="60%" />
            <p class="caption">
              Figure 14: Complete minimum spanning tree
            </p>
          </div>
          <p>In Figure <a href="#fig:skatermst6">15</a>, these edges are removed and the MST corresponding with the
            final solution
            (for k=6) is displayed. This illustrates both the hierarchical nature of the algorithm as well as
            how the result follows from pruning the MST.</p>
          <div class="figure" style="text-align: center"><span id="fig:skatermst6"></span>
            <img src="pics9c/33_skater_mst_6.png" alt="Minimum spanning tree for k=6" width="60%" />
            <p class="caption">
              Figure 15: Minimum spanning tree for k=6
            </p>
          </div>
          <p>Since the MST is added to the <strong>Weights Manager</strong> as the last file, it becomes the default for
            future
            spatial analyses. This is typically not what one wants, so the active weight needs to be reset to a proper
            spatial weights definition before proceeding further.</p>
        </div>
        <div id="setting-a-minimum-cluster-size" class="section level4 unnumbered" number="">
          <h4>Setting a minimum cluster size</h4>
          <p>In several applications of spatially constrained clustering, a minimum cluster size needs to
            be taken into account. For example, this is the case when the new regional groupings are intended
            to be used in a computation of rates. In those instances, the denominator should be sufficiently large to
            avoid extreme variance instability, which is accomplished by setting a minimum population size.</p>
          <p>In <code>GeoDa</code>, there are two options to implement this additional constraint. One is through
            the <strong>Minimum Bound</strong> settings, as shown in Figure <a href="#fig:skaterminbd">16</a>. The check
            box activates a drop down list of
            variables where a <em>spatialy extensive</em> variable can be selected for the minimum bound
            constraint, such as the population (or, in other examples, total number of households, housing
            units, etc.). In our example, we select the departmental population, <strong>Pop1831</strong>. The default
            is to take <strong>10%</strong> of the total over all observations as the minimum bound. This can be
            adjusted
            by means of a slider bar (to set the percentage), or by typing in a different value in the minimum
            bound box. Here, we take the default.</p>
          <div class="figure" style="text-align: center"><span id="fig:skaterminbd"></span>
            <img src="pics9c/33_skater_mb.png" alt="SKATER minimum bound settings" width="30%" />
            <p class="caption">
              Figure 16: SKATER minimum bound settings
            </p>
          </div>
          <p>The resulting spatial alignment of clusters is quite different from the unconstrained
            solution, but again can be seen to be the result of a hierarchical subdivision of the three
            large horizontal subregions. As shown in the left panel of Figure <a href="#fig:skaterminbdmap">17</a>, the
            northernmost region is divided into three,
            and the southernmost one into two subregions. Compared to the unconstrained solution, where
            the clusters consisted of 29, 28, 11, 8, 5, and 4 departments, the constrained regions have
            a much more even distribution of 21, 17, 16, and 12 (twice) and 7 elements.</p>
          <div class="figure" style="text-align: center"><span id="fig:skaterminbdmap"></span>
            <img src="pics9c/33_skater_minbnd.png" alt="SKATER cluster map, min pop 10% (k=6)" width="80%" />
            <p class="caption">
              Figure 17: SKATER cluster map, min pop 10% (k=6)
            </p>
          </div>
          <p>The effect of the constraint is to lower the objective function from 0.420 to 0.376. The
            within-cluster sum of squares of the six regions is much more evenly distributed than in
            the unconstrained solution, due to the similar number of elements in each cluster.</p>
          <p>The second way to constrain the regionalization process is to specify a minimum number of
            units that each cluster should contain. Again, this is a way to ensure that all clusters have
            somewhat similar size, although it is not based on a substantive variable, only on the
            number of elemental units. The constraint is set in the <strong>Min Region Size</strong> box of the
            parameters
            panel in Figure <a href="#fig:skaterminbd">16</a>. In our example, we set the value to <strong>12</strong>.
          </p>
          <p>The result is slighlty different from that generated by the minimum population constraint.
            As shown in Figure <a href="#fig:skaterminsize">18</a>, we now have the three smallest clusters with size 12
            (as mandated by the constraint). The ratio of between to total sum of squares is slightly
            inferior at 0.374.</p>
          <div class="figure" style="text-align: center"><span id="fig:skaterminsize"></span>
            <img src="pics9c/33_skater_minsize.png" alt="SKATER cluster map, min size 12 (k=6)" width="80%" />
            <p class="caption">
              Figure 18: SKATER cluster map, min size 12 (k=6)
            </p>
          </div>
          <p>Note that the minimum region size setting will override the number of clusters when it is set
            too high. For example, setting the minimum cluster size to 14 will only yield clusters with k=5.
            Again, this is a consequence of the hierarchical nature of the minimum spanning tree pruning used
            in the skater algorithm. More specifically, a given sub-tree may not have sufficient nodes to
            allow for further subdivisions that meet the minimum size requirement. A similar problem
            occurs when the mininum population size is set too high.</p>
        </div>
      </div>
    </div>
    <div id="redcap" class="section level2 unnumbered" number="">
      <h2>REDCAP</h2>
      <div id="principle-2" class="section level3 unnumbered" number="">
        <h3>Principle</h3>
        <p>Whereas SCHC involves an agglomerative hierarchical approach and SKATER takes a divisive perspective, the
          REDCAP collection of methods suggested by <span class="citation">Guo (<a href="#ref-Guo:08"
              role="doc-biblioref">2008</a>)</span> combines the two ideas <span class="citation">(see also Guo and Wang
            <a href="#ref-GuoWang:11" role="doc-biblioref">2011</a>)</span>. In this,
          a distinction is made between the linkage update function (originally, single, complete and average linkage),
          and between
          the treatment of contiguity.</p>
        <p>As <span class="citation">Guo (<a href="#ref-Guo:08" role="doc-biblioref">2008</a>)</span> points out, the
          MST that is at the basis of the SKATER algorithm only
          takes into account first order contiguity among pairs of observations. Unlike what we saw in SCHC, the
          contiguity relations are not updated to consider the newly formed clusters. As a result of this,
          observations that are part of a cluster that borders on a given spatial unit are not considered to be
          neighbors of that unit unless they are also first order contiguous. The distinction between a fixed contiguity
          relation and an updated spatial weights matrix is called <em>FirstOrder</em> and <em>FullOrder</em>. As a
          result, there are six
          possible methods, combining the three linkage functions with the two views of contiguity. In later work,
          Ward’s linkage was implemented as well.</p>
        <p>We already saw earlier in the discussion of density-based clustering how the single-linkage dendrogram
          corresponds with a minimum spanning tree (MST). As a result,
          REDCAP’s <em>FirstOrder-SingleLinkage</em> and SKATER are identical. Since the <em>FirstOrder</em> methods are
          generally inferior to the
          <em>FullOrder</em> approaches, the latter are the main focus in <code>GeoDa</code> (for comparison to SKATER,
          <em>FirstOrder-SingleLinkage</em> is
          included as well, but the other <em>FirstOrder</em> methods are not).
        </p>
        <p>A careful consideration of the various REDCAP algorithms reveals that they essentially consist of three
          steps.
          First, a dendrogram for contiguity constrained hierarchical clustering is constructed, using the given linkage
          function. This yields the exact same dendrogram as produced by SCHC. Next, this dendrogram is turned into a
          spanning tree,
          using standard graph manipulation principles. Finally, the optimal cuts in the spanning tree are obtained
          using the
          same logic (and computations) as in SKATER, up to the desired level of k.</p>
      </div>
      <div id="implementation-2" class="section level3 unnumbered" number="">
        <h3>Implementation</h3>
        <p>The collection of REDCAP algorithms is invoked as the third item in the hierarchical group on the
          <strong>Clusters</strong> toolbar icon
          (Figure <a href="#fig:schc">1</a>), or from the
          main menu as <strong>Clusters &gt; redcap</strong>.
        </p>
        <p>Selecting this option brings up the <strong>REDCAP Settings</strong> dialog, shown in Figure <a
            href="#fig:redcapvars">19</a>.
          This interface has the same structure as for SKATER, except that multiple methods are available.
          The panel provides a way to select the variables, the number of clusters and different
          options to determine the clusters. As for the previous methods, we again have
          a <strong>Weights</strong> drop down list, where the contiguity weights must be specified. The REDCAP
          algorithms do not work without a spatial weights file.</p>
        <p>The <strong>Distance Function</strong> and <strong>Transformation</strong> options operate in the same manner
          as
          before. At the bottom of the dialog, we specify the <strong>Field</strong> or
          variable name where the classification will be saved. The
          <strong>Minimum Bound</strong> and <strong>Min Region Size</strong> options work in the same way as for SKATER
          and will not
          be considered separately here.
        </p>
        <p>We continue with the Guerry example data set and select the same six variables as before:
          <strong>Crm_prs</strong>, <strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>,
          <strong>Infants</strong>,
          and <strong>Suicids</strong>. The spatial weights are based on queen contiguity (<strong>guerry_85_q</strong>)
          and the
          the number of clusters is set to <strong>6</strong>.
        </p>
        <div class="figure" style="text-align: center"><span id="fig:redcapvars"></span>
          <img src="pics9c/55_redcap_vars.png" alt="REDCAP cluster settings" width="30%" />
          <p class="caption">
            Figure 19: REDCAP cluster settings
          </p>
        </div>
        <div id="full-order-wards-linkage" class="section level4 unnumbered" number="">
          <h4>Full-Order Ward’s linkage</h4>
          <p>We select the default method as <strong>FullOrder-WardLinkage</strong>, which implements Ward’s linkage
            with dynamically
            updated spatial weights. This yields the cluster map shown in the left-hand panel of Figure <a
              href="#fig:redcapw">20</a>.</p>
          <p>The results differ only marginally from the cluster map for SCHC using Ward’s linkage, shown in Figure <a
              href="#fig:schcwardmap">3</a>. One department moves from the cluster containing eight units to the cluster
            in the south with 27 units,
            making the latter now the largest with 28 departments.</p>
          <p>The cluster summary statistics, shown in the right-hand panel of Figure <a href="#fig:redcapw">20</a> are
            essentially the
            same as for the SCHC case, but slightly worse, with a between to total SS ratio of 0.460.</p>
          <div class="figure" style="text-align: center"><span id="fig:redcapw"></span>
            <img src="pics9c/55_redcap_ward.png" alt="REDCAP Full Order, Ward's linkage, k=6" width="80%" />
            <p class="caption">
              Figure 20: REDCAP Full Order, Ward’s linkage, k=6
            </p>
          </div>
          <p>As for SKATER, we can save the spanning tree associated with the linkage method, shown in Figure <a
              href="#fig:redcapspanning">21</a>.
            There are some general similarities with the SKATER MST (Figure <a href="#fig:skatermst">14</a>), but a
            close comparison
            reveals several important differences as well.</p>
          <div class="figure" style="text-align: center"><span id="fig:redcapspanning"></span>
            <img src="pics9c/55_redcapw_spanning.png" alt="Spanning tree for REDCAP Full Order Ward's linkage"
              width="60%" />
            <p class="caption">
              Figure 21: Spanning tree for REDCAP Full Order Ward’s linkage
            </p>
          </div>
          <p>The options for minimum size work in the same way as for SKATER and are not further considered.</p>
          <p>For the sake of completeness, we show the results for the three other FullOrder linkage methods as well.
            <code>GeoDa</code> also includes <strong>FirstOrder-SingleLinkage</strong>, but the results are identical to
            those of SKATER and
            are not further reported here.
          </p>
        </div>
        <div id="full-order-average-linkage" class="section level4 unnumbered" number="">
          <h4>Full-Order Average linkage</h4>
          <p>The cluster map and summary characteristics of Full-Order Average linkage are shown in Figure <a
              href="#fig:redcapavg">22</a>.
            Compared to Ward’s results, the clusters are more balanced and there are no longer any singletons.
            The dominance of the large northern and southern clusters remains, but the departments in the middle of the
            country are arranged in different clusters. The ratio of between to total sum of squares is 0.429, somewhat
            worse than for Ward’s.</p>
          <div class="figure" style="text-align: center"><span id="fig:redcapavg"></span>
            <img src="pics9c/55_redcap_avg.png" alt="REDCAP Full Order, average linkage, k=6" width="80%" />
            <p class="caption">
              Figure 22: REDCAP Full Order, average linkage, k=6
            </p>
          </div>
        </div>
        <div id="full-order-complete-linkage" class="section level4 unnumbered" number="">
          <h4>Full-Order Complete linkage</h4>
          <p>The result for Full-Order Complete linkage is shown in Figure <a href="#fig:redcapcomp">23</a>. We again
            have two singletons, one
            of which is in the same (northern) location as for Ward’s linkage, but the other one is not. As before, the
            main
            differences are for the departments in the middle of the country. The ratio of between to total sum of
            squares
            is slightly better than for average linkage, at 0.435.</p>
          <div class="figure" style="text-align: center"><span id="fig:redcapcomp"></span>
            <img src="pics9c/55_redcap_comp.png" alt="REDCAP Full Order, complete linkage, k=6" width="80%" />
            <p class="caption">
              Figure 23: REDCAP Full Order, complete linkage, k=6
            </p>
          </div>
        </div>
        <div id="full-order-single-linkage" class="section level4 unnumbered" number="">
          <h4>Full-Order Single linkage</h4>
          <p>Finally, the results for Full-Order Single linkage are the worst. As shown in Figure <a
              href="#fig:redcapsingle">24</a>, the clusters
            share a singleton with Ward’s linkage and another cluster with average linkage, but otherwise the spatial
            pattern
            is quite distinct from the previous cases. The ratio of between to total sum of squares is the worst of the
            four options, at 0.388.</p>
          <div class="figure" style="text-align: center"><span id="fig:redcapsingle"></span>
            <img src="pics9c/55_redcap_single.png" alt="REDCAP Full Order, single linkage, k=6" width="80%" />
            <p class="caption">
              Figure 24: REDCAP Full Order, single linkage, k=6
            </p>
          </div>
        </div>
      </div>
    </div>
    <div id="assessment" class="section level2 unnumbered" number="">
      <h2>Assessment</h2>
      <p>Clearly, different assumptions and different algorithms yield greatly varying results. This may be
        discomforting, but
        it is important to keep in mind that each of these approaches have a slightly different way to handle the
        tension
        between attribute and locational similarity.</p>
      <p>In the end, the results can be evaluated on a number of different criteria. <code>GeoDa</code> reports the
        ratio of the between
        sum of squares to the total sum of squares, but that is only one of a number of possible metrics, such as
        compactness,
        balance, etc. Also, the commonalities and differences between the various approaches highlight where the
        tradeoffs
        are particularly critical.</p>
      <p>Finally, it should be kept in mind that the solutions offered by the different algorithms have no guarantee to
        yield
        <em>global</em> optima. Therefore, it is important to consider more than one method, in order to gain insight
        into the
        sensitivity of the results to the approach chosen.
      </p>
    </div>
    <div id="appendix" class="section level2 unnumbered" number="">
      <h2>Appendix</h2>
      <div id="arizona-counties-sample-data-set" class="section level3 unnumbered" number="">
        <h3>Arizona counties sample data set</h3>
        <p>We will use a small enough data set to illustrate the various spatially constrained algorithms.
          As it turns out, the state of Arizona has only 14 counties, which is perfect for our purposes.
          We create this data set by starting with the <strong>natregimes</strong> data set and selecting the counties
          with <strong>STFIPS</strong> equal to 4, which are the counties in Arizona. If we then use <strong>Save
            Selected As</strong>
          we obtain the data set shown in Figure <a href="#fig:schcaz">25</a>. We created a new variable
          <strong>AZID</strong> which
          gives an id to the counties in alphabetical order. This identifier is shown in Figure <a
            href="#fig:schcaz">25</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz"></span>
          <img src="pics9c/11_azcounties.png" alt="Arizona counties" width="50%" />
          <p class="caption">
            Figure 25: Arizona counties
          </p>
        </div>
        <p>To keep things simple, we will only use a single variable, the unemployment rate in 1990,
          <strong>UE90</strong>. We standardize
          this variable (using the <strong>Calculator</strong>) as <strong>SUE</strong> and show the associated values
          in Figure <a href="#fig:schcazvars">26</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcazvars"></span>
          <img src="pics9c/11_azvariables.png" alt="County identifiers and standardized variable" width="20%" />
          <p class="caption">
            Figure 26: County identifiers and standardized variable
          </p>
        </div>
        <p>The final piece to complete our analysis is a spatial weight matrix. In Figure <a
            href="#fig:schcazgal">27</a> we show
          the contents of the queen contiguity gal file.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcazgal"></span>
          <img src="pics9c/11_az_gal.png" alt="AZ county queen contiguity information" width="20%" />
          <p class="caption">
            Figure 27: AZ county queen contiguity information
          </p>
        </div>
      </div>
      <div id="schc-complete-linkage-worked-example" class="section level3 unnumbered" number="">
        <h3>SCHC Complete Linkage worked example</h3>
        <p>We illustrate the spatially constrained hierarchical clustering using complete linkage. Even though this is
          not
          an ideal linkage selection, it is easy to implement and helps to illustrate the logic of the SCHC.</p>
        <p>The point of departure is the matrix of Euclidean distance in attribute space. In our example, there is
          only one variable, so the distance is equivalent to the absolute difference between the values at two
          locations.
          In Figure <a href="#fig:schcaz1">28</a>, the full distance matrix is shown. Distinct from the example in the
          unconstrained case, we limit our attention to those pairs of observations that are spatially contiguous,
          shown highlighted in red in the matrix.</p>
        <p>The first step is to identify the pair of observations that are closest in attribute space and
          contiguous (red in the table). Inspection
          of the distance matrix in Figure <a href="#fig:schcaz1">28</a> finds the pair 8-14 as the least different,
          with a
          distance value of 0.03.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz1"></span>
          <img src="pics9c/11_az_step1.png" alt="SCHC complete linkage step 1" width="65%" />
          <p class="caption">
            Figure 28: SCHC complete linkage step 1
          </p>
        </div>
        <p>The next step is to combine observations 8 and 14 and recompute the distance from other observations
          as the <em>largest</em> from either 8 or 14. The updated matrix is shown in Figure <a
            href="#fig:schcaz2">29</a>. In
          addition, the contiguity matrix must be updated with neigbors to the cluster 8-14 as those who
          were either neighbors to 8 or 14, or to both. The updated neighbor relation is shown as the red
          values in the matrix. The smallest value is between 9 and 8-14, which yields a new cluster of 8-9-14.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz2"></span>
          <img src="pics9c/11_az_step2.png" alt="SCHC complete linkage step 2" width="60%" />
          <p class="caption">
            Figure 29: SCHC complete linkage step 2
          </p>
        </div>
        <p>The remaining steps proceed in the same manner. The contiguity relations are updated and the largest distance
          between the two elements of the cluster is entered as the new distance in the matrix. In step 3,
          shown in Figure <a href="#fig:schcaz3">30</a>, we identify the pair with
          the smallest value of 0.12, between 2 and 13, which is our new cluster.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz3"></span>
          <img src="pics9c/11_az_step3.png" alt="SCHC complete linkage step 3" width="55%" />
          <p class="caption">
            Figure 30: SCHC complete linkage step 3
          </p>
        </div>
        <p>With the updated contiguities, step 4 identifies the pair 4-12 as the new cluster, with a distance of 0.26,
          as in Figure <a href="#fig:schcaz4">31</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz4"></span>
          <img src="pics9c/11_az_step4.png" alt="SCHC complete linkage step 4" width="50%" />
          <p class="caption">
            Figure 31: SCHC complete linkage step 4
          </p>
        </div>
        <p>We continue in the same way with step 5, identifying a new cluster of 3-4-12 with a distance of 0.29.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz5"></span>
          <img src="pics9c/11_az_step5.png" alt="SCHC complete linkage step 5" width="45%" />
          <p class="caption">
            Figure 32: SCHC complete linkage step 5
          </p>
        </div>
        <p>In step 6, we add 11 to the existing cluster of 8-9-14, with a distance of 0.34, as shown in Figure <a
            href="#fig:schcaz6">33</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz6"></span>
          <img src="pics9c/11_az_step6.png" alt="SCHC complete linkage step 6" width="40%" />
          <p class="caption">
            Figure 33: SCHC complete linkage step 6
          </p>
        </div>
        <p>Next, we add 6 to 2-13, with a distance of 0.37, as in Figure <a href="#fig:schcaz">25</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz7"></span>
          <img src="pics9c/11_az_step7.png" alt="SCHC complete linkage step 7" width="35%" />
          <p class="caption">
            Figure 34: SCHC complete linkage step 7
          </p>
        </div>
        <p>Now, 5 joins 3-4-12, with a distance of 0.77, in Figure <a href="#fig:schcaz8">35</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz8"></span>
          <img src="pics9c/11_az_step8.png" alt="SCHC complete linkage step 8" width="30%" />
          <p class="caption">
            Figure 35: SCHC complete linkage step 8
          </p>
        </div>
        <p>The final few steps continue to reduce the number of clusters. In this stage, the two clusters
          3-4-5-12 and 2-6-13 are joined, with a distance of 0.85, as in Figure <a href="#fig:schcaz9">36</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz9"></span>
          <img src="pics9c/11_az_step9.png" alt="SCHC complete linkage step 9" width="25%" />
          <p class="caption">
            Figure 36: SCHC complete linkage step 9
          </p>
        </div>
        <p>Next, 7 is added to 8-9-11-14, with a distance of 1.04, in Figure <a href="#fig:schcaz10">37</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz10"></span>
          <img src="pics9c/11_az_step10.png" alt="SCHC complete linkage step 10" width="25%" />
          <p class="caption">
            Figure 37: SCHC complete linkage step 10
          </p>
        </div>
        <p>At this point, 1 and 10 are joined, with a distance of 1.38, shown in Figure <a href="#fig:schcaz11">38</a>.
        </p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz11"></span>
          <img src="pics9c/11_az_step11.png" alt="SCHC complete linkage step 11" width="25%" />
          <p class="caption">
            Figure 38: SCHC complete linkage step 11
          </p>
        </div>
        <p>The next to final step, resulting in two clusters, merges 3-4-5-12-2-6-13 with 7-8-9-11-14, with
          a distance of 1.42, as in Figure <a href="#fig:schcaz12">39</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcaz12"></span>
          <img src="pics9c/11_az_step12.png" alt="SCHC complete linkage step 12" width="20%" />
          <p class="caption">
            Figure 39: SCHC complete linkage step 12
          </p>
        </div>
        <p>One interesting aspect of these steps is that at each stage, the distance involved is larger.
          As mentioned above, the complete linkage approach avoids the inversion problem, so that we indeed
          see a uniform increase in the distances involved.</p>
        <p>We can also visualize the process by the dendrogram generate by <code>GeoDa</code> for this example, shown
          in Figure <a href="#fig:schcazdendro4">40</a>. We see from the distance associated with each pair how the
          first merger is between 8 and 14, followed by adding 9, etc. The steps in the dendrogram identically follow
          the steps illustrated above.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcazdendro4"></span>
          <img src="pics9c/11_az_k4_dendro.png" alt="SCHC complete linkage dendrogram, k=4" width="40%" />
          <p class="caption">
            Figure 40: SCHC complete linkage dendrogram, k=4
          </p>
        </div>
        <p>Finally, we show the spatial layout of the clusters for k=4 in Figure <a href="#fig:schcazmap4">41</a>.
          Clearly,
          the contiguity constraint has been satisfied.</p>
        <div class="figure" style="text-align: center"><span id="fig:schcazmap4"></span>
          <img src="pics9c/44_az_k4.png" alt="SCHC complete linkage cluster map, k=4" width="60%" />
          <p class="caption">
            Figure 41: SCHC complete linkage cluster map, k=4
          </p>
        </div>
      </div>
      <div id="skater-worked-example" class="section level3 unnumbered" number="">
        <h3>SKATER worked example</h3>
        <p>We illustrate the SKATER algorithm with the same Arizona county example, with k (the number of clusters) set
          to 4. In Figure <a href="#fig:skaterv">42</a>, we list the
          observations and their squares, which form the basis for the computation of the relevant sums of squared
          deviations
          (SSD) for each subset.</p>
        <div class="figure" style="text-align: center"><span id="fig:skaterv"></span>
          <img src="pics9c/22_skater_vars.png" alt="AZ county variables" width="20%" />
          <p class="caption">
            Figure 42: AZ county variables
          </p>
        </div>
        <p>The first step in the process is to reduce the information for contiguous pairs in the distance matrix of
          Figure <a href="#fig:schcaz1">28</a> (shown in red) to
          a minimum spanning tree (MST). The result is shown in Figure <a href="#fig:mstfull">43</a>, against the
          backdrop of the Arizona
          county map.</p>
        <div class="figure" style="text-align: center"><span id="fig:mstfull"></span>
          <img src="pics9c/22_az_mst.png" alt="SKATER minimum spanning tree" width="40%" />
          <p class="caption">
            Figure 43: SKATER minimum spanning tree
          </p>
        </div>
        <p>We now have to evaluate every possible cut in the MST in terms of its contribution to reducing the overall
          sum of
          squared deviations (SSD). We know the data in third column of Figure <a href="#fig:skaterv">42</a> is
          standardized, so the
          mean is zero by construction. As a result, the total sum of squared deviations is the sum of squares, i.e.,
          the
          sum of the values listed in the fourth column. This sum is 13.000 (rounded).<a href="#fn6"
            class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
        <p>The vertices for each possible cut are shown as the first two columns in Figure <a
            href="#fig:skater1">44</a>. For each
          subtree, we need to compute the corresponding SSD. The next step then implements the optimal cut such that the
          total SSD decreases the most, i.e., max[<span class="math inline">\(\mbox{SSD}_T - (\mbox{SSD}_a +
            \mbox{SSD}_b)\)</span>], where <span class="math inline">\(\mbox{SSD}_T\)</span> is the SSD for the
          corresponding
          tree, and <span class="math inline">\(\mbox{SSD}_a\)</span> and <span
            class="math inline">\(\mbox{SSD}_b\)</span> are the totals for the subtrees corresponding to the cut.</p>
        <p>In order to accomplish this, we first compute the mean of the values associated
          with the vertices of each subtree, and then calculate the SSD for each subtree k as <span
            class="math inline">\(\sum_i x_i^2 - n_k \bar{x}_k^2\)</span>,
          with <span class="math inline">\(\bar{x}_k\)</span> as the mean for the average value for the subtree and
          <span class="math inline">\(n_k\)</span> as the number of elements in the subtree.</p>
        <p>In the interest of space, we do not show all the calculations, but give a few examples. For the cut
          1-10, the vertex 1 becomes a singleton, so its SSD is 0 (<span class="math inline">\(SSD_a\)</span> in the
          fourth column and first row in Figure <a href="#fig:skater1">44</a>).
          For vertex 10, the subtree consists of all 13 elements other than 1, with a mean of -0.211 and an SSD of 4.894
          (
          <span class="math inline">\(SSD_b\)</span> in the fifth column
          and first row in Figure <a href="#fig:skater1">44</a>). Consequently, the contribution to reducing the overall
          SSD amounts to
          13.000 - (0 + 4.894) = 8.106.
        </p>
        <p>In a similar fashion, we compute the SSD for each possible subtree and enter the values in Figure <a
            href="#fig:skater1">44</a>.
          The largest decrease in overall SSD is achieved by the cut between 5 and 10, which yields a reduction of
          9.820.</p>
        <div class="figure" style="text-align: center"><span id="fig:skater1"></span>
          <img src="pics9c/22_skater_step1.png" alt="SKATER step 1" width="30%" />
          <p class="caption">
            Figure 44: SKATER step 1
          </p>
        </div>
        <p>The new MST after cutting the link between 5 and 10 is shown in Figure <a href="#fig:mstsstep1">45</a>,
          yielding two initial
          clusters.</p>
        <div class="figure" style="text-align: center"><span id="fig:mstsstep1"></span>
          <img src="pics9c/22_az_skater_2.png" alt="SKATER minimum spanning tree - first split" width="40%" />
          <p class="caption">
            Figure 45: SKATER minimum spanning tree - first split
          </p>
        </div>
        <p>We now repeat the process, looking for the greatest decrease in overall SSD. We separate the data into two
          subtrees,
          one for 1-10 and the other for the remaining vertices. In each, we recalculate the SSD for the subtree, which
          yields
          0.958 for 1-10 and 2.222 for the other cluster. For each possible cut, we compute the SSD for the
          corresponding subtrees,
          with the results shown in Figure <a href="#fig:skater2">46</a>. At this point, the greatest contribution
          towards reducing the
          overall SSD is offered by a cut between 8 and 11, with a contribution of 1.441.</p>
        <div class="figure" style="text-align: center"><span id="fig:skater2"></span>
          <img src="pics9c/22_skater_step2.png" alt="SKATER step 2" width="30%" />
          <p class="caption">
            Figure 46: SKATER step 2
          </p>
        </div>
        <p>The corresponding MST for the three clusters is shown in Figure <a href="#fig:mstsstep2">47</a>. We now have
          one cluster of
          two units, one with three, and one with nine.</p>
        <div class="figure" style="text-align: center"><span id="fig:mstsstep2"></span>
          <img src="pics9c/22_az_skater_3.png" alt="SKATER minimum spanning tree - second split" width="40%" />
          <p class="caption">
            Figure 47: SKATER minimum spanning tree - second split
          </p>
        </div>
        <p>At this point, we only need to make one more cut (k=4). When we compute the SSD for each subtree, we find a
          total of 0.0009 for 8-9-14, and
          a value of 0.779 for the remainder, both well below the contribution to decreasing the SSD
          of 0.958 obtained by 1-10 (since the split yields two single observations, the decrease in SSD equals the
          total
          SSD for the subtree). Consequently, we do not need to carry out the detailed calculations for each other
          subtree, since the final cut is between 1 and 10, yielding two singletons. As a result, we have four clusters,
          with the final MST shown in Figure <a href="#fig:mstsstep3">48</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:mstsstep3"></span>
          <img src="pics9c/22_az_skater_4.png" alt="SKATER minimum spanning tree - third split" width="40%" />
          <p class="caption">
            Figure 48: SKATER minimum spanning tree - third split
          </p>
        </div>
        <p>The matching cluster map created by <code>GeoDa</code> for SKATER with k=4 is shown in Figure <a
            href="#fig:skateraz">49</a>.
          The four regions match the subtrees in the MST, with 1 and 10 being singletons.</p>
        <div class="figure" style="text-align: center"><span id="fig:skateraz"></span>
          <img src="pics9c/44_skater_cluster.png" alt="SKATER cluster map, k=4" width="60%" />
          <p class="caption">
            Figure 49: SKATER cluster map, k=4
          </p>
        </div>
      </div>
      <div id="redcap-worked-example" class="section level3 unnumbered" number="">
        <h3>REDCAP worked example</h3>
        <p>We now use the Arizona counties example to illustrate the logic of the REDCAP
          <em>FullOrder-CompleteLinkage</em> option. As it turns out, we can reuse the results from SCHC and the logic
          from SKATER.</p>
        <p>The first stage in the REDCAP algorithm is to construct a <em>spanning tree</em> that corresponds to a
          spatially constrained
          complete linkage hierarchical clustering dendrogram.</p>
        <p>In our illustration, we implement <em>FullOrder</em>, i.e., the contiguity
          relations between clusters is dynamically updated. Consequently, the steps to follow are the same as for
          complete linkage SCHC, yielding
          the dendrogram in Figure <a href="#fig:schcazdendro4">40</a>.</p>
        <p>To create a spanning tree representation of this dendrogram, we connect the observations following the merger
          of entities in the dendrogram. In our example, this means we first connect 8 and 14. In the second step, we
          connect
          9 to the cluster 8-14, but since 9 is only contiguous to 14, we make the edge 9-14. The full sequence of edges
          is
          given in the list below. Whenever we connect a new entity to a cluster, we either connect it to the only
          contiguous
          unit, or to the contiguous one that is closest (using the distance measures in Figure <a
            href="#fig:schcaz1">28</a> ):</p>
        <ul>
          <li>Step 1: 8-14</li>
          <li>Step 2: 9-14</li>
          <li>Step 3: 2-13</li>
          <li>Step 4: 4-12</li>
          <li>Step 5: 3-4</li>
          <li>Step 6: 11-8</li>
          <li>Step 7: 6-2</li>
          <li>Step 8: 5-4</li>
          <li>Step 9: 2-5</li>
          <li>Step 10: 7-11</li>
          <li>Step 11: 1-10</li>
          <li>Step 12: 11-12</li>
          <li>Step 13: 10-5</li>
        </ul>
        <p><br></p>
        <p>The resulting spanning tree is shown in Figure <a href="#fig:redcaptree">50</a>, with the sequence of steps
          marked in blue.
          The tree is largely the same as the MST in Figure <a href="#fig:mstfull">43</a>, except that the link between
          11 and 2 is
          replaced by a link between 5 and 2.</p>
        <div class="figure" style="text-align: center"><span id="fig:redcaptree"></span>
          <img src="pics9c/44_redcap_az_mst.png" alt="REDCAP FullOrder-CompleteLinkage spanning tree" width="40%" />
          <p class="caption">
            Figure 50: REDCAP FullOrder-CompleteLinkage spanning tree
          </p>
        </div>
        <p>At this point, we search for an optimal cut using the same approach as for SKATER.
          Given the similarity
          of the two trees, we will be able to reuse a lot of the previous results. Only those subtrees affected by the
          new
          link 5-2 replacing the link 11-2 need to be considered again. Specifically, in the first step, these are
          the edges 5-2, 5-4, 4-12 and 11-12. The other results can be borrowed from the first step in SKATER, shown
          in Figure <a href="#fig:skater1">44</a>. The new results are given in Figure <a href="#fig:redcap1">51</a>. As
          in the SKATER case,
          the first cut is between 5 and 10.</p>
        <div class="figure" style="text-align: center"><span id="fig:redcap1"></span>
          <img src="pics9c/44_redcap_step1.png" alt="REDCAP step 1" width="30%" />
          <p class="caption">
            Figure 51: REDCAP step 1
          </p>
        </div>
        <p>We proceed in the same manner for the second step, where again we can borrow several results directly from
          the SKATER case. Figure <a href="#fig:redcap2">52</a> lists the updated values. Here again, the optimal cut is
          between 8 and
          11.</p>
        <div class="figure" style="text-align: center"><span id="fig:redcap2"></span>
          <img src="pics9c/44_redcap_step2.png" alt="REDCAP step 2" width="30%" />
          <p class="caption">
            Figure 52: REDCAP step 2
          </p>
        </div>
        <p>The final step is the same as for SKATER, yielding the same layout as in Figure <a
            href="#fig:skateraz">49</a> for
          our four clusters.</p>
        <p><br></p>
      </div>
    </div>
    <div id="references" class="section level2 unnumbered" number="">
      <h2>References</h2>
      <div id="refs" class="references hanging-indent">
        <div id="ref-Assuncaoetal:06">
          <p>Assunção, Renato M., M. C. Neves, G. Câmara, and C. Da Costa Freitas. 2006. “Efficient Regionalization
            Techniques for Socio-Economic Geographical Units Using Minimum Spanning Trees.” <em>International Journal of
              Geographical Information Science</em> 20 (7): 797–811.</p>
        </div>
        <div id="ref-Duqueetal:07">
          <p>Duque, Juan Carlos, Raúl Ramos, and Jordi Suriñach. 2007. “Supervised Regionalization Methods: A Survey.”
            <em>International Regional Science Review</em> 30: 195–220.</p>
        </div>
        <div id="ref-Duqueatal:11a">
          <p>Duque, Juan C., Richard L. Church, and Richard S. Middleton. 2011. “The p-Regions Problem.”
            <em>Geographical Analysis</em> 43: 104–26.</p>
        </div>
        <div id="ref-Gordon:96">
          <p>Gordon, A. D. 1996. “A Survey of Constrained Classification.” <em>Computational Statistics and Data
              Analysis</em> 21: 17–29.</p>
        </div>
        <div id="ref-Guo:08">
          <p>Guo, Diansheng. 2008. “Regionalization with Dynamically Constrained Agglomerative Clustering and
            Partitioning (REDCAP).” <em>International Journal of Geographical Information Science</em> 22 (7): 801–23.
          </p>
        </div>
        <div id="ref-Guo:09">
          <p>———. 2009. “Greedy Optimization for Contiguity-Constrained Hierarchical Clustering.” In <em>2013 IEEE 13th
              International Conference on Data Mining Workshops</em>, 591–96. Los Alamitos, CA, USA: IEEE Computer
            Society. <a href="https://doi.org/10.1109/ICDMW.2009.75">https://doi.org/10.1109/ICDMW.2009.75</a>.</p>
        </div>
        <div id="ref-GuoWang:11">
          <p>Guo, Diansheng, and Hu Wang. 2011. “Automatic Region Building for Spatial Analysis.” <em>Transactions in
              GIS</em> 15: 29–45.</p>
        </div>
        <div id="ref-Lankford:69">
          <p>Lankford, Philip M. 1969. “Regionalization: Theory and Alternative Algorithms.” <em>Geographical
              Analysis</em> 1: 196–212.</p>
        </div>
        <div id="ref-MurrayGrubesic:02">
          <p>Murray, Alan T., and Tony H. Grubesic. 2002. “Identifying Non-Hierarchical Spatial Clusters.”
            <em>International Journal of Industrial Engineering</em> 9: 86–95.</p>
        </div>
        <div id="ref-Murtagh:85">
          <p>Murtagh, Fionn. 1985. “A Survey of Algorithms for Contiguity-Constrained Clustering and Related Problems.”
            <em>The Computer Journal</em> 28: 82–88.</p>
        </div>
        <div id="ref-Recchia:10">
          <p>Recchia, Anthony. 2010. “Contiguity-Constrained Hierarchical Agglomerative Clustering Using SAS.”
            <em>Journal of Statistical Software</em> 22 (2).</p>
        </div>
      </div>
    </div>
    <div class="footnotes">
      <hr />
      <ol>
        <li id="fn1">
          <p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu"
              class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn2">
          <p>This <em>skater</em> algorithm is not to be confused with tools to interpret the results of deep learning
            <a href="https://github.com/oracle/Skater" class="uri">https://github.com/oracle/Skater</a>.<a
              href="#fnref2" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn3">
          <p>This can readily
            be computed as <span class="math inline">\(\sum_i x_i^2 - n_T \bar{x}_T^2\)</span>, where <span
              class="math inline">\(\bar{x}_T\)</span> is the average for the subtree, and
            <span class="math inline">\(n_T\)</span> is the number of nodes in the subtree.<a href="#fnref3"
              class="footnote-back">↩︎</a>
          </p>
        </li>
        <li id="fn4">
          <p>While this exhaustive evaluation
            of all potential cuts is inherently slow, it can be speeded up by exploiting certain heuristics, as
            described in <span class="citation">Assunção et al. (<a href="#ref-Assuncaoetal:06"
                role="doc-biblioref">2006</a>)</span>. In <code>GeoDa</code>, we use full enumeration, but implement
            parallelization to obtain
            better performance.<a href="#fnref4" class="footnote-back">↩︎</a></p>
        </li>
        <li id="fn5">
          <p>To obtain this graph, use
            <strong>Change Edge Thickness &gt; Strong</strong> and set the color to yellow.<a href="#fnref5"
              class="footnote-back">↩︎</a>
          </p>
        </li>
        <li id="fn6">
          <p>Since the variable is standardized, the estimated variance <span class="math inline">\(\hat{\sigma}^2 =
              \sum_i x_i^2 / (n - 1) = 1\)</span>. Therefore, <span class="math inline">\(\sum_i x_i^2 = n - 1\)</span>,
            or 13 in our example.<a href="#fnref6" class="footnote-back">↩︎</a></p>
        </li>
      </ol>
    </div>


  </section>


  <!-- code folding -->


  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>

</html>