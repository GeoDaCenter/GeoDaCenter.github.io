<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Luc Anselin" />


<title>Cluster Analysis (3)</title>

<script src="lab7c_files/header-attrs-2.2/header-attrs.js"></script>
<link href="lab7c_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lab7c_files/highlightjs-9.12.0/highlight.js"></script>
    <title>GeoDa on Github</title>

    <style>
    	*{margin:0;padding:0;}
	    .shadowfilter {
	       -webkit-filter: drop-shadow(12px 12px 7px rgba(0,0,0,0.5));
	        filter: url(shadow.svg#drop-shadow);
	     }
	     .intro1 { margin-left: -45px;}
    </style>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
    <style>
    ul {padding-left:30px;}
	figcaption {
	  top: .70em;
   	  left: .35em;
 	  bottom: auto!important;
	  right: auto!important;
	}
    </style>

        <style>
    h1 {
        text-align: center;
    }
    h3.subtitle {
        text-align: center;
    }
    h4.author {
        text-align: center;
    }
    h4.date {
        text-align: center;
    }
    p.caption {
        font-size : 12px;
    }
    </style>

<!-- Google Analytics -->
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-72724100-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<!-- End Google Analytics -->
<!-- Google Tag Manager -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-53RVF8"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-53RVF8');</script>
<!-- End Google Tag Manager -->

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








</head>

<body>


    <section class="page-header">
      <h1 class="project-name">GeoDa</h1>
      <h2 class="project-tagline">An Introduction to Spatial Data Analysis</h2>
      <a href="https://geodacenter.github.io/index.html" class="btn">Homepage</a>
      <a href="https://geodacenter.github.io/download.html" class="btn">Download</a>
      <a href="https://github.com/GeoDaCenter/geoda/" class="btn">View on GitHub</a>
      <a href="https://spatial.uchicago.edu/sample-data"  target="_blank" class="btn">Data</a>
       <a href="https://geodacenter.github.io/documentation.html" class="btn">Documentation</a>
       <a href="https://geodacenter.github.io/support.html" class="btn">Support</a>
       <a href="https://geodacenter.github.io/index-cn.html" class="btn">中文</a>
    </section>

    <section class="main-content">


<h1 class="title toc-ignore">Cluster Analysis (3)</h1>
<h3 class="subtitle">Advanced Clustering Methods</h3>
<h4 class="author">Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h4>
<h4 class="date">06/29/2020 (latest update)</h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#objectives">Objectives</a>
<ul>
<li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
</ul></li>
<li><a href="#getting-started">Getting started</a></li>
</ul></li>
<li><a href="#k-medians">K Medians</a>
<ul>
<li><a href="#principle">Principle</a></li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#variable-settings-panel">Variable Settings Panel</a></li>
<li><a href="#cluster-results">Cluster results</a></li>
</ul></li>
<li><a href="#options-and-sensitivity-analysis">Options and sensitivity analysis</a>
<ul>
<li><a href="#mad-standardization">MAD standardization</a></li>
</ul></li>
</ul></li>
<li><a href="#k-medoids">K Medoids</a>
<ul>
<li><a href="#principle-1">Principle</a>
<ul>
<li><a href="#the-pam-algorithm-for-k-medoids">The PAM algorithm for k-medoids</a></li>
<li><a href="#improving-on-the-pam-algorithm">Improving on the PAM algorithm</a></li>
</ul></li>
<li><a href="#implementation-1">Implementation</a>
<ul>
<li><a href="#variable-settings-panel-1">Variable Settings Panel</a></li>
<li><a href="#cluster-results-1">Cluster results</a></li>
</ul></li>
<li><a href="#options-and-sensitivity-analysis-1">Options and sensitivity analysis</a>
<ul>
<li><a href="#clara">CLARA</a></li>
<li><a href="#clarans">CLARANS</a></li>
<li><a href="#comparison-of-methods-and-initialization-settings">Comparison of methods and initialization settings</a></li>
</ul></li>
</ul></li>
<li><a href="#spectral-clustering">Spectral Clustering</a>
<ul>
<li><a href="#principle-2">Principle</a>
<ul>
<li><a href="#clustering-as-a-graph-partitioning-problem">Clustering as a graph partitioning problem</a></li>
</ul></li>
<li><a href="#the-spectral-clustering-algorithm">The spectral clustering algorithm</a>
<ul>
<li><a href="#creating-an-adjacency-matrix">Creating an adjacency matrix</a></li>
<li><a href="#clustering-on-the-eigenvectors-of-the-graph-laplacian">Clustering on the eigenvectors of the graph Laplacian</a></li>
<li><a href="#spectral-clustering-parameters">Spectral clustering parameters</a></li>
</ul></li>
<li><a href="#implementation-2">Implementation</a>
<ul>
<li><a href="#variable-settings-panel-2">Variable Settings Panel</a></li>
<li><a href="#cluster-results-2">Cluster results</a></li>
</ul></li>
<li><a href="#options-and-sensitivity-analysis-2">Options and sensitivity analysis</a>
<ul>
<li><a href="#k-nearest-neighbors-affinity-matrix">K-nearest neighbors affinity matrix</a></li>
<li><a href="#gaussian-kernel-affinity-matrix">Gaussian kernel affinity matrix</a></li>
</ul></li>
</ul></li>
<li><a href="#appendix">Appendix</a>
<ul>
<li><a href="#worked-example-for-k-medians">Worked example for k-medians</a></li>
<li><a href="#worked-example-for-pam">Worked example for PAM</a>
<ul>
<li><a href="#build">BUILD</a></li>
<li><a href="#swap">SWAP</a></li>
</ul></li>
<li><a href="#the-graph-laplacian">The graph Laplacian</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p><br></p>
<div id="introduction" class="section level2 unnumbered" number="">
<h2>Introduction</h2>
<p>In this chapter, we consider some more advanced partitioning methods. First, we cover two
variants of K-means, i.e., <em>K-medians</em> and <em>K-medoids</em>. These operate in the same
manner as K-means, but differ in the way the central point of each cluster is defined and the
manner in which the <em>nearest</em> points are assigned. In addition,
we discuss <em>spectral clustering</em>, a graph partitioning method that can be interpreted as simultaneously implementing
dimension reduction with cluster identification.</p>
<p>As implemented in <code>GeoDa</code>, these methods share almost all the same options with the partitioning and hierarchical
clustering methods discussed in the previous chapters. These common aspects will not be considered again. We refer to the previous chapters
for details on the common options and sensitivity analyses.</p>
<p>We continue to use the Guerry data set to illustrate k-medians and k-medoids, but introduce a new sample data set,
<strong>spirals.csv</strong>, for the spectral clustering examples.</p>
<div id="objectives" class="section level3 unnumbered" number="">
<h3>Objectives</h3>
<ul>
<li><p>Understand the difference between k-median and k-medoid clustering</p></li>
<li><p>Carry out and interpret the results of k-median clustering</p></li>
<li><p>Gain insight into the logic behind the PAM, CLARA and CLARANS algorithms</p></li>
<li><p>Carry out and interpret the results of k-medoid clustering</p></li>
<li><p>Understand the graph-theoretic principles underlying spectral clustering</p></li>
<li><p>Carry out and interpret the results of spectral clustering</p></li>
</ul>
<div id="geoda-functions-covered" class="section level4 unnumbered" number="">
<h4>GeoDa functions covered</h4>
<ul>
<li>Clusters &gt; K Medians
<ul>
<li>select variables</li>
<li>MAD standardization</li>
</ul></li>
<li>Clusters &gt; K Medoids</li>
<li>Clusters &gt; Spectral</li>
</ul>
<p><br></p>
</div>
</div>
<div id="getting-started" class="section level3 unnumbered" number="">
<h3>Getting started</h3>
<p>The Guerry data set can be loaded in the same way as before.</p>
<p>The <strong>spirals</strong> data set is specifically designed to illustrate some of the special characteristics
of spectral clustering. It is one of the GeoDaCenter sample data sets.</p>
<p>To activate this data set, you load the file <strong>spirals.csv</strong> and select <strong>x</strong> and <strong>y</strong> as the coordinates (the
data set only has two variables), as in Figure <a href="#fig:spiralscsv">1</a>. This will ensure that the resulting layer is
represented as a point map.</p>
<div class="figure" style="text-align: center"><span id="fig:spiralscsv"></span>
<img src="pics7c/00_spirals_csv.png" alt="Spirals convert csv file to point map" width="40%" />
<p class="caption">
Figure 1: Spirals convert csv file to point map
</p>
</div>
<p>The result shows the 300 points, consisting of two distinct but interwoven spirals, as in Figure <a href="#fig:spiralmap">2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:spiralmap"></span>
<img src="pics7c/00_spirals_map.png" alt="Spirals themeless point map" width="60%" />
<p class="caption">
Figure 2: Spirals themeless point map
</p>
</div>
<p>We will not be needing this data set until we cover spectral clustering. For k-medians and k-medoids, we
use the Guerry data set.</p>
</div>
</div>
<div id="k-medians" class="section level2 unnumbered" number="">
<h2>K Medians</h2>
<div id="principle" class="section level3 unnumbered" number="">
<h3>Principle</h3>
<p>K-medians is a variant of k-means clustering. As a partitioning method, it starts
by randomly picking k starting points and assigning observations to the nearest initial
point. After the assignment,
the center for each cluster is re-calculated and the assignment process repeats itself.
In this way, k-medians proceeds in exactly the same manner as k-means. It is in fact also
an EM algorithm.</p>
<p>In contrast to k-means, the central point is not the average
(in multiattribute space), but instead the median of the cluster observations. The median center
is computed separately for each dimension, so it is not necessarily an actual observation
(similar to what is the case for the cluster average in k-means).</p>
<p>The objective function for k-medians is to find the allocation <span class="math inline">\(C(i)\)</span> of observations <span class="math inline">\(i\)</span> to clusters
<span class="math inline">\(h = 1, \dots k\)</span>, such that the sum of the Manhattan distances between the members of
each cluster and the cluster median is minimized:
<span class="math display">\[\mbox{argmin}_{C(i)} \sum_{h=1}^k \sum_{i \in h} || x_i - x_{h_{med}} ||_{L_1},\]</span>
where the distance metric follows the <span class="math inline">\(L_1\)</span> norm, i.e., the Manhattan block distance.</p>
<p>K-medians is often
confused with k-medoids. However, there is an important difference in that in
k-medoids, the central point has to be one of the observations <span class="citation">(Kaufman and Rousseeuw <a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>.
We consider k-medoids in the next section.</p>
<p>The Manhattan distance metric is used to assign observations to the nearest center.
From a theoretical perspective, this is superior to using Euclidean distance since it
is consistent with the notion of a median as the center <span class="citation">(Hoon, Imoto, and Miyano <a href="#ref-deHoonetal:17" role="doc-biblioref">2017</a>, 16)</span>.</p>
<p>In all other respects, the implementation and interpretation is the same as for
k-means. To illustrate the logic, a simple worked example is provided in the <a href="#appendix">Appendix</a>.</p>
<p><code>GeoDa</code> employs the k-medians implementation that is part of the C clustering library
of <span class="citation">Hoon, Imoto, and Miyano (<a href="#ref-deHoonetal:17" role="doc-biblioref">2017</a>)</span>.</p>
</div>
<div id="implementation" class="section level3 unnumbered" number="">
<h3>Implementation</h3>
<p>Just as the previous clustering techniques, k-medians is invoked from the <strong>Clusters</strong> toolbar.
From the menu, it is selected as <strong>Clusters &gt; K Medians</strong>, the second item in the classic clustering subset, as shown
in Figure <a href="#fig:kmedian">3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedian"></span>
<img src="pics7c/1_071_kmedians.png" alt="K Medians Option" width="10%" />
<p class="caption">
Figure 3: K Medians Option
</p>
</div>
<p>This brings up the <strong>K Medians Clustering Settings</strong> dialog, with the <strong>Input</strong> options
in the left-hand side panel, shown in Figure <a href="#fig:kmedianvars">4</a>.</p>
<div id="variable-settings-panel" class="section level4 unnumbered" number="">
<h4>Variable Settings Panel</h4>
<p>The user interface is identical to that for k-means, to which we refer for details. The main difference
is that the <strong>Distance Function</strong> is Manhattan distance. In the example in Figure <a href="#fig:kmedianvars">4</a>,
we again select the same six variables as before, with the <strong>Number of Clusters</strong> set to 5 and all other
options left to the default settings.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedianvars"></span>
<img src="pics7c/1_072_kmedian_vars.png" alt="K Medians variable selection" width="35%" />
<p class="caption">
Figure 4: K Medians variable selection
</p>
</div>
<p>Selecting <strong>Run</strong> brings up the cluster map and fills out the right-hand panel with some
cluster characteristics, listed under <strong>Summary</strong>. The cluster categories are added to the Table
using the variable name specified in the dialog (default is <strong>CL</strong>, in our example we use <strong>CLme1</strong>).</p>
</div>
<div id="cluster-results" class="section level4 unnumbered" number="">
<h4>Cluster results</h4>
<p>The cluster map is shown in Figure <a href="#fig:kmedianmap">5</a>. The three largest clusters
(they are labeled in sequence of their size) are well-balanced, with 24, 20 and 20 observations.
The two others are much smaller, at 12 and 9. Interesting is that the clusters are
also geographically quite compact, except for cluster 4, which consists of four different spatial
subgroups. Cluster 2, in the south of the country, is actually fully contiguous (without imposing
any spatial constraints). This is not the case for k-means.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedianmap"></span>
<img src="pics7c/1_073_kmedian_map.png" alt="K Medians cluster map (k=5)" width="60%" />
<p class="caption">
Figure 5: K Medians cluster map (k=5)
</p>
</div>
<p>While the grouping may seem similar to what we obtained with other methods, this is in fact not the
case. In Figure <a href="#fig:kmeanmed">6</a>, the cluster map for k-means and k-medians are shown next to each other,
with the labels for k-medians adjusted so as the get similar colors for each category. This highlights
some of the important differences between the two methods. First of all, the size of the different “matching”
clusters is not the same, nor is their geographic configuration. Considering the clusters for k-medians (with
their new labels), we see that the largest cluster, with 24 observations, corresponds most closely with cluster
3 for k-means, which had 18 observations.</p>
<p>The closest match between the two results is for cluster 2, with only one mismatch out of 9 observations,
although that cluster is much larger for k-means, with 19 observations.
The worst match is for cluster 5, where only three observations are shared by the two methods for that
cluster (out of 12). For the others, there is about a 3/4 match. In other words, the two methods pick out
different patterns of similarity in the data. There is no “best” method, since each uses a different
objective function. It is up to the analyst to decide which of the objectives
makes most sense, in light of the goals of a particular study.</p>
<div class="figure" style="text-align: center"><span id="fig:kmeanmed"></span>
<img src="pics7c/4_kmeankmed.png" alt="K Means and K Medians compared (k=5)" width="90%" />
<p class="caption">
Figure 6: K Means and K Medians compared (k=5)
</p>
</div>
<p>Further insight into the characteristics of the clusters obtained by the k-medians algorithm are found
in the <strong>Summary</strong> panel on the right side of the settings dialog, shown in Figure <a href="#fig:kmediansummary">7</a>.</p>
<p>The first set of items summarizes the settings for the analysis, such as the method
used, the number of clusters and the various options for initialization, standardization, etc.
Next follow the values for each of the variables associated with the <em>median</em> center of each cluster. These
results are given in the <em>original scale</em> for the variables, whereas the other summary measures depend
on the standardization used. Typically,
the median center values are used to interpret the type of grouping that is obtained. This is not always easy, since one
has to look for systematic combinations of variables with high or low values for the median so as to characterize the cluster.</p>
<p>The third set of items contains the summary statistics, using the squared difference and mean as the criterion,
similar to what is used for k-means. Note that this is only for a general comparison, since this is <em>not</em>
the criterion used in the objective function. So, in a sense, it gives a general impression of how the k-medians
results compare using the standard used for k-means. In our example, we obtain a ratio of between to total
sum of squares of 0.447, compared to 0.497 for k-means (with the default settings). This does not mean
that the k-medians result is <em>worse</em> than that for k-means, but it gives a sense of how it performs
under a different criterion that what it is optimized for.</p>
<p>The final set of summary characteristics are the proper ones for the objective of minimizing the within-cluster Manhattan
distance relative to the cluster median. The total sum of the distances is 372.318. This is the sum of
the distances between all observations and the overall median (using the z-standardized values for
the variables). For k-medians, the objective is to decrease
this value by grouping the observations into clusters with their own medians. The within-cluster total distance
is listed for each cluster. In our results, there is quite a range in these values, going from 15.97 in the
smallest cluster (with only 9 observations) to 70.49 in cluster 3 (with 20 observations). Clusters 1 and 2, that
are larger or equal to the size of cluster 3, have a much better fit. This is also reflected in the average within-cluster
distance results, with the smallest value of 1.77 for C5, followed by 2.61 for C1. Interestingly, the latter has about
double the total distance compared to C4, but its average is better (2.61 compared to 2.84). The averages correct for
the size of the cluster and are thus a good comparative measure of <em>fit</em>.</p>
<p>The total of the within-cluster distances is 250.399, a decrease of 121.9 from the original total. As a fraction
of the original total, the final result is 0.673. When comparing results for different values of k, we would look for
a bend in the elbow plot as this ratio decreases with increasing values of k.</p>
<div class="figure" style="text-align: center"><span id="fig:kmediansummary"></span>
<img src="pics7c/1_074_kmedians_sum.png" alt="K Medians cluster characteristics (k=5)" width="50%" />
<p class="caption">
Figure 7: K Medians cluster characteristics (k=5)
</p>
</div>
</div>
</div>
<div id="options-and-sensitivity-analysis" class="section level3 unnumbered" number="">
<h3>Options and sensitivity analysis</h3>
<p>The variables settings panel contains all the same options as for k-means, except that
initialization is always by randomization, since there is no k-means++ method for k-medians.
One option that is particularly useful in the context of k-medians (and k-medoids) is the
use of a different standardization.</p>
<div id="mad-standardization" class="section level4 unnumbered" number="">
<h4>MAD standardization</h4>
<p>The default z-standardization uses the mean and the variance of the original variables. Both of these are
sensitive to the influence of outliers. Since the use of Manhattan distance and the median center for
clusters in k-medians already reduces the effect of such outliers, it makes sense to also use a
standardization that is less sensitive to those. We considered range standardization in the discussion of k-means. Here, we look at
the mean absolute deviation, or MAD. As usual, this is selected as one of the <strong>Transformation</strong> options, as shown
in Figure <a href="#fig:kmedianmad">8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedianmad"></span>
<img src="pics7c/1_kmedians_mad.png" alt="MAD variable standardization" width="40%" />
<p class="caption">
Figure 8: MAD variable standardization
</p>
</div>
<p>The resulting cluster map and summary characteristics are shown in
Figures <a href="#fig:kmedianmadmap">9</a> and <a href="#fig:kmedianmadsummary">10</a>.</p>
<p>The main effect seems to be on the largest cluster, which grows from 24 to 27 observations, mostly
at the expense of what was the second largest cluster (which goes from 20 to 18 observations).
As a result, none of the clusters are fully contiguous any more.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedianmadmap"></span>
<img src="pics7c/1_kmedian_mad_map.png" alt="K Medians cluster map - MAD standardization (k=5)" width="60%" />
<p class="caption">
Figure 9: K Medians cluster map - MAD standardization (k=5)
</p>
</div>
<p>The distance measures listed in the summary show a different starting point, with a total distance
sum of 490.478, compared to 372.318 for z-standardization (recall that these measures are expressed
in whatever units were used for the standardization). Therefore, the values for the within-cluster
distance and their averages are not directly comparable to those using z-standardization. Only
relative comparisons are warranted.</p>
<p>In the end, the total within-clusters are reduced
to 0.677 of the original total, a slightly worse result than for z-standardization. However, this does
not necessarily mean that z-standardization is superior. The choice of a particular transformation should
be made within the context of the substantive research question. When no strong guidelines exist, a
sensitivity analysis comparing, for example, z-standardization, range standardization and MAD may be
the best strategy.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedianmadsummary"></span>
<img src="pics7c/1_kmedian_mad_summary.png" alt="K Medians cluster characteristics - MAD standardization (k=5)" width="50%" />
<p class="caption">
Figure 10: K Medians cluster characteristics - MAD standardization (k=5)
</p>
</div>
</div>
</div>
</div>
<div id="k-medoids" class="section level2 unnumbered" number="">
<h2>K Medoids</h2>
<div id="principle-1" class="section level3 unnumbered" number="">
<h3>Principle</h3>
<p>The objective of the k-medoids algorithm is to minimize the sum of the distances
from the observations in each cluster to a <em>representative center</em> for that cluster.
In contrast to k-means and k-medians, those centers do not need to be computed,
since they are actual observations. As a consequence, k-medoids works with any
dissimilarity matrix. If actual observations are available (as in the implementation
in <code>GeoDa</code>), the Manhattan distance is the preferred metric, since
it is less affected by outliers. In addition, since the objective function is
based on the sum of distances instead of their squares, the influence of
outliers is even smaller.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>The objective function can thus be expressed as finding the cluster assignments <span class="math inline">\(C(i)\)</span> such that:
<span class="math display">\[\mbox{argmin}_{C(i)} \sum_{h=1}^k \sum_{i \in h} d_{i,h_c},\]</span>
where <span class="math inline">\(h_c\)</span> is a representative center for cluster <span class="math inline">\(h\)</span> and <span class="math inline">\(d\)</span> is the distance metric used
(from a dissimilarity matrix). As was the case for k-means (and k-medians), the problem is NP hard and
an exact solution does not exist.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>The main approach to the k-medoids problem is the so-called <em>partitioning around medoids</em> (PAM) algorithm
of <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>.
The logic underlying the PAM algorithm consists of two stages, <em>BUILD</em> and <em>SWAP</em>. In the first, a set of <span class="math inline">\(k\)</span> starting centers are selected
from the <span class="math inline">\(n\)</span> observations. In some implementations, this is a random selection, but <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>, and,
more recently <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span> prefer
a step-wise procedure that optimizes the initial set. The main part of the algorithm
proceeds in a greedy iterative manner by swapping a current center with a candidate from the
remaining non-centers, as long as the objective function can be improved. Detailed descriptions are given
in <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span>, Chapters 2 and 3, as well as in <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-Hastieetal:09" role="doc-biblioref">2009</a>)</span>, pp. 515-520, and <span class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12" role="doc-biblioref">2012</a>)</span>, pp. 454-457. A brief
outline is presented next.</p>
<div id="the-pam-algorithm-for-k-medoids" class="section level4 unnumbered" number="">
<h4>The PAM algorithm for k-medoids</h4>
<p>The <em>BUILD</em> phase of the algorithm consists of identifying <span class="math inline">\(k\)</span> observations
out of the <span class="math inline">\(n\)</span> and assigning them to be cluster centers <span class="math inline">\(h\)</span>, with <span class="math inline">\(h = 1, \dots, k\)</span>.
This can be accomplished by randomly selecting the starting points a number of times
and picking the one with the best (lowest) value for the objective function, i.e., the
lowest sum of distances from observations to their cluster centers.</p>
<p>As a preferred option, <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span> outline a step-wise approach that starts by picking the center, say <span class="math inline">\(h_1\)</span>,
that minimizes the overall sum. This is readily accomplished by taking the observation that
corresponds with the smallest row or column sum of the dissimilarity matrix. Next, each additional center
(for <span class="math inline">\(h = 2, \dots, k\)</span>) is selected that maximizes the difference between the closest distance to existing
centers and the new potential center for all points (in practice, for the points that are closer to the
new center than to existing centers).</p>
<p>For example, given the distance to <span class="math inline">\(h_1\)</span>, we compute for each of the
remaining <span class="math inline">\(n - 1\)</span> points <span class="math inline">\(j\)</span>, its distance to all candidate centers <span class="math inline">\(i\)</span> (i.e., the same <span class="math inline">\(n-1\)</span> points). Consider
a <span class="math inline">\((n-1) \times (n-1)\)</span> matrix where each observation is both row (<span class="math inline">\(i\)</span>) and column (<span class="math inline">\(j\)</span>). For each column <span class="math inline">\(j\)</span>,
we compare the distance to the row-element <span class="math inline">\(i\)</span> with the distance to the nearest current center for <span class="math inline">\(j\)</span>. In the
second iteration, this is simply the distance to <span class="math inline">\(h_1\)</span>, but at later iterations different <span class="math inline">\(j\)</span> will have different
centers closest to them. If <span class="math inline">\(i\)</span> is closer to <span class="math inline">\(j\)</span> than its current closest center, then <span class="math inline">\(d_{j,h_1} - d_{ji} &gt; 0\)</span>. The
maximum of this difference and 0 is entered in position <span class="math inline">\(i,j\)</span> of the matrix (in other words, negatives are not counted). The row <span class="math inline">\(i\)</span> with the
largest row sum (i.e., the largest improvement in the objective function) is selected as the next center.</p>
<p>At this point, the distance for each <span class="math inline">\(j\)</span> to its nearest center is updated and the process starts anew for <span class="math inline">\(n-2\)</span> observations.
This continues until <span class="math inline">\(k\)</span> centers have been picked.</p>
<p>In the <em>SWAP</em> phase,
we consider all possible pairs that consist of a
cluster center <span class="math inline">\(i\)</span> and one of the <span class="math inline">\(n - k\)</span> non-centers, <span class="math inline">\(r\)</span>, for a possible swap, for a total
of <span class="math inline">\(k \times (n-k)\)</span> pairs <span class="math inline">\((i, r)\)</span>.</p>
<p>We proceed by evaluating the change to the objective function that would follow from removing center <span class="math inline">\(i\)</span> and
replacing it with <span class="math inline">\(r\)</span>, as it affects the allocation of each other point (non-center and non-candidate) to either
the new center <span class="math inline">\(r\)</span> or one of the current centers <span class="math inline">\(g\)</span> (but not <span class="math inline">\(i\)</span>, since that is no longer a center). This
contribution follows from the change in distance that may occur between <span class="math inline">\(j\)</span> and its new center. Those values
are summed over all <span class="math inline">\(n - k - 1\)</span> points <span class="math inline">\(j\)</span>.
We compute this sum for each pair <span class="math inline">\(i, r\)</span> and find the minimum over all pairs. If this minimum
is negative (i.e., it decreases the total sum of distances), then <span class="math inline">\(i\)</span> and <span class="math inline">\(r\)</span> are swapped. This continues until
there are no more improvements, i.e., the minimum is positive.</p>
<p>We label the change in the objective from <span class="math inline">\(j\)</span> from a swap between <span class="math inline">\(i\)</span> and <span class="math inline">\(r\)</span> as <span class="math inline">\(C_{jir}\)</span>. The
total improvement for a given pair <span class="math inline">\(i, r\)</span> is the sum over all <span class="math inline">\(j\)</span>, <span class="math inline">\(T_{ir} = \sum_j C_{jir}\)</span>. The pair <span class="math inline">\(i, r\)</span> is selected for
a swap for which the minimum over all pairs of <span class="math inline">\(T_{ir}\)</span> is negative, i.e., <span class="math inline">\(\mbox{argmin}_{i,r} T_{ir} &lt; 0\)</span>.</p>
<p>The computational burden associated with this algorithm is quite high, since at each iteration <span class="math inline">\(k \times (n - k)\)</span> pairs need to be evaluated. On the other hand, no calculations other than comparison and addition/subtraction
are involved, and all the information is in the (constant) dissimilarity matrix.</p>
<p>To compute the net change in the objective function due to <span class="math inline">\(j\)</span> that follows from a swap between <span class="math inline">\(i\)</span> and <span class="math inline">\(r\)</span>, we distinguish
between two cases. In one, <span class="math inline">\(j\)</span> belongs to the cluster <span class="math inline">\(i\)</span>, such that <span class="math inline">\(d_{ji} &lt; d_{jg}\)</span> for all other
centers <span class="math inline">\(g\)</span>. In the other case, <span class="math inline">\(j\)</span> belongs to a different cluster, say <span class="math inline">\(g\)</span>, and <span class="math inline">\(d_{jg} &lt; d_{ji}\)</span>. In both instances,
we have to compare the distances from the nearest current center (<span class="math inline">\(i\)</span> or <span class="math inline">\(g\)</span>) to the distance to the candidate point, <span class="math inline">\(r\)</span>.
Note that we don’t actually have to carry out the cluster assignments, since we compare the distance for all <span class="math inline">\(n - k - 1\)</span>
points <span class="math inline">\(j\)</span> to the closest center (<span class="math inline">\(i\)</span> or <span class="math inline">\(g\)</span>) and the candidate center <span class="math inline">\(r\)</span>. All this information is contained
in the elements of the dissimilarity matrix.</p>
<p>Consider the first case, where <span class="math inline">\(j\)</span> is not part of the cluster <span class="math inline">\(i\)</span>, as in Figure <a href="#fig:pam2">11</a>. We see two scenarios for the
configuration of the point <span class="math inline">\(j\)</span>, labeled <span class="math inline">\(j1\)</span> and <span class="math inline">\(j2\)</span>. These points are closer to <span class="math inline">\(g\)</span> than to <span class="math inline">\(i\)</span>, since they are <em>not</em> part
of the cluster around <span class="math inline">\(i\)</span>. We now need to check whether <span class="math inline">\(j\)</span> is closer
to <span class="math inline">\(r\)</span> than to its current cluster center <span class="math inline">\(g\)</span>. If <span class="math inline">\(d_{jg} \leq d_{jr}\)</span>, then nothing changes and <span class="math inline">\(C_{jir} = 0\)</span>. This is the case for point
<span class="math inline">\(j1\)</span>. The dashed red line gives the distance to the current center <span class="math inline">\(g\)</span> and the dashed green line gives the distance to <span class="math inline">\(r\)</span>. Otherwise, if <span class="math inline">\(d_{jr} &lt; d_{jg}\)</span>, as is the case for point <span class="math inline">\(j2\)</span>,
then <span class="math inline">\(j\)</span> is assigned to <span class="math inline">\(r\)</span> and <span class="math inline">\(C_{jir} = d_{jr} - d_{jg}\)</span>, a negative value, which decreases the overall cost. In the figure, we can compare the length of the dashed red line to the length of the solid green line, which designates a re-assignment to the candidate center <span class="math inline">\(r\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:pam2"></span>
<img src="pics7c/7_pam2.png" alt="PAM SWAP - case 1" width="40%" />
<p class="caption">
Figure 11: PAM SWAP - case 1
</p>
</div>
<p>When <span class="math inline">\(j\)</span> is part of cluster <span class="math inline">\(i\)</span>, then we need to assess whether <span class="math inline">\(j\)</span> would be assigned to <span class="math inline">\(r\)</span> or to the next closest center,
say <span class="math inline">\(g\)</span>, since <span class="math inline">\(i\)</span> would no longer be part of the cluster centers. This is illustrated in Figure <a href="#fig:pam1">12</a>, which has three
options for the location of <span class="math inline">\(j\)</span> relative to <span class="math inline">\(g\)</span> and <span class="math inline">\(r\)</span>. In the first case, illustrated by point <span class="math inline">\(j1\)</span>, <span class="math inline">\(j\)</span> is closer to <span class="math inline">\(g\)</span> than to <span class="math inline">\(r\)</span>. This is illustrated by the difference in length between the dashed green line (<span class="math inline">\(d_{j1r}\)</span>) and the solid green line (<span class="math inline">\(d_{j1g}\)</span>). More
precisely, <span class="math inline">\(d_{jr} \geq d_{jg}\)</span> so that <span class="math inline">\(j\)</span> is now
assigned to <span class="math inline">\(g\)</span>. The change in the objective is
<span class="math inline">\(C_{jir} = d_{jg} - d_{ji}\)</span>. This value is positive, since <span class="math inline">\(j\)</span> was part of cluster <span class="math inline">\(i\)</span> and thus was closer to <span class="math inline">\(i\)</span> than to <span class="math inline">\(g\)</span>
(compare the length of the red dashed line between <span class="math inline">\(j1\)</span> and <span class="math inline">\(i\)</span> and the length of the line connecting <span class="math inline">\(j1\)</span> to <span class="math inline">\(g\)</span>).</p>
<p>If <span class="math inline">\(j\)</span> is closer to <span class="math inline">\(r\)</span>, i.e., <span class="math inline">\(d_{jr} &lt; d_{jg}\)</span>, then we can distinguish between two cases, one depicted by <span class="math inline">\(j2\)</span>, the other
by <span class="math inline">\(j3\)</span>. In both instances, the result is that
<span class="math inline">\(j\)</span> is
assigned to <span class="math inline">\(r\)</span>, but the effect on the objective differs. In the Figure, for both <span class="math inline">\(j2\)</span> and <span class="math inline">\(j3\)</span> the dashed green line to <span class="math inline">\(g\)</span> is
longer than the solid green line to <span class="math inline">\(r\)</span>. The change in the objective
is the difference between the new distance and the old one (<span class="math inline">\(d_{ji}\)</span>), or
<span class="math inline">\(C_{jir} = d_{jr} - d_{ji}\)</span>. This value could be either positive or negative, since what matters is that <span class="math inline">\(j\)</span> is closer to
<span class="math inline">\(r\)</span> than to <span class="math inline">\(g\)</span>, irrespective of how close <span class="math inline">\(j\)</span> might have been to <span class="math inline">\(i\)</span>. For point <span class="math inline">\(j2\)</span>, the distance to <span class="math inline">\(i\)</span> (dashed red line)
was smaller than the new distance to <span class="math inline">\(r\)</span> (solid green line), so <span class="math inline">\(d_{jr} - d_{ji} &gt; 0\)</span>. In the case of <span class="math inline">\(j3\)</span>, the opposite
holds, and the length to <span class="math inline">\(i\)</span> (dashed red line) is larger than the distance to the new center (solid green line). In this
case, the change to the objective is <span class="math inline">\(d_{jr} - d_{ji} &lt; 0\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:pam1"></span>
<img src="pics7c/7_pam1.png" alt="PAM SWAP - case 2" width="40%" />
<p class="caption">
Figure 12: PAM SWAP - case 2
</p>
</div>
<p>After the value for <span class="math inline">\(C_{jir}\)</span> is computed for all <span class="math inline">\(j\)</span>, the sum <span class="math inline">\(T_{ir}\)</span> is evaluated. This is repeated for every
possible pair <span class="math inline">\(i,r\)</span> (i.e., <span class="math inline">\(k\)</span> centers to be replaced by <span class="math inline">\(n-k\)</span> candidate centers). If the minimum over all pairs is negative, then <span class="math inline">\(i\)</span> and <span class="math inline">\(r\)</span> for the selected pair are
exchanged, and the process is repeated. If the minimum is positive, the iterations end.</p>
<p>An illustrative worked example is given in the <a href="#appendix">Appendix</a>.</p>
</div>
<div id="improving-on-the-pam-algorithm" class="section level4 unnumbered" number="">
<h4>Improving on the PAM algorithm</h4>
<p>The complexity of each iteration in the original PAM algorithm is of the order <span class="math inline">\(k \times (n - k)^2\)</span>, which means
it will not scale well to large data sets with potentially large values of <span class="math inline">\(k\)</span>. To address this issue,
<span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span> proposed the algorithm CLARA, based on a sampling strategy.</p>
<p>Instead of considering the
full data set, a subsample is drawn. Then PAM is applied to find the
best <span class="math inline">\(k\)</span> medoids in the sample. Next, the distance from all observations (not just those in the sample) to their closest medoid is computed
to assess the overall quality of the clustering.</p>
<p>The sampling process can be repeated for several more samples (keeping the best solution
from the previous iteration as part of the sampled observations), and at the end the best solution is selected. While
easy to implement, this approach does not guarantee that the best local optimum solution is found. In fact, if one of the
best medoids is never sampled, it is impossible for it to become part of the final solution. Note that as the
sample size is increased, the results will tend to be closer to those given by PAM.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>In practical applications, <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span> suggest to use
a sample size of 40 + 2k and to repeat the process 5 times.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>In <span class="citation">Ng and Han (<a href="#ref-NgHan:02" role="doc-biblioref">2002</a>)</span>, a different sampling strategy is outlined that keeps the full set of observations
under consideration. The problem is formulated as finding the best node in a graph that consists
of all possible combinations of <span class="math inline">\(k\)</span> observations that could serve as the <span class="math inline">\(k\)</span> medoids. The nodes
are connected by edges to the <span class="math inline">\(k \times (n - k)\)</span> nodes that differ in one medoid (i.e., for each
edge, one of the <span class="math inline">\(k\)</span> medoid nodes is swapped with one of the <span class="math inline">\(n - k\)</span> candidates).</p>
<p>The algorithm CLARANS starts an iteration by randomly picking a node (i.e., a set of <span class="math inline">\(k\)</span> candidate medoids). Then, it
randomly picks a neighbor of this node in the graph. This is a set of <span class="math inline">\(k\)</span> medoids where one is swapped
with the current set. If this leads to an improvement in the cost, then the new node becomes the new
start of the next set of searches (still part of the same iteration). If not, another neighbor is picked and evaluated, up to <em>maxneighbor</em>
times. This ends an iteration.</p>
<p>At the end of the iteration the cost of the last solution is compared to the stored current <em>best</em>. If the
new solution constitutes an improvement, it becomes the new <em>best</em>. This search process
is carried out a total of <em>numlocal</em> iterations and at the end the best overall solution is kept. Because of the special
nature of the graph, not that many steps are required to achieve a local minimum (technically, there are many
paths that lead to the local minimum, even when starting at a random node).</p>
<p>To make this concept more concrete, consider the toy example used in the Appendix, which has 7 observations.
To construct k=2 clusters, any pair of 2 observations from the 7 could be considered a potential medoid. All
those pairs constitute the nodes in the graph. The total number of nodes is given by the binomial coefficient
<span class="math inline">\({n}\choose{k}\)</span>. In our example, <span class="math inline">\({{7}\choose{2}} = 21\)</span>.</p>
<p>Each of the 21 nodes in the graph has <span class="math inline">\(k \times (n - k) = 2 \times 5 = 10\)</span> <em>neighbors</em> that differ only
in one medoid connected with an edge.
In our example, let’s say we pick (4,7) as a starting
node, as we did in the worked example. It will be connected to all the nodes that differ by one medoid, i.e., either 4 or 7 is replaced (<em>swapped</em> in
PAM terminology) by one of the <span class="math inline">\(n - k = 5\)</span> remaining nodes. Specifically, this includes the following
10 neighbors: 1-7, 2-7, 3-7, 5-7, 6-7, and 4-1, 4-2, 4-3, 4-5 and 4-6. Rather than evaluating all 10 potential swaps,
as we did for PAM, only a maximum number (<em>maxneighbor</em>) are evaluated. At the end of those evaluations, the best
solution is kept. Then the process is repeated, up to the specified total number of iterations, which <span class="citation">Ng and Han (<a href="#ref-NgHan:02" role="doc-biblioref">2002</a>)</span> call
<em>numlocal</em>.</p>
<p>Let’s say we set <em>maxneighbors</em> to 2. Consider the first step of the random evaluation in which we <em>randomly</em> pick the pair 4-5.
Using the values from the worked example in the Appendix, we have <span class="math inline">\(T_{45} = 3\)</span>, so this does not improve the
objective function. We increase the iteration count (for <em>maxneighbors</em>) and pick a second random node, say 4-2.
Now the value <span class="math inline">\(T_{42} = -3\)</span> so the objective is improved to 20-3 = 17. Since we have reached the end of <em>maxneighbors</em>,
we store this value as <em>best</em> and now repeat the process, picking a different random starting point. We continue this
until we have obtained <em>numlocal</em> local optima and keep the best overall solution.</p>
<p>Based on their numerical experiments, <span class="citation">Ng and Han (<a href="#ref-NgHan:02" role="doc-biblioref">2002</a>)</span> suggest that no more than 2 iterations need to be pursued (i.e., <em>numlocal</em> = 2),
with some evidence that more operations are not cost-effective. They also suggest a sample size of 1.25% of <span class="math inline">\(k \times (n-k)\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>Both CLARA and CLARANS are large data methods, since for smaller data sizes (say &lt; 100), PAM will be feasible and obtain
better solutions (since it implements an exhaustive evaluation).</p>
<p>Further speedup of PAM, CLARA and CLARANS is outlined in <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span>, where some redundancies in the comparison
of distances in the SWAP phase are removed. In essence, this exploits the fact that observations allocated to a medoid that will be swapped out, will move
to either the second closest medoid or to the swap point. Observations that are not currently allocated to the medoid under
consideration will either stay in their current cluster, or move to the swap point, depending on how the distance to
their cluster center compares to the distance to the swap point. These ideas shorten the number of loops that need to be
evaluated and allow the algorithms to scale to much larger problems <span class="citation">(details are in Schubert and Rousseeuw <a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>, 175)</span>. In addition, they provide
and option to
carry out the swaps for all current k medoids simultaneously, similar to the logic in k-means <span class="citation">(this is implemented
in the FASTPAM2 algorithm, see Schubert and Rousseeuw <a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>, 178)</span>.</p>
<p>A second improvement in the algorithm pertains to the BUILD phase. The original approach is replaced by a so-called
<em>Linear Approximative BUILD</em> (LAB), which achieves linear runtime in <span class="math inline">\(n\)</span>. Instead of considering all candidate points, only
a subsample from the data is used, repeated <span class="math inline">\(k\)</span> times (once for each medoid).</p>
<p>The FastPAM2 algorithm tends to yield the best cluster results relative to the other methods, in terms of the smallest sum of distances to the respective
medoids. However, especially for large n and large k, FastCLARANS yields much smaller
compute times, although the quality of the clusters is not as good as for FastPAM2. FastCLARA is always much slower than the other two.
In terms of the initialization methods, LAB tends to be much faster than BUILD, especially for larger n and k.</p>
<p>The FastPAM2, FastCLARA and FastCLARANS algorithms
from <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span> were ported to C++ in <code>GeoDa</code> from the original Java code by the authors.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
</div>
</div>
<div id="implementation-1" class="section level3 unnumbered" number="">
<h3>Implementation</h3>
<p>K-medoids is invoked from the <strong>Clusters</strong> toolbar, as the third item in the classic clustering subset, as shown
in Figure <a href="#fig:kmedoid">13</a>.
Alternatively, from the menu, it is selected as <strong>Clusters &gt; K Medoids</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoid"></span>
<img src="pics7c/4_kmedoids.png" alt="K Medoids Option" width="10%" />
<p class="caption">
Figure 13: K Medoids Option
</p>
</div>
<p>This brings up the usual variable settings panel.</p>
<div id="variable-settings-panel-1" class="section level4 unnumbered" number="">
<h4>Variable Settings Panel</h4>
<p>The variable settings panel in Figure <a href="#fig:kmedoidvars">14</a> has the by now familiar layout for the
input section. It is
identical to that for k-medians, except for the <strong>Method</strong> selection and the associated options.
In our example with the same six variables as before (z-standardized), we use the default method
of <strong>FastPAM</strong>. The default <strong>Initialization Method</strong> is <strong>LAB</strong>, with <strong>BUILD</strong> available as an
alternative. In practice, LAB is much faster than BUILD.</p>
<p>There are no other options to be set, since the PAM algorithm proceeds with an exhaustive
search in the SWAP phase, after an initial set of medoids is selected.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidvars"></span>
<img src="pics7c/5_medoid_vars.png" alt="K Medoids variable selection" width="35%" />
<p class="caption">
Figure 14: K Medoids variable selection
</p>
</div>
<p>Clicking on <strong>Run</strong> brings up the cluster map, as well as the usual cluster characteristics in the right-hand panel.
In addition, the cluster classifications are saved to the table using the variable name specified in
the dialog (here <strong>CLmd1</strong>).</p>
</div>
<div id="cluster-results-1" class="section level4 unnumbered" number="">
<h4>Cluster results</h4>
<p>The cluster map in Figure <a href="#fig:kmedoidmap">15</a> show results that are fairly similar to k-medians, although different
in some important respects. In essence, the first cluster changes slightly, now with 26 members, while CL2 and CL3 swap places.
The new CL2 has 21 members (compared to 20 for CL2 in k-median) and CL3 has 18 (compared to 20 for CL2 in k-median).</p>
<p>The two smallest clusters have basically the same size in both applications, but their match differs considerably. CL4 has
no overlap between the two methods. On the other hand, CL5 is mostly the same between
the two (7 out of 9 members in common). As we have pointed out before, this highlights how the various methods pick up
different aspects of multi-attribute similarity. In many applications, k-medoids is preferred over k-means, since it tends to
be less sensitive to outliers.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidmap"></span>
<img src="pics7c/5_kmedoid_map.png" alt="K Medoids cluster map (k=5)" width="60%" />
<p class="caption">
Figure 15: K Medoids cluster map (k=5)
</p>
</div>
<p>One additional useful characteristic of the k-medoids approach is that the cluster centers are actual observations.
In Figure <a href="#fig:kmedoidcenters">16</a>, these are highlighted for our example. Note that these observations are <em>centers</em>
in multi-attribute space, and clearly not in geographical space (we address this in the next chapter).</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidcenters"></span>
<img src="pics7c/5_kmedoid_centers.png" alt="K Medoids cluster centers (k=5)" width="60%" />
<p class="caption">
Figure 16: K Medoids cluster centers (k=5)
</p>
</div>
<p>A more meaningful substantive interpretation of the cluster centers can be obtained from the cluster characteristics in
Figure <a href="#fig:kmedoidsummary">17</a>. Below the usual listing of methods and options, the <em>observation numbers</em> of the
cluster medoids are given. These can be used to select the corresponding observations in the table.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> As before, the values for the different variables in
each cluster center are listed, but now these correspond to actual observations, so we can also look these up in the
data table.</p>
<p>While these values are expressed in the original units, the remainder of the summary characteristics are in the units
used for the specified standardization, z-standardized in our example. First are the results for the within and between
sum of squares. The summary ratio is 0.414, slightly worse than for k-median (but recall that this is not the proper
objective function).</p>
<p>The point of departure for the within-cluster distances is the total distance to the overall medoid for the
data. This is the observation for which the sum of distances from all other observations is the smallest
(i.e., the observation with the smallest row or column sum of the elements in the distance matrix).</p>
<p>In our example, the within-cluster distances to each medoid and their averages show fairly tight clusters for
CL1 and CL5, and less so for the other three. The original total distance decreases from 398.5 to 265.1, a
ratio of 0.665, compared to 0.673 for k-median. This would suggest a slightly greater success at reducing the overall sum
of distances (smaller values are better).</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidsummary"></span>
<img src="pics7c/5_kmedoid_summary.png" alt="K Medoids cluster characteristics (k=5)" width="50%" />
<p class="caption">
Figure 17: K Medoids cluster characteristics (k=5)
</p>
</div>
</div>
</div>
<div id="options-and-sensitivity-analysis-1" class="section level3 unnumbered" number="">
<h3>Options and sensitivity analysis</h3>
<p>The main option for k-medoids is the choice of the <strong>Method</strong>. As shown in
Figure <a href="#fig:kmedoidmethods">18</a>, in addition to the default <strong>FastPAM</strong>, the algorithms <strong>FastCLARA</strong> and
<strong>FastCLARANS</strong> are available as well. The latter two are large data methods and they will always
perform (much) worse than FastPAM in small to medium-sized data sets. In our example, it would not
be appropriate to use these methods, but we provide a brief illustration anyway to highlight the
deterioration in the solutions found.</p>
<p>In addition, there is a choice of <strong>Initialization Method</strong> between <strong>LAB</strong> and <strong>BUILD</strong>. In most
circumstances, LAB is the preferred option, but BUILD is included for the sake of completeness and
to allow for a full range of comparisons.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidmethods"></span>
<img src="pics7c/5_method_options.png" alt="K Medoids method options" width="35%" />
<p class="caption">
Figure 18: K Medoids method options
</p>
</div>
<div id="clara" class="section level4 unnumbered" number="">
<h4>CLARA</h4>
<p>The two main parameters that need to be specified for the CLARA method are the number of samples considered
(by default set to 2) and the sample size. Since n &lt; 100 in our example, the latter is set to 40+2K = 50,
shown in Figure <a href="#fig:kmedoidclara">19</a>. In addition, the option to include the best previous medoids in the
sample is checked.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidclara"></span>
<img src="pics7c/5_clara_options.png" alt="K Medoids CLARA parameters" width="35%" />
<p class="caption">
Figure 19: K Medoids CLARA parameters
</p>
</div>
<p>The resulting cluster map is given in Figure <a href="#fig:kmedoidclaramap">20</a>, with associated cluster characteristics
in Figure <a href="#fig:kmedoidclarasummary">21</a>.</p>
<p>The main effect seems to be on CL4, which, even though it has the same number of members as for PAM, they are in
totally different locations. CL1 increases its membership from 26 to 30. CL2 and CL3 shrink somewhat, mostly
due to the (new) presence of the members of CL4. CL5 does not change.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidclaramap"></span>
<img src="pics7c/5_clara_map.png" alt="K Medoids CLARA cluster map (k=5)" width="60%" />
<p class="caption">
Figure 20: K Medoids CLARA cluster map (k=5)
</p>
</div>
<p>As the summary characteristics show, CL3 and CL4 now have different medoids.
Overall, there is a slight deterioration of the quality of the clusters, with the total within-cluster distance now
268.9 (compared to 265.1), for a ratio of 0.675 (compared to 0.665).</p>
<p>In this example, the use of a sample instead of the full data set does not lead to
a meaningful deterioration of the results. This is not totally surprising, since the sample size of 50 is almost 59% of
the total sample size. For larger data sets with k=5, the sample size will remain at 50, thus constituting a smaller and smaller
share of the actual observations as the sample size increases.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<p>To illustrate the effect of sample size, we can set its value to 30. This results in a much larger overall within sum of distances
of 273.9, with a ratio of 0.687 (details not shown). On the other hand, if we set the sample size to 85, then we obtain the exact
same results as for PAM.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidclarasummary"></span>
<img src="pics7c/5_clara_summary.png" alt="K Medoids CLARA cluster characteristics (k=5)" width="50%" />
<p class="caption">
Figure 21: K Medoids CLARA cluster characteristics (k=5)
</p>
</div>
</div>
<div id="clarans" class="section level4 unnumbered" number="">
<h4>CLARANS</h4>
<p>For CLARANS, the two relevant parameters pertain to the number of iterations and the sample rate, as shown
in Figure <a href="#fig:kmedoidclarans">22</a>. The former corresponds to the <em>numlocal</em> parameter in <span class="citation">Ng and Han (<a href="#ref-NgHan:02" role="doc-biblioref">2002</a>)</span>,
i.e., the number of times a local optimum is computed (default = 2). The sample rate pertains to the maximum number
of <em>neighbors</em> that will be randomly sampled in each iteration (<em>maxneighbors</em> in the paper). This is
expressed as a fraction of <span class="math inline">\(k \times (n - k)\)</span>. We use the value of 0.025 recommended by <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span>, which
yields a maximum neighbors of 10 (0.025 x 400) for each iteration in our example. Unlike PAM and CLARA, there is no
initialization option.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidclarans"></span>
<img src="pics7c/5_clarans_options.png" alt="K Medoids CLARANS parameters" width="35%" />
<p class="caption">
Figure 22: K Medoids CLARANS parameters
</p>
</div>
<p>The results are presented in Figures <a href="#fig:kmedoidclaransmap">23</a> and <a href="#fig:kmedoidclaranssummary">24</a>.
The cluster map is quite different from the result for PAM. All but the original CL5 are considerably affected.
CL1 from PAM moves southward and grows to 32 members. All the other clusters lose members. CL2 from PAM disintegrates
and drops to 16 members. CL3 shifts in location, but stays roughly the same size. CL4 moves north.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidclaransmap"></span>
<img src="pics7c/5_clarans_map.png" alt="K Medoids CLARANS cluster map (k=5)" width="60%" />
<p class="caption">
Figure 23: K Medoids CLARANS cluster map (k=5)
</p>
</div>
<p>The cluster medoids listed in Figure <a href="#fig:kmedoidclaranssummary">24</a> show all different cluster centers under CLARANS, except for CL5 (which
is essentially unchanged). The total within sum of distances is 301.177, quite a bit larger than 265.147 for PAM. Correspondingly, the
ratio of within to total is much higher as well, at 0.756.</p>
<p>CLARANS is a large data method and is optimized for speed (especially with large n and large k). It should not be used in smaller samples, where the exhaustive search carried out by PAM can be computed in a reasonable time.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedoidclaranssummary"></span>
<img src="pics7c/5_clarans_summary.png" alt="K Medoids CLARANS cluster characteristics (k=5)" width="50%" />
<p class="caption">
Figure 24: K Medoids CLARANS cluster characteristics (k=5)
</p>
</div>
</div>
<div id="comparison-of-methods-and-initialization-settings" class="section level4 unnumbered" number="">
<h4>Comparison of methods and initialization settings</h4>
<p>In order to provide a better idea of the various trade-offs involved in the selection of algorithms and initialization
settings, Figures <a href="#fig:sdoh">25</a> and <a href="#fig:natregimes">26</a> list the results of a number of experiments, for different
values of k and n.</p>
<p>Figure <a href="#fig:sdoh">25</a> pertains to a sample of 791 Chicago census tracts and a clustering of socio-economic characteristics
that were reduced to 5 principal components.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> For this number of
observations, computation time is irrelevant, since all results are obtained almost instantaneously (in less than a second).</p>
<p>The number of clusters is considered for k = 5, 10, 30, 77 (the number of community areas in the city), and 150. Recall
that the value of k is a critical component of the sample size for CLARA (80 + 2k) and CLARANS (a percentage of <span class="math inline">\(k \times (n-k)\)</span>).
In all cases, PAM with LAB obtains the best local minimum (for k=5, it is tied with PAM-BUILD). Also, CLARANS consistently
gives the worst result. The gap with the best local optimum grows as k gets larger, from about 6% larger for k=5 to more than
20% for k=150. The results for CLARA are always in the middle, with LAB superior for k=5 and 10, and BUILD superior in the
other instances.</p>
<div class="figure" style="text-align: center"><span id="fig:sdoh"></span>
<img src="pics7c/6_sdoh.png" alt="Local Optima - Chicago Census Tracts (n=791)" width="60%" />
<p class="caption">
Figure 25: Local Optima - Chicago Census Tracts (n=791)
</p>
</div>
<p>Figure <a href="#fig:natregimes">26</a> uses the <strong>natregimes</strong> sample data set with 3085 observations for U.S. counties. The number
of variables to be clustered was increased to 20, consisting of the variables RD, PS, UE, DV and MA in all four years.</p>
<p>In this instance, compute time does matter, and significantly so starting with k=30. CLARANS is always the fastest, from
instantaneous at k=30 to 1 second for k=300 and 2 seconds for k=500. PAM is also reasonable, but quite a bit slower, with
the LAB option always faster than BUILD. For k=30, the difference between the two is minimal (2 seconds vs 3 seconds), but
for k=300 PAM-LAB is about twice as fast (12 seconds vs 26 seconds), and for k=500 almost four times as fast (14 seconds vs
41 seconds). The compute time for CLARA is problematic, especially when using BUILD and for k=300 and 500 (for smaller k, it
is on a par with PAM). For k=500, CLARA-BUILD takes some 6 minutes, whereas CLARA-LAB only take somewhat over a minute.</p>
<p>The best local optimum is again obtained for PAM, with BUILD slightly better for k=10 and 30, whereas for the other cases
the LAB option is superior. In contrast to the Chicago example, CLARANS is not always worst and is better than CLARA for
k = 5, 10 and 30, but not for k = 300 and 500.</p>
<div class="figure" style="text-align: center"><span id="fig:natregimes"></span>
<img src="pics7c/6_natregimes.png" alt="Local Optima - U.S. Counties (n=3085)" width="60%" />
<p class="caption">
Figure 26: Local Optima - U.S. Counties (n=3085)
</p>
</div>
<p>Overall, it seems that the FastPAM approach with the default setting of LAB performs very reliably and
with decent (to superior) computation times. However, it remains a good idea to carry out further
sensitivity analysis. Also, the algorithms only obtain <em>local</em> optima, and there is no guarantee of
a global optimum. Therefore, if there is the opportunity to explore different options, they should be
investigated.</p>
</div>
</div>
</div>
<div id="spectral-clustering" class="section level2 unnumbered" number="">
<h2>Spectral Clustering</h2>
<div id="principle-2" class="section level3 unnumbered" number="">
<h3>Principle</h3>
<p>Clustering methods like k-means, k-medians or k-medoids are designed to discover convex
clusters in the multidimensional data cloud. However, several interesting cluster shapes are
not convex, such as the classic textbook spirals or moons example, or the famous <em>Swiss roll</em> and similar lower
dimensional shapes embedded in a higher dimensional space. These problems are characterized
by a property that projections of the data points onto the original orthogonal coordinate axes
(e.g., the x, y, z, etc. axes) do not create good separations. <em>Spectral clustering</em> approaches
this issue by reprojecting the observations onto a new axes system and carrying out the
clustering on the projected data points. Technically, this will boil down to the use
of eigenvalues and eigenvectors, hence the designation as <em>spectral</em> clustering (recall the
spectral decomposition of a matrix discussed in the chapter on principal components).</p>
<p>To illustrate the problem, consider the result of k-means and k-medoids (for k=2) applied to the famous <em>spirals</em> data set from
Figure <a href="#fig:spiralmap">2</a>. As Figure <a href="#fig:spiralskmean">27</a> shows, these
methods tend to yield convex clusters, which fail to detect the nonlinear arrangement of the data. The k-means
clusters are arranged above and below a diagonal, whereas the k-medoids result shows more of a left-right pattern.</p>
<div class="figure" style="text-align: center"><span id="fig:spiralskmean"></span>
<img src="pics7c/8_kmean_kmedoid_spiral.png" alt="K-means and K-medoids on spirals data set (k=2)" width="90%" />
<p class="caption">
Figure 27: K-means and K-medoids on spirals data set (k=2)
</p>
</div>
<p>In constrast, as shown in Figure <a href="#fig:spiralsspectral">28</a>, a spectral clustering algorithm applied to this data set perfectly extracts the two underlying
patterns (initialized with a knn parameter of 3, see below for further details and illustration).</p>
<div class="figure" style="text-align: center"><span id="fig:spiralsspectral"></span>
<img src="pics7c/8_spectral_spiral.png" alt="Spectral clustering on spirals data set (k=2)" width="60%" />
<p class="caption">
Figure 28: Spectral clustering on spirals data set (k=2)
</p>
</div>
<p>The mathematics underlying spectral clustering view it as a problem of graph partitioning, i.e.,
separating a graph into subgraphs that are internally well connected, but only weakly connected
with the other subgraphs. We briefly discuss this idea first, followed by an overview of the
main steps in a spectral clustering algorithm, including a review of some
important parameters that need to be tuned in practice.</p>
<p>An exhaustive overview of the various mathematical properties associated with spectral clustering
is contained in the <em>tutorial</em> by <span class="citation">von Luxburg (<a href="#ref-vonLuxburg:07" role="doc-biblioref">2007</a>)</span>, to which we refer for technical details. An intuitive
description is also given in <span class="citation">Han, Kamber, and Pei (<a href="#ref-Hanetal:12" role="doc-biblioref">2012</a>)</span>, pp. 519-522.</p>
<div id="clustering-as-a-graph-partitioning-problem" class="section level4 unnumbered" number="">
<h4>Clustering as a graph partitioning problem</h4>
<p>So far, the clustering methods we have discussed were based on a <em>dissimilarity</em> matrix and had the
objective of minimizing within-cluster dissimilarity. In spectral clustering, the focus is on the
complement, i.e., a <em>similarity</em> matrix, and the goal is to maximize the internal similarity within a
cluster. Of course, any dissimilarity matrix can be turned into a similarity matrix using a number
of different methods, such as the use of a distance decay function (inverse distance, negative exponential) or
by simply taking the difference from the maximum (e.g., <span class="math inline">\(d_{max} - d_{ij}\)</span>).</p>
<p>A similarity matrix <span class="math inline">\(S\)</span> consisting of elements <span class="math inline">\(s_{ij}\)</span> can be viewed as the basis for the <em>adjacency matrix</em>
of a weighted undirected graph <span class="math inline">\(G = (V,E)\)</span>. In this graph, the vertices <span class="math inline">\(V\)</span> are the observations and the edges <span class="math inline">\(E\)</span>
give the strength of the similarity between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, <span class="math inline">\(s_{ij}\)</span>. This is identical to the interpretation of a
spatial weights matrix that we have seen before. In fact, the standard notation for the adjacency matrix is
to use <span class="math inline">\(W\)</span>, just as we did for spatial weights.</p>
<p>Note that, in practice, the adjacency matrix is typically not the same as the full similarity matrix, but
follows from a transformation of the latter to a <em>sparse</em> form (see below for specifics).</p>
<p>The goal of graph partitioning is to delineate subgraphs that are internally strongly connected, but only
weakly connected with the other subgraphs. In the ideal case, the subgraphs are so-called connected components,
in that their elements are all internally connected, but there are no connections to the other subgraphs. In
practice, this will rarely be the case. The objective thus becomes one of finding a set of <em>cuts</em> in the graph that
create a partioning of <span class="math inline">\(k\)</span> subsets to maximize internal connectivity and minimize in-between connectivity. A naive application
of this principle would lead to the least connected vertices to become singletons (similar to what we found
in single and average linkage hierarchical clustering) and all the rest to form one large cluster. Better
suited partitioning methods include a weighting of the cluster <em>size</em> so as to end up with well-balanced
subset.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></p>
<p>A very important concept in this regard is the <em>graph Laplacian</em> associated with the adjacency matrix <span class="math inline">\(W\)</span>. We
discuss this further in the <a href="#appendix">Appendix</a>.</p>
</div>
</div>
<div id="the-spectral-clustering-algorithm" class="section level3 unnumbered" number="">
<h3>The spectral clustering algorithm</h3>
<p>In general terms, a spectral clustering algorithm consists of four phases:</p>
<ul>
<li><p>turning the similarity matrix into an adjacency matrix</p></li>
<li><p>computing the <span class="math inline">\(k\)</span> smallest eigenvalues and eigenvectors of the graph Laplacian
(alternatively, the <span class="math inline">\(k\)</span> largest eigenvalues and eigenvectors of the affinity matrix are calculated)</p></li>
<li><p>using the (rescaled) resulting eigenvectors to carry out k-means clustering</p></li>
<li><p>associating the resulting clusters back to the original observations</p></li>
</ul>
<p>The most important step is the construction of the adjacency matrix, which we consider first.</p>
<div id="creating-an-adjacency-matrix" class="section level4 unnumbered" number="">
<h4>Creating an adjacency matrix</h4>
<p>The first phase consists of selecting a criterion to turn the <em>dense</em> similarity matrix into a
<em>sparse</em> adjacency matrix, sometimes also referred to as the <em>affinity matrix</em>. The logic is very similar to that of creating spatial weights by means of
a distance criterion.</p>
<p>For example, we could use a distance band to select <em>neighbors</em> that are within a critical
distance <span class="math inline">\(\delta\)</span>. In the spectral clustering literature, this is referred to as an epsilon (<span class="math inline">\(\epsilon\)</span>) criterion.
This approach shares the same issues as in the spatial case when the observations are distributed with
very different densities. In order to avoid isolates, a max-min nearest neighbor distance needs to be
selected, which can result in a very unbalanced adjacency matrix. An adjacency matrix derived from the
<span class="math inline">\(\epsilon\)</span> criterion is typically used in unweighted form.</p>
<p>A preferred approach is to use k nearest neighbors, although this is not a symmetric property. Consequently,
the resulting adjacency matrix is for a directed graph, since <span class="math inline">\(i\)</span> being one of the k nearest neighbors of <span class="math inline">\(j\)</span> does not
guarantee that <span class="math inline">\(j\)</span> is one of the k nearest neighbors for <span class="math inline">\(i\)</span>. We can illustrate this with our toy example, using two
nearest neighbors for observations 5 and 6 in Figure <a href="#fig:knn65">29</a>. In the left hand panel, the two neighbors of observation
5 are shown as 4 and 7. In the right hand panel, 5 and 7 are neighbors of 6. So, while 5 is a neighbor of 6, the reverse is
not true, creating an asymmetry in the affinity matrix (the same is true for 2 and 3: 3 is a neighbor of 2, but 2 is not
a neighbor of 3).</p>
<div class="figure" style="text-align: center"><span id="fig:knn65"></span>
<img src="pics7c/8_nbrs5_6.png" alt="Asymmetry of nearest neighbors" width="60%" />
<p class="caption">
Figure 29: Asymmetry of nearest neighbors
</p>
</div>
<p>Since the eigenvalue computations require a symmetric matrix, there are two approaches to remedy the asymmetry.
In one, the affinity matrix is made symmetric as <span class="math inline">\((1/2) (W + W&#39;)\)</span>. In other words, if <span class="math inline">\(w_{ij} = 1\)</span>, but <span class="math inline">\(w_{ji} = 0\)</span>
(or the reverse), a
new set of weights is created with <span class="math inline">\(w_{ij} = w_{ji} = 1/2\)</span>. This is illustrated in the left-hand panel of
Figure <a href="#fig:mutualknn">30</a>, where the associated symmetric connectivity graph is shown.</p>
<p>Instead of a pure k-nearest neighbor criterion, so-called <em>mutual</em> k nearest
neighbors can be defined, which consists of those neighbors among the k-nearest neighbor set that <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>
have in common. More precisely, only those connectivities are kept for which <span class="math inline">\(w_{ij} = w_{ji} = 1\)</span>. The
corresponding connectivity graph for our example is shown in the right hand panel of Figure <a href="#fig:mutualknn">30</a>. The links
between 2 and 3 as well as between 5 and 6 have been removed.</p>
<p>Once the affinity matrix has been turned into a symmetric form, the resulting adjacencies are weighted
with the original <span class="math inline">\(s_{ij}\)</span> values.</p>
<div class="figure" style="text-align: center"><span id="fig:mutualknn"></span>
<img src="pics7c/8_mutualandfullknn.png" alt="Symmetric k-nearest neighbor matrices" width="60%" />
<p class="caption">
Figure 30: Symmetric k-nearest neighbor matrices
</p>
</div>
<p>The knn adjacency matrix can join points in disconnected parts of the graph, whereas the mutual
k nearest neighbors will be sparser and tends to connect observations in
regions with constant density. Each has pros and cons, depending on the underlying structure of the data.</p>
<p>A final approach is to compute a similarity matrix that has a built-in distance decay. The most common
method is to use a Gaussian density (or <em>kernel</em>) applied to the Euclidean distance between observations:
<span class="math display">\[s_{ij} = \exp [- (x_i - x_j)^2 / 2\sigma^2],\]</span>
where the standard deviation <span class="math inline">\(\sigma\)</span> plays the role of a bandwidth. With the right choice of <span class="math inline">\(\sigma\)</span>, we
can make the corresponding adjacency matrix more or less sparse.</p>
<p>Note that, in fact, the Gaussian transformation translates a
dissimilarity matrix (Euclidean distances) into a similarity matrix. The new similarity matrix plays the role
of the adjacency matrix in the remainder of the algorithm.</p>
</div>
<div id="clustering-on-the-eigenvectors-of-the-graph-laplacian" class="section level4 unnumbered" number="">
<h4>Clustering on the eigenvectors of the graph Laplacian</h4>
<p>With the adjacency matrix in place, the <span class="math inline">\(k\)</span> smallest eigenvalues and associated eigenvectors
of the normalized graph Laplacian can be computed. Since we only need a few eigenvalues, specialized
algorithms are used that only extract the smallest or largest eigenvalues/eigenvectors.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p>When a symmetric normalized graph Laplacian is used, the <span class="math inline">\(n \times k\)</span> matrix of eigenvectors, say <span class="math inline">\(U\)</span>,
is row-standardized such that the norm of each row equals 1. The new matrix <span class="math inline">\(T\)</span> has elements:<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>
<span class="math display">\[t_{ij} = \frac{u_{ij}}{(\sum_{h=1}^k u_{ih}^2)^{1/2}}.\]</span></p>
<p>The new “observations” consist of the values of <span class="math inline">\(t_{ij}\)</span> for each <span class="math inline">\(i\)</span>. These values are used in a
standard k-means clustering algorithm to yield <span class="math inline">\(k\)</span> clusters. Finally, the labels for the clusters
are associated with the original observations and several cluster characteristics can be computed.</p>
<p>In <code>GeoDa</code>, the symmetric normalized graph Laplacian is used, porting to C++ the algorithm of
<span class="citation">Ng, Jordan, and Weiss (<a href="#ref-Ngetal:02" role="doc-biblioref">2002</a>)</span> as implemented in Python in <code>scikit-learn</code>.</p>
</div>
<div id="spectral-clustering-parameters" class="section level4 unnumbered" number="">
<h4>Spectral clustering parameters</h4>
<p>In practice, the results of spectral clustering tend to be highly sensitive to the choice of the
parameters used to define the adjacency matrix. For example, when using k-nearest neighbors,
the choice of the number of neighbors is an important decision. In the literature,
a value of <span class="math inline">\(k\)</span> (neighbors, not clusters) of the order of <span class="math inline">\(\log(n)\)</span> is suggested for large <span class="math inline">\(n\)</span> <span class="citation">(von Luxburg <a href="#ref-vonLuxburg:07" role="doc-biblioref">2007</a>)</span>. In practice,
both <span class="math inline">\(\ln(n)\)</span> as well as <span class="math inline">\(\log_{10}(n)\)</span> are used. <code>GeoDa</code> provides both as options.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<p>Simlarly, the bandwidth of the Gaussian transformation is determined by the value for the standard
deviation, <span class="math inline">\(\sigma\)</span>.
One suggestion for the value of <span class="math inline">\(\sigma\)</span> is to
take the mean distance to the k nearest neighbor, or <span class="math inline">\(\sigma \sim \log(n) + 1\)</span> <span class="citation">(von Luxburg <a href="#ref-vonLuxburg:07" role="doc-biblioref">2007</a>)</span>.
Again, either <span class="math inline">\(\ln(n) + 1\)</span> or <span class="math inline">\(\log_{10}(n) + 1\)</span> can be implemented. In addition, the default
value used in the <code>scikit-learn</code> implementation suggests <span class="math inline">\(\sigma = \sqrt{1/p}\)</span>, where <span class="math inline">\(p\)</span> is the
number of variables (features) used.
<code>GeoDa</code> includes all three options.</p>
<p>In practice, these parameters are best set by trial and error, and a careful sensitivity
analysis is in order.</p>
</div>
</div>
<div id="implementation-2" class="section level3 unnumbered" number="">
<h3>Implementation</h3>
<p>Spectral clustering is invoked from the <strong>Clusters</strong> toolbar, as the next to last item in the classic clustering subset, as shown
in Figure <a href="#fig:spectral">31</a>.
Alternatively, from the menu, it is selected as <strong>Clusters &gt; Spectral</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:spectral"></span>
<img src="pics7c/8_spectral.png" alt="Spectral Clustering Option" width="10%" />
<p class="caption">
Figure 31: Spectral Clustering Option
</p>
</div>
<p>We use the <strong>spirals</strong> data set, with an associated variable settings panel that only
contains the variables <strong>x</strong> and <strong>y</strong>, as shown in Figure <a href="#fig:spectralvars">32</a>.</p>
<div id="variable-settings-panel-2" class="section level4 unnumbered" number="">
<h4>Variable Settings Panel</h4>
<p>The variable settings panel has the same general layout as for the other clustering methods. One distinction
is that there are two sets of parameters. One set pertains to the construction of the <strong>Affinity</strong> (or adjacency) matrix,
the others are relevant for the k-means algorithm that is applied to the transformed eigenvectors. The k-means options
are the standard ones.</p>
<p>The <strong>Affinity</strong> option provides three alternatives, <strong>K-NN</strong>, <strong>Mutual K-NN</strong> and <strong>Gaussian</strong>, with specific parameters for
each. For now, we set the number of clusters to 2 and keep all options to the default setting. This includes <strong>K-NN</strong> with 3 neighbors
for the affinity matrix, and all the default settings for k-means. The value of 3 for knn corresponds to <span class="math inline">\(\log_{10}(300) = 2.48\)</span>, rounded up to the
next integer.</p>
<p>We save the cluster in the field <strong>CLs1</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralvars"></span>
<img src="pics7c/00_spectral_variables.png" alt="Spectral clustering variable selection" width="35%" />
<p class="caption">
Figure 32: Spectral clustering variable selection
</p>
</div>
</div>
<div id="cluster-results-2" class="section level4 unnumbered" number="">
<h4>Cluster results</h4>
<p>The cluster map in Figure <a href="#fig:spectralmap">33</a> shows a perfect separation of the two spirals, with 150 observations
each. As mentioned earlier, neither k-means nor k-medoids is able to extract these highly nonlinear and non-convex clusters.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralmap"></span>
<img src="pics7c/00_spectral_default_map.png" alt="Spectral cluster map (k=2)" width="60%" />
<p class="caption">
Figure 33: Spectral cluster map (k=2)
</p>
</div>
<p>The cluster characteristics in Figure <a href="#fig:spectralsummary">34</a> list the parameter settings first, followed by the values
for the cluster centers (the mean) for the two (standardized) coordinates and the decomposition of the sum of squares. The
ratio of between sum of squares to total sum of squares is a dismal 0.04. This is not surprising, since this criterion provides a
measure of the degree of compactness for the cluster, which a non-convex cluster like the spirals example does not meet.</p>
<p>In this example, it is easy to visually assess the extent to which the nonlinearity is captured. However, in the typical high-dimensional
application, this will be much more of a challenge, since the usual measures of compactness may not be informative. A careful
inspection of the distribution of the different variables across the observations in each cluster is therefore in order.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralsummary"></span>
<img src="pics7c/00_spectral_default_results.png" alt="Spectral cluster characteristics (k=2)" width="50%" />
<p class="caption">
Figure 34: Spectral cluster characteristics (k=2)
</p>
</div>
</div>
</div>
<div id="options-and-sensitivity-analysis-2" class="section level3 unnumbered" number="">
<h3>Options and sensitivity analysis</h3>
<p>The results of spectral clustering are extremely sensitive to the parameters chosen to create
the affinity matrix. Suggestions for default values are only suggestions, and the particular values
many sometimes be totally unsuitable. Experimentation is therefor a necessity. There are two
classes of parameters. One set pertains to the number of nearest neighbors for knn or mutual knn.
The other set relates to the bandwidth of the Gaussian kernel, determined by the standard deviation
<strong>sigma</strong>.</p>
<div id="k-nearest-neighbors-affinity-matrix" class="section level4 unnumbered" number="">
<h4>K-nearest neighbors affinity matrix</h4>
<p>The two default values for the number of nearest neighbors are contained in a drop-down list, as shown in
Figure <a href="#fig:spectralknnparms">35</a>. In our example, with n=300, <span class="math inline">\(\log_{10}(n) = 2.48\)</span>, which rounds up to 3,
and <span class="math inline">\(\ln(n) = 5.70\)</span>, which rounds up to 6. These are the two default values provided. Any other value can
be entered manually in the dialog as well.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralknnparms"></span>
<img src="pics7c/00_affinity_knn.png" alt="KNN affinity parameter values" width="25%" />
<p class="caption">
Figure 35: KNN affinity parameter values
</p>
</div>
<p>The cluster map with knn = 6 is shown in Figure <a href="#fig:spectralknn6">36</a>. Unlike what we found for the
default option, this value is not able to yield a clean separation of the spirals. In addition, the
resulting clusters are highly unbalanced, with respectively 241 and 59 members.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralknn6"></span>
<img src="pics7c/00_spectral_knn_6.png" alt="KNN affinity k=6 cluster map" width="60%" />
<p class="caption">
Figure 36: KNN affinity k=6 cluster map
</p>
</div>
<p>The options for a mutual knn affinity matrix have the same entries, as in Figure <a href="#fig:spectralmutualknnparms">37</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralmutualknnparms"></span>
<img src="pics7c/00_affinity_mutualknn.png" alt="Mutual KNN affinity parameter values" width="25%" />
<p class="caption">
Figure 37: Mutual KNN affinity parameter values
</p>
</div>
<p>Here again, neither of the options yields a satisfactory solution, as illustrated in Figure <a href="#fig:spectralmutual">38</a>,
for knn=3 in the left panel and knn=6 in the right panel. The first solution has members in both spirals, whereas the second
solution does not cross over, but it only picks up part of the separate spiral.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralmutual"></span>
<img src="pics7c/00_mutualknn.png" alt="Mutual KNN affinity cluster maps" width="90%" />
<p class="caption">
Figure 38: Mutual KNN affinity cluster maps
</p>
</div>
</div>
<div id="gaussian-kernel-affinity-matrix" class="section level4 unnumbered" number="">
<h4>Gaussian kernel affinity matrix</h4>
<p>The built-in options for <strong>sigma</strong>, the standard deviation of the Gaussian kernel are listed in Figure <a href="#fig:spectralgaussianparms">39</a>.
The smallest value of 0.707107 corresponds to <span class="math inline">\(\sqrt{1/p}\)</span>, where <span class="math inline">\(p\)</span>, the number of variables, equals 2 in our example.
The other two values are <span class="math inline">\(\log_{10}(n) + 1\)</span> and <span class="math inline">\(\ln(n) + 1\)</span>, yielding respectively 3.477121 and 6.703782 for n=300. In addition,
any other value can be entered in the dialog.</p>
<div class="figure" style="text-align: center"><span id="fig:spectralgaussianparms"></span>
<img src="pics7c/00_affinity_sigma.png" alt="Gaussian kernel affinity parameter values" width="25%" />
<p class="caption">
Figure 39: Gaussian kernel affinity parameter values
</p>
</div>
<p>None of the default values yield particularly good results, as illustrated in Figure <a href="#fig:spectralgaussian2">40</a>.
In the left hand panel, the clusters are shown with <span class="math inline">\(\sigma = 0.707107\)</span>. The result totally fails to extract the shape of
the separate spirals and looks similar to the results for k-mean and k-median in Figure <a href="#fig:spiralskmean">27</a>. The result
for <span class="math inline">\(\sigma = 6.703782\)</span> is almost identical, with the roles of cluster 1 and 2 switched. Both are perfectly balanced
clusters (the result for <span class="math inline">\(\sigma = 3.477121\)</span> are similar).</p>
<div class="figure" style="text-align: center"><span id="fig:spectralgaussian2"></span>
<img src="pics7c/00_gaussian.png" alt="Gaussian kernel affinity cluster maps" width="90%" />
<p class="caption">
Figure 40: Gaussian kernel affinity cluster maps
</p>
</div>
<p>In order to find a solution that provides the same separation as in
Figure <a href="#fig:spectralmap">33</a>, we need to experiment with different values for <span class="math inline">\(\sigma\)</span>. As it turns
out, we obtain the same result as for knn with 3 neighbors for <span class="math inline">\(\sigma = 0.08\)</span> or <span class="math inline">\(0.07\)</span>, neither of which
are even close to the default values. This illustrates how in an actual example, where the results cannot
be readily visualized in two dimensions, it may be very difficult to find the parameter values that
discover the true underlying patterns.</p>
</div>
</div>
</div>
<div id="appendix" class="section level2 unnumbered" number="">
<h2>Appendix</h2>
<div id="worked-example-for-k-medians" class="section level3 unnumbered" number="">
<h3>Worked example for k-medians</h3>
<p>We use the same toy example as in the previous chapters to illustrate the workings of the
k-median algorithm. For easy reference, the coordinates of the seven points are listed again
in the second and third columns of Figure <a href="#fig:kmedianex1">41</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedianex1"></span>
<img src="pics7c/2_exampledata.png" alt="Worked example - basic data" width="35%" />
<p class="caption">
Figure 41: Worked example - basic data
</p>
</div>
<p>The corresponding dissimilarity matrix is based on the Manhattan distance metric, i.e., the sum
of the absolute difference in the x and y coordinates between the points. The result is given in
Figure <a href="#fig:manhattand">42</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:manhattand"></span>
<img src="pics7c/2_manhattan_distance.png" alt="Manhattan distance matrix" width="80%" />
<p class="caption">
Figure 42: Manhattan distance matrix
</p>
</div>
<p>The median center of the data cloud consists of the median of the x and y coordinates. In our case,
this is the point (6,6), as shown in Figure <a href="#fig:kmedianex1">41</a>. This happens to coincide with one of the
observations (4), but in general this would not be the case. The distances from each observation to the
median center are listed in the fourth column of <a href="#fig:kmedianex1">41</a>. The sum of these distances (24) can
be used to gauge the improvement that follows from each subsequent cluster allocation.</p>
<p>As in the k-means algorithm, the first step consists of randomly selecting k starting centers. In our
example, with k=2, we select observations 4 and 7, as we did for k-means.
Figure <a href="#fig:kmedian1">43</a> gives the Manhattan distance between each observation and each of the centers.
Observations closest to the respective center are grouped into the two initial clusters. In our example,
these first two clusters consist of 1, 2, 3, and 5 assigned to center 4, and 6 assigned to center 7.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedian1"></span>
<img src="pics7c/2_kmedian1.png" alt="K-median - step 1" width="25%" />
<p class="caption">
Figure 43: K-median - step 1
</p>
</div>
<p>Next, we calculate the new cluster medians and compute the total within cluster distance as the sum of
the distances from each allocated observation to the median center. This is illustrated in
Figure <a href="#fig:kmedian1a">44</a>. For the first cluster, the median is (4,5) and the total within cluster
distance is 14. In the second cluster, the median is (8.5,7) and the total is 3 (in the case of
an even number of observations in the cluster, the midpoint
between the two values closest to the median is chosen, hence 8.5 and 7). The value of the objective
function after this first step is thus 14 + 3 = 17.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedian1a"></span>
<img src="pics7c/2_kmedian_alloc1.png" alt="K-median - step 1 distance to median" width="70%" />
<p class="caption">
Figure 44: K-median - step 1 distance to median
</p>
</div>
<p>We repeat the procedure, calculate the distance from each observation to both cluster centers and
assign the observation to the closest center. This results in observation 5 moving from cluster 1
to cluster 2, as illustrated in Figure <a href="#fig:kmedian2">45</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedian2"></span>
<img src="pics7c/2_kmedian2.png" alt="K-median - step 2" width="25%" />
<p class="caption">
Figure 45: K-median - step 2
</p>
</div>
<p>The new allocation results in (4,3.5) as the median center for the first cluster, and (8,6) as the
center for the second cluster, as shown in
Figure <a href="#fig:kmedian2a">46</a>. The total distance decreases to 10 + 4 = 14.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedian2a"></span>
<img src="pics7c/2_kmedian_alloc2.png" alt="K-median - step 2 distance to median" width="70%" />
<p class="caption">
Figure 46: K-median - step 2 distance to median
</p>
</div>
<p>We compute the distances to the two new centers in Figure <a href="#fig:kmedian3">47</a> and assign the
observations to the closest center. Now, observation 4 moves from the first cluster to the second cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedian3"></span>
<img src="pics7c/2_kmedian3.png" alt="K-median - step 3" width="25%" />
<p class="caption">
Figure 47: K-median - step 3
</p>
</div>
<p>The updated median centers are (4,3) and (7.5,6) as shown in Figure <a href="#fig:kmedian3a">48</a>. The total
distance is 5 + 6 = 11.</p>
<div class="figure" style="text-align: center"><span id="fig:kmedian3a"></span>
<img src="pics7c/2_kmedian_alloc3.png" alt="K-median - step 3 distance to median" width="70%" />
<p class="caption">
Figure 48: K-median - step 3 distance to median
</p>
</div>
<p>Finally, computing the distances to the new centers does not induce any more changes, and the
algorithm concludes (Figure <a href="#fig:kmedianf">49</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:kmedianf"></span>
<img src="pics7c/2_kmedian_final.png" alt="K-median - final allocation" width="25%" />
<p class="caption">
Figure 49: K-median - final allocation
</p>
</div>
<p>The first cluster consists of 1, 2, and 3 with a median center of (4,3), the second cluster consists
of 4, 5, 6, and 7 with a median center of (7.5,6). Neither of these are actual observations. The clustering
resulted in a reduction of the total sum of distances to the center from 24 to 11.</p>
<p>The ratio 11/24 = 0.458 gives a
measure of the relative improvement of the objective function. It should not be confused with the ratio of
within sum of squares to total sum of squares, since the latter uses a different measure of fit (squared differences
instead of absolute differences) and a different reference point (the mean of the cluster rather than the median).
So, the two measures are not comparable.</p>
</div>
<div id="worked-example-for-pam" class="section level3 unnumbered" number="">
<h3>Worked example for PAM</h3>
<p>We continue to use the same toy example, with the point coordinates as in Figure <a href="#fig:kmedianex1">41</a>
and associated Manhattan distance matrix in Figure <a href="#fig:manhattand">42</a>.</p>
<div id="build" class="section level4 unnumbered" number="">
<h4>BUILD</h4>
<p>In the BUILD phase, since <span class="math inline">\(k=2\)</span>, we need find two starting centers. The first center is the observation that minimizes the row or
column sum of the Manhattan distance matrix. In Figure <a href="#fig:kmedianex1">41</a>, we saw that the median center
coincided with observation 4 (6,6), with an associated sum of distances to the center of 24. That is our point
of departure. To find the second center, we need for each candidate (i.e., the 6 remaining observations) the
distance to observation 4, the current <em>closest</em> center (there is only one, so that is straightforward in our
example). The associated values are listed below the observation ID in the second row of Figure <a href="#fig:build">50</a>.</p>
<p>Next, we evaluate for each row-column combination <span class="math inline">\(i,j\)</span> the expression <span class="math inline">\(\mbox{max}(d_{j4} - d_{ij},0)\)</span>, and enter that
in the corresponding element of the matrix. For example, for column 1 and row 2, that value is <span class="math inline">\(7 - d_{2,1} = 7 - 3 = 4\)</span>.
For column 1 and row 5, the corresponding value is <span class="math inline">\(7 - 8 = -1\)</span>, which results in a zero entry.</p>
<p>With all the values entered in the matrix, we compute the row sums. There is a tie between observations 3 and 7. For
consistency with our other examples, we pick 7 as the second starting point.</p>
<div class="figure" style="text-align: center"><span id="fig:build"></span>
<img src="pics7c/3_build.png" alt="PAM - finding the second initial center" width="70%" />
<p class="caption">
Figure 50: PAM - finding the second initial center
</p>
</div>
</div>
<div id="swap" class="section level4 unnumbered" number="">
<h4>SWAP</h4>
<p>The initial stage is the same as for the k-median example, given in Figure <a href="#fig:kmedian1">43</a>. The total
distance to each cluster center equals 20, with cluster 1 contributing 17 and cluster 2, 3.</p>
<p>The first step in the swap procedure consists of evaluating whether center 4 or center 7 can be replaced
by any of the current non-centers, i.e., 1, 2, 3, 5, or 6. The comparison involves three distances for each
point: the distance
to the closest center, the distance to the second closest center, and the distance to the candidate center.
In our example, this is greatly simplified, since there are only two centers, with distances <span class="math inline">\(d_{j4}\)</span> and
<span class="math inline">\(d_{j7}\)</span> for each non-candidate and non-center point <span class="math inline">\(j\)</span>. In addition, we need the distance to the candidate
center <span class="math inline">\(d_{jr}\)</span>, where each non-center point is in turn considered as a candidate (<span class="math inline">\(r\)</span> in our notation).</p>
<p>All the evaluations for the first step are included in Figure <a href="#fig:swap1">51</a>. There are five main panels,
one for each current non-center point. The rows in each panel are the non-center, non-candidate points.
For example, in the top panel, 1 is considered a candidate, so the rows pertain to 2, 3, 5, and 6.
Columns 3-4 give the distance to, respectively, center 4 (<span class="math inline">\(d_{j4}\)</span>), center 7 (<span class="math inline">\(d_{j7}\)</span>) and candidate center 1 (<span class="math inline">\(d_{j1}\)</span>).</p>
<p>Columns 5 and 6 give the contribution of each row to the objective with point 1 replacing, respectively 4 (<span class="math inline">\(C_{j41}\)</span>) and
7 (<span class="math inline">\(C_{j71}\)</span>). First consider a replacement of 4 by 1. For point 2, the distances to 4 and 7 are 6 and 9, so point 2 is closest to center 4. As a result 2 will be allocated to either the new candidate center 1 or the current center 7. It is closest to the new center (3 relative to 9). The decrease in the objective from assigning 2 to 1 rather than 4 is 3 - 6 = -3, the entry in the
column <span class="math inline">\(C_{j41}\)</span>.</p>
<p>Now consider the contribution of 2 to the replacement of 7 by 1. Since 2 is closer to 4, we have the situation that a point is <em>not</em> closest to the center that is to be replaced. So, now we need to check whether it would stay with its current center (4) or move to the candidate. We already know that 2 is closer to 1 than to 4, so the gain from the swap is again -3, entered
under <span class="math inline">\(C_{j71}\)</span>. We proceed in the same way for each of the other non-center and non-candidate points and find the sum of
the contributions, listed in the row labeled <span class="math inline">\(T\)</span>. For a replacement of 4 by 1, the sum of -3, 1, 1, and 0 gives -1 as the
value of <span class="math inline">\(T_{41}\)</span>. Similarly, the value of <span class="math inline">\(T_{47}\)</span> is the sum of -3, 0, 0, and 1, or -2.</p>
<p>The remaining panels show the results when each of the other current non-centers is evaluated
as a center candidate. The minimum value over all pairs <span class="math inline">\(i,r\)</span> is obtained for <span class="math inline">\(T_{43} = -5\)</span>. This suggests that center 4 should
be replaced by point 3 (there is actually a tie with <span class="math inline">\(T_{73}\)</span>, so
in each case 3 should enter the center set; we take it to replace 4). The improvement in the overall objective function from this step
is -5.</p>
<div class="figure" style="text-align: center"><span id="fig:swap1"></span>
<img src="pics7c/3_step1.png" alt="PAM - swap step 1" width="50%" />
<p class="caption">
Figure 51: PAM - swap step 1
</p>
</div>
<p>This process is repeated in Figure <a href="#fig:swap2">52</a>, but now using <span class="math inline">\(d_{j3}\)</span> and <span class="math inline">\(d_{j7}\)</span> as reference
distances. The smallest value for <span class="math inline">\(T\)</span> is found for <span class="math inline">\(T_{75} = -2\)</span>, which is also the improvement
to the objective function (note that the improvement is smaller than for the first step, something
we would expect from a gradient descent method). This suggests that 7 should be replaced by 5.</p>
<div class="figure" style="text-align: center"><span id="fig:swap2"></span>
<img src="pics7c/3_step2.png" alt="PAM - swap step 2" width="50%" />
<p class="caption">
Figure 52: PAM - swap step 2
</p>
</div>
<p>In the next step, we repeat the calculations, using <span class="math inline">\(d_{j3}\)</span> and <span class="math inline">\(d_{j5}\)</span> as the distances. The smallest
value for <span class="math inline">\(T\)</span> is found for <span class="math inline">\(T_{32} = -1\)</span>, suggesting that 3 should be replaced by 2. The improvement in
the objective is -1 (again, smaller than in the previous step).</p>
<div class="figure" style="text-align: center"><span id="fig:swap3"></span>
<img src="pics7c/3_step3.png" alt="PAM - swap step 3" width="50%" />
<p class="caption">
Figure 53: PAM - swap step 3
</p>
</div>
<p>In the last step, we compute everything again for <span class="math inline">\(d_{j2}\)</span> and <span class="math inline">\(d_{j5}\)</span>. At this stage, none of the <span class="math inline">\(T\)</span> yield
a negative value, so the algorithm has reached a local optimum and stops.</p>
<div class="figure" style="text-align: center"><span id="fig:swap4"></span>
<img src="pics7c/3_step4.png" alt="PAM - swap step 4" width="50%" />
<p class="caption">
Figure 54: PAM - swap step 4
</p>
</div>
<p>The final result consists of a cluster of three elements, centered on 2, and a cluster of four elements,
centered on 5. As Figure <a href="#fig:swapfinal">55</a> shows, both clusters contribute 6 to the total sum of deviations,
for a final value of 12. This also turns out to be 20 - 5 - 2 - 1, or the total effect of each swap on the
objective function.</p>
<div class="figure" style="text-align: center"><span id="fig:swapfinal"></span>
<img src="pics7c/3_final.png" alt="PAM - final solution" width="25%" />
<p class="caption">
Figure 55: PAM - final solution
</p>
</div>
</div>
</div>
<div id="the-graph-laplacian" class="section level3 unnumbered" number="">
<h3>The graph Laplacian</h3>
<p>A useful property of the adjancency matrix <span class="math inline">\(W\)</span> associated with a graph is the <em>degree</em> of a vertex <span class="math inline">\(i\)</span>:
<span class="math display">\[d_i = \sum_j w_{ij},\]</span>
the row-sum of the similarity weights.
Previously, we used an identical concept to characterize the connectivity structure of a spatial weights, and referred to it as neighbor cardinality.</p>
<p>The <em>graph Laplacian</em> is the following matrix:
<span class="math display">\[L = D - W,\]</span>
where <span class="math inline">\(D\)</span> is a diagonal matrix containing the degree of each vertex. The graph Laplacian has the
property that all its eigenvalues are real and non-negative, and, most importantly, that its smallest
eigenvalue is zero.</p>
<p>In the (ideal) case where the adjacency matrix can be organized into separate partitions (unconnected to
each other), it takes on a block-diagonal form, with each block containing the partioning sub-matrix for
the matching group. The corresponding graph Laplacian will similarly have a block-diagonal structure.
Since each of these sub-blocks is itself a graph Laplacian (for the subnetwork corresponding to the
partition), its smallest eigenvalue is zero as well. An important result is then that if the graph
is partitioned into <span class="math inline">\(k\)</span> disconnected blocks, the graph Laplacian will have <span class="math inline">\(k\)</span> zero eigenvalues. This
forms the basis for the logic of using the <span class="math inline">\(k\)</span> smallest eigenvalues of <span class="math inline">\(L\)</span> to find the corresponding
clusters.</p>
<p>While it can be used to proceed with spectral clustering, the unnormalized Laplacian <span class="math inline">\(L\)</span> has some undesirable properties.
Instead, the preferred approach is to use a so-called <em>normalized</em> Laplacian. There are two ways to normalize the adjacency
matrix and thus the associated Laplacian. One is to <em>row-standardize</em> the adjacency matrix, or <span class="math inline">\(D^{-1}W\)</span>. This is exactly
the same idea as row-standardizing a spatial weights matrix. When applied to the Laplacian, this yields:
<span class="math display">\[L_{rw} = D^{-1}L = D^{-1}D - D^{-1}W = I - D^{-1}W.\]</span>
This is referred to as a <em>random walk</em> normalized graph Laplacian, since the row elements can be viewed
as transition probabilities from state <span class="math inline">\(i\)</span> to each of the other states <span class="math inline">\(j\)</span>. As we saw with spatial weights, the
resulting normalized matrix is no longer symmetric, although its eigenvalues remain real, with the smallest eigenvalue
being zero. The associated eigenvector is <span class="math inline">\(\iota\)</span>, a vector of ones.</p>
<p>A second transformation pre- and post-multiplies the Laplacian by <span class="math inline">\(D^{-1/2}\)</span>, the inverse of the square root of the degree.
This yields a <em>symmetric</em> normalized Laplacian as:
<span class="math display">\[L_{sym} = D^{-1/2}LD^{-1/2} = D^{-1/2}DD^{-1/2} - D^{-1/2}WD^{-1/2} = I - D^{-1/2}WD^{-1/2}.\]</span>
Again, the smallest eigenvalue is zero, but the associated eigenvector is <span class="math inline">\(D^{1/2}\iota\)</span>.</p>
<p>Spectral clustering algorithms differ by whether the unnormalized or normalized Laplacian is used to compute
the <span class="math inline">\(k\)</span> smallest eigenvalues and associated eigenvectors and whether the Laplacian or the adjacency matrix
is the basis for the calculation.</p>
<p>Specifically, as an alternative to using the smallest eigenvalues and associated eigenvectors of the normalized Laplacian,
the <em>largest</em> eigenvalues/eigenvectors of the normalized adjacency (or affinity) matrix can be computed. The standard eigenvalue
expression is the following equality:
<span class="math display">\[Lu = \lambda u,\]</span>
where <span class="math inline">\(u\)</span> is the eigenvector associated with eigenvalue <span class="math inline">\(\lambda\)</span>. If we subtract both sides from <span class="math inline">\(Iu\)</span>, we get:
<span class="math display">\[(I - L)u = (1 - \lambda)u.\]</span>
In other words, if the smallest eigenvalue of <span class="math inline">\(L\)</span> is 0, then the largest eigenvalue of <span class="math inline">\(I - L\)</span> is 1. Moreover:
<span class="math display">\[I - L = I - [I - D^{-1/2}WD^{-1/2}] = D^{-1/2}WD^{-1/2},\]</span>
so that the search for the smallest eigenvalue of <span class="math inline">\(L\)</span> is equivalent to the search for the <em>largest</em> eigenvalue
of <span class="math inline">\(D^{-1/2}WD^{-1/2}\)</span>, the normalized adjacency matrix.</p>
<p><br></p>
</div>
</div>
<div id="references" class="section level2 unnumbered" number="">
<h2>References</h2>
<div id="refs" class="references hanging-indent">
<div id="ref-Anselin:19b">
<p>Anselin, Luc. 2019. “Quantile Local Spatial Autocorrelation.” <em>Letters in Spatial and Resource Sciences</em> 12 (2): 155–66.</p>
</div>
<div id="ref-Hanetal:12">
<p>Han, Jiawei, Micheline Kamber, and Jian Pei. 2012. <em>Data Mining (Third Edition)</em>. Amsterdam: MorganKaufman.</p>
</div>
<div id="ref-Hastieetal:09">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning (2nd Edition)</em>. New York, NY: Springer.</p>
</div>
<div id="ref-deHoonetal:17">
<p>Hoon, Michiel de, Seiya Imoto, and Satoru Miyano. 2017. “The C Clustering Library.” Tokyo, Japan: The University of Tokyo, Institute of Medical Science, Human Genome Center.</p>
</div>
<div id="ref-KaufmanRousseeuw:05">
<p>Kaufman, L., and P. Rousseeuw. 2005. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. New York, NY: John Wiley.</p>
</div>
<div id="ref-Ngetal:02">
<p>Ng, Andrew Y., Michael I. Jordan, and Yair Weiss. 2002. “On Spectral Clustering: Analysis and an Algorithm.” In <em>Advances in Neural Information Processing Systems 14</em>, edited by T. G. Dietterich, S. Becker, and Z. Ghahramani, 849–56. Cambridge, MA: MIT Press.</p>
</div>
<div id="ref-NgHan:02">
<p>Ng, Raymond R., and Jiawei Han. 2002. “CLARANS: A Method for Clustering Objects for Spatial Data Mining.” <em>IEEE Transactions on Knowledge and Data Engineering</em> 14: 1003–16.</p>
</div>
<div id="ref-SchubertRousseeuw:19">
<p>Schubert, Erich, and Peter J. Rousseeuw. 2019. “Faster k-Medoids Clustering: Improving the PAM, CLARA, and CLARANS Algorithms.” In <em>Similarity Search and Applications, SISAP 2019</em>, edited by Giuseppe Amato, Claudio Gennaro, Vincent Oria, and Miloš Radovanović, 171–87. Cham, Switzerland: Springer Nature.</p>
</div>
<div id="ref-ShiMalik:00">
<p>Shi, Jianbo, and Jitendra Malik. 2000. “Normalized Cuts and Image Segmentation.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 22 (8): 888–905.</p>
</div>
<div id="ref-vonLuxburg:07">
<p>von Luxburg, Ulrike. 2007. “A Tutorial on Spectral Clustering.” <em>Statistical Computing</em> 27 (4): 395–416.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>University of Chicago, Center for Spatial Data Science – <a href="mailto:anselin@uchicago.edu" class="email">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Strictly speaking, k-medoids
minimizes the average distance to the representative center, but the sum
is easier for computational reasons.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>As <span class="citation">Kaufman and Rousseeuw (<a href="#ref-KaufmanRousseeuw:05" role="doc-biblioref">2005</a>)</span> show in Chapter 2, the problem is identical
to some facility location problems for which integer programming branch and bound solutions have
been suggested. But these tend to be limited so smaller sized problems.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Clearly, with a sample
size of 100%, CLARA becomes the same as PAM.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>More precisely, the first sample consists of 40 + 2k
random points. From the second sample on, the best k medoids found in a previous iteration are included, so that
there are 40 + k additional random points. Also, in <span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span>, they suggested to use 80 + 2k
and 10 repetitions for larger data sets. In the implementation in <code>GeoDa</code>, the latter is used for data
sets larger than 100.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p><span class="citation">Schubert and Rousseeuw (<a href="#ref-SchubertRousseeuw:19" role="doc-biblioref">2019</a>)</span> also consider 2.5% in larger data sets with 4 iterations instead of 2.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>The Java
code is contained in the open source <code>ELKI</code> software, available from <a href="https://elki-project.github.io" class="uri">https://elki-project.github.io</a>.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>This is the approach
used to obtain the selections in Figure <a href="#fig:kmedoidcenters">16</a>.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>As mentioned, for sample sizes &gt; 100, <code>GeoDa</code> uses a sample
size of 80 + 2k.<a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>The data set is available as one of the GeoDa Center sample data. Details about
the computation of the principal components and other aspects of the data are given in <span class="citation">Anselin (<a href="#ref-Anselin:19b" role="doc-biblioref">2019</a>)</span>.<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>For details, see <span class="citation">Shi and Malik (<a href="#ref-ShiMalik:00" role="doc-biblioref">2000</a>)</span>, as well as the discussion of a “graph cut point of view” in <span class="citation">von Luxburg (<a href="#ref-vonLuxburg:07" role="doc-biblioref">2007</a>)</span>.<a href="#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p><code>GeoDa</code> uses
the <a href="https://spectralib.org">Spectra</a> C++ library for large scale eigenvalue problems. Note that the particular
routines implemented in this library extract the <em>largest</em> eigenvalues/eigenvectors from the symmetric normalized
affinity matrix, not the smallest from the graph Laplacian. The latter is the textbook explanation, but not always
the most efficient implementation in practice. The equivalence between the two approaches is detailed in the
<a href="#appendix">Appendix</a>.<a href="#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Alternative normalizations are used as well.
For example, in the implementation in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html">scikit-learn</a>,
the eigenvectors are rescaled by the inverse square root of the eigenvalues.<a href="#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>The values for the number of nearest
neighbors are <em>rounded up</em> to the nearest integer.<a href="#fnref14" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<footer class="site-footer">
  <span class="site-footer-owner"><a href="https://github.com/lixun910/geoda">GeoDa</a> is maintained by <a href="#">lixun910</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
</footer>

</section>


<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
