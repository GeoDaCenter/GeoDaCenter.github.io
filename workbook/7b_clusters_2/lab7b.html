<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

  <meta charset="utf-8" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="pandoc" />

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="author" content="Luc Anselin" />


  <title>Cluster Analysis (2)</title>

  <link href="lab7b_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
  <script src="lab7b_files/highlightjs-9.12.0/highlight.js"></script>
  <title>GeoDa on Github</title>

  <style>
    * {
      margin: 0;
      padding: 0;
    }

    .shadowfilter {
      -webkit-filter: drop-shadow(12px 12px 7px rgba(0, 0, 0, 0.5));
      filter: url(shadow.svg#drop-shadow);
    }

    .intro1 {
      margin-left: -45px;
    }
  </style>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="https://geodacenter.github.io/stylesheets/github-light.css"
    media="screen">
  <link rel="stylesheet" href="https://geodacenter.github.io/stylesheets/simple-slideshow-styles.css">
  <style>
    ul {
      padding-left: 30px;
    }

    figcaption {
      top: .70em;
      left: .35em;
      bottom: auto !important;
      right: auto !important;
    }
  </style>

  <style>
    h1 {
      text-align: center;
    }

    h3.subtitle {
      text-align: center;
    }

    h4.author {
      text-align: center;
    }

    h4.date {
      text-align: center;
    }

    p.caption {
      font-size: 12px;
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LC0QJ53WFS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-LC0QJ53WFS');
  </script>
  <!-- End Google tag -->

  <style type="text/css">
    code {
      white-space: pre;
    }
  </style>
  <script type="text/javascript">
    if (window.hljs) {
      hljs.configure({ languages: [] });
      hljs.initHighlightingOnLoad();
      if (document.readyState && document.readyState === "complete") {
        window.setTimeout(function () { hljs.initHighlighting(); }, 0);
      }
    }
  </script>





</head>

<body>


  <section class="page-header">
    <h1 class="project-name">GeoDa</h1>
    <h2 class="project-tagline">An Introduction to Spatial Data Science</h2>
    <a href="https://geodacenter.github.io/index.html" class="btn">Homepage</a>
    <a href="https://geodacenter.github.io/download.html" class="btn">Download</a>
    <a href="https://github.com/GeoDaCenter/geoda/" class="btn">View on GitHub</a>
    <a href="https://spatial.uchicago.edu/sample-data" target="_blank" class="btn">Data</a>
    <a href="https://geodacenter.github.io/documentation.html" class="btn">Documentation</a>
    <a href="https://geodacenter.github.io/support.html" class="btn">Support</a>
    <a href="https://geodacenter.github.io/index-cn.html" class="btn">中文</a>
  </section>

  <section class="main-content">


    <h1 class="title toc-ignore">Cluster Analysis (2)</h1>
    <h3 class="subtitle"><em>Classic Clustering Methods</em></h3>
    <h4 class="author"><em>Luc Anselin<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></em></h4>
    <h4 class="date"><em>Latest update 11/19/2018</em></h4>


    <div id="TOC">
      <ul>
        <li><a href="#introduction">Introduction</a>
          <ul>
            <li><a href="#objectives">Objectives</a>
              <ul>
                <li><a href="#geoda-functions-covered">GeoDa functions covered</a></li>
              </ul>
            </li>
            <li><a href="#getting-started">Getting started</a></li>
          </ul>
        </li>
        <li><a href="#k-means">K Means</a>
          <ul>
            <li><a href="#principle">Principle</a></li>
            <li><a href="#implementation">Implementation</a>
              <ul>
                <li><a href="#variable-settings-panel">Variable Settings Panel</a></li>
                <li><a href="#cluster-results">Cluster results</a></li>
                <li><a href="#saving-the-cluster-results">Saving the cluster results</a></li>
              </ul>
            </li>
            <li><a href="#options-and-sensitivity-analysis">Options and sensitivity analysis</a>
              <ul>
                <li><a href="#changing-the-number-of-initial-runs-in-k-means">Changing the number of initial runs in
                    k-means++</a></li>
                <li><a href="#random-initialization">Random initialization</a></li>
                <li><a href="#setting-a-minimum-bound">Setting a minimum bound</a></li>
                <li><a href="#aggregation-by-cluster">Aggregation by cluster</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#hierarchical-clustering">Hierarchical Clustering</a>
          <ul>
            <li><a href="#principle-1">Principle</a></li>
            <li><a href="#implementation-1">Implementation</a>
              <ul>
                <li><a href="#variable-settings-panel-1">Variable Settings Panel</a></li>
              </ul>
            </li>
            <li><a href="#cluster-results-1">Cluster results</a>
              <ul>
                <li><a href="#dendrogram">Dendrogram</a></li>
                <li><a href="#cluster-map">Cluster map</a></li>
                <li><a href="#cluster-summary">Cluster summary</a></li>
              </ul>
            </li>
            <li><a href="#options-and-sensitivity-analysis-1">Options and sensitivity analysis</a>
              <ul>
                <li><a href="#single-linkage">Single linkage</a></li>
                <li><a href="#complete-linkage">Complete linkage</a></li>
                <li><a href="#average-linkage">Average linkage</a></li>
                <li><a href="#distance-metric">Distance metric</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <p><br></p>
    <div id="introduction" class="section level2 unnumbered">
      <h2>Introduction</h2>
      <p>In this second of three chapters that deal with multivariate clustering methods, we will
        cover two classic clustering methods, i.e., <strong>k-means</strong>, and <strong>hierarchical
          clustering</strong>.</p>
      <p>The problem addressed
        by a clustering method is to group the <em>n</em> observations into <em>k</em> <strong>clusters</strong> such
        that
        the intra-cluster similarity is maximized (or, dissimilarity minimized), and the
        between-cluster similarity minimized (or, dissimilarity maximized). K-means is a so-called
        partitioning clustering method in which the data are <em>partitioned</em> into k groups, with
        k determined beforehand. In constrast,
        hierarchical clustering builds up the clusters from the bottom up (or top down) and can be considered for
        many values of k.</p>
      <p>The clustering methods
        are standard tools of so-called unsupervised learning and constitute a core element in
        any machine learning toolbox. An extensive technical discussion is beyond the scope
        of this document, but a thorough treatment can be found in <span class="citation">Hastie, Tibshirani, and
          Friedman (<a href="#ref-Hastieetal:09">2009</a>)</span> and
        <span class="citation">James et al. (<a href="#ref-Jamesetal:13">2013</a>)</span>, among others.
      </p>
      <p>GeoDa implements the cluster algorithms by leveraging the <em>C Clustering Library</em>
        <span class="citation">(Hoon, Imoto, and Miyano <a href="#ref-deHoonetal:17">2017</a>)</span>, augmented by the
        k-means++ algorithm from <span class="citation">Arthur and Vassilvitskii (<a
            href="#ref-ArthurVassilvitskii:07">2007</a>)</span>.
      </p>
      <p>To illustrate these techniques, we will continue to use the Guerry data set on
        moral statistics in 1830 France, which comes pre-installed with GeoDa.</p>
      <div id="objectives" class="section level3 unnumbered">
        <h3>Objectives</h3>
        <ul>
          <li>
            <p>Carry out k-means clustering</p>
          </li>
          <li>
            <p>Interpret the characteristics of a cluster analysis</p>
          </li>
          <li>
            <p>Carry out a sensitivity analysis to starting options</p>
          </li>
          <li>
            <p>Impose a bound on the clustering solutions</p>
          </li>
          <li>
            <p>Compute aggregate values for the new clusters</p>
          </li>
          <li>
            <p>Carry out hierarchical clustering</p>
          </li>
        </ul>
        <div id="geoda-functions-covered" class="section level4 unnumbered">
          <h4>GeoDa functions covered</h4>
          <ul>
            <li>Clusters &gt; K Means
              <ul>
                <li>select variables</li>
                <li>select k-means starting algorithms</li>
                <li>k-means characteristics</li>
                <li>mapping the clusters</li>
                <li>changing the cluster labels</li>
                <li>saving the cluster classification</li>
                <li>setting a minimum bound</li>
              </ul>
            </li>
            <li>Clusters &gt; Hierarchical
              <ul>
                <li>select distance criterion</li>
              </ul>
            </li>
            <li>Table &gt; Aggregate</li>
          </ul>
          <p><br></p>
        </div>
      </div>
      <div id="getting-started" class="section level3 unnumbered">
        <h3>Getting started</h3>
        <p>As before, with GeoDa launched and all previous projects closed, we again load the Guerry sample data set
          from the <strong>Connect to Data Source</strong> interface. We either load it from the sample data
          collection and then save the file in a working directory, or we use a previously saved version. The process
          should yield the familiar themeless base map, showing the 85 French departments,
          as in Figure <a href="#fig:guerrybase">1</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:guerrybase"></span>
          <img src="pics7b/0_547_themelessbasemap.png" alt="French departments themeless map" width="80%" />
          <p class="caption">
            Figure 1: French departments themeless map
          </p>
        </div>
      </div>
    </div>
    <div id="k-means" class="section level2 unnumbered">
      <h2>K Means</h2>
      <div id="principle" class="section level3 unnumbered">
        <h3>Principle</h3>
        <p>The k-means algorithm starts with an initial (random) partioning of the <strong>n</strong> data
          points into <strong>k</strong>
          groups, and then incrementally improves upon this until convergence. In a general sense,
          observations are assigned to the cluster centroid to which they are closest, using
          an Euclidean (squared difference) dissimilarity
          criterion.</p>
        <p>A key element in this method is the choice of the number of clusters, k. Typically,
          several values for k are considered, and the resulting clusters are then compared in terms of the
          objective function. Since the total variance equals the sum of the within-group variances
          and the total between-group variance, a common criterion is to assess the ratio of the total
          between-group variance to the total variance. A higher value for this ratio suggests a better
          separation of the clusters. However, since this ratio increases with k, the selection
          of a <em>best</em> k is not straightforward. In practice, one can plot the ratio against values for
          k and select a point where the additional improvement in the objective is no longer
          meaningful. Several ad hoc rules have been suggested, but none is totally satisfactory.</p>
        <p>The k-means algorithm does not guarantee a global optimum, but only a local one.
          In addition, it is sensitive to the starting
          point used to initiate the iterative procedure. In GeoDa, two different approaches are
          implemented. One uses a series of random initial assignments, creating several initial clusters and starting
          the iterative process with the best among these solutions. In order to ensure replicability, it
          is important to set a seed value for the random number generator. Also, to assess the
          sensitivity of the result to the starting point, different seeds should be tried (as well as a
          different number of initial solutions).</p>
        <p>A second approach uses a careful consideration of initial seeds, following the procedure
          outlined in <span class="citation">Arthur and Vassilvitskii (<a
              href="#ref-ArthurVassilvitskii:07">2007</a>)</span>, commonly referred to as <strong>k-means++</strong>.
          While generally being
          faster and
          resulting in a superior solution in small to medium sized data sets,
          this method does not scale well (as it requires
          k passes through the whole data set to find the initial values). Note that a choice of a large number of
          random initial allocations may
          yield a better outcome than the application of k-means++, at the expense of a
          somewhat longer execution time.</p>
      </div>
      <div id="implementation" class="section level3 unnumbered">
        <h3>Implementation</h3>
        <p>We invoke the k-means functionality from the <strong>Clusters</strong> toolbar icon,
          in Figure <a href="#fig:kmeansicon">2</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansicon"></span>
          <img src="pics7b/1_683_cluster_toolbaricon.png" alt="Clusters toolbar icon" width="15%" />
          <p class="caption">
            Figure 2: Clusters toolbar icon
          </p>
        </div>
        <p>We select <strong>K Means</strong> from the list of options. Alternatively, from the main menu,
          shown in Figure <a href="#fig:kmeansmenu">3</a>, we
          can select <strong>Clusters &gt; K Means</strong>.</p>
        <div class="figure" style="text-align: center"><span id="fig:kmeansmenu"></span>
          <img src="pics7b/1_714_kmeans_option.png" alt="K Means Option" width="15%" />
          <p class="caption">
            Figure 3: K Means Option
          </p>
        </div>
        <p>This brings up the <strong>K Means Clustering Settings</strong> dialog, shown
          in Figure <a href="#fig:kmeansvars">4</a>, the main interface through which variables
          are chosen, options selected, and summary results are provided.</p>
        <div id="variable-settings-panel" class="section level4 unnumbered">
          <h4>Variable Settings Panel</h4>
          <p>We select the variables and set the parameters for the K Means cluster analysis
            through the options in the left hand panel of the interface. We choose the same six
            variables as in the previous chapter: <strong>Crm_prs</strong>,
            <strong>Crm_prp</strong>, <strong>Litercy</strong>, <strong>Donatns</strong>, <strong>Infants</strong>, and
            <strong>Suicids</strong>.
            These variables
            appear highlighted in the <strong>Select Variables</strong> panel.
          </p>
          <div class="figure" style="text-align: center"><span id="fig:kmeansvars"></span>
            <img src="pics7b/2_715_kmeans_variables.png" alt="K Means variable selection" width="80%" />
            <p class="caption">
              Figure 4: K Means variable selection
            </p>
          </div>
          <p>The next option is to select the <strong>Number of Clusters</strong>. The initial setting is blank. One can
            either choose a value from
            the drop-down list, or enter an integer value directly.<a href="#fn2" class="footnote-ref"
              id="fnref2"><sup>2</sup></a> In our example, we set the number of clusters to <strong>5</strong>.</p>
          <p>A default option is to use the variables in standardized form, i.e., in standard
            deviational units,<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> expressed with
            <strong>Transformation</strong> set to <strong>Standardize (Z)</strong>.</p>
          <p>The default algorithm is
            <strong>KMeans++</strong> with initialization re-runs set to 150 and maximal iterations to 1000.
            The <strong>seed</strong> is the global random number seed set for GeoDa, which can be changed by means of
            the
            <strong>Use specified seed</strong> option. Finally, the default <strong>Distance Function</strong> is
            <strong>Euclidean</strong>.
            For k-means, this option cannot be changed, but for hierarchical clustering, there is also
            a <strong>Manhattan</strong> distance metric.
          </p>
          <p>The cluster classification will be
            saved in the variable specified in the <strong>Save Cluster in Field</strong> box. The default of
            <strong>CL</strong>
            is likely not very useful if several options will be explored. In our first example, we
            set the variable name to <strong>CLa</strong>.</p>
        </div>
        <div id="cluster-results" class="section level4 unnumbered">
          <h4>Cluster results</h4>
          <p>After pressing <strong>Run</strong>, and keeping all the settings as above, a cluster map is created as a
            new view and the characteristics of
            the cluster are listed in the <strong>Summary</strong> panel.</p>
          <p>The cluster map in Figure <a href="#fig:kmeansmap5">5</a> reveals quite evenly balanced clusters, with 22,
            19, 18, 16 and 10 members
            respectively. Keep in mind that the clusters are based on attribute similarity and they do not respect
            contiguity or compactness (we will examine this aspect in a later chapter).</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeansmap5"></span>
            <img src="pics7b/2_717_clustermap_5.png" alt="K Means cluster map (k=5)" width="80%" />
            <p class="caption">
              Figure 5: K Means cluster map (k=5)
            </p>
          </div>
          <p>The cluster characteristics are listed in the <strong>Summary</strong> panel, shown
            in Figure <a href="#fig:kmeanssummary5">6</a>. This lists, for each cluster, the
            method (KMeans), the value of k (here, 5), as well as
            the parameters specified (i.e., the initialization methods, number of initialization
            re-runs, the maximum iterations, transformation, and distance function). Next
            follows the values of the cluster centers for each of the variables involved in the
            clustering algorithm (with the <strong>Standardize</strong> option on, these variables have been transformed
            to have zero mean and variance one overall, but not in each of the clusters).</p>
          <p>In addition, some summary measures are provided to assess the extent to which the clusters
            achieve within-cluster similarity and between-cluster dissimilarity. The total sum of
            squares is listed, as well as the within-cluster sum of squares for each of the clusters.
            Finally, these statistics are summarized as the total within-cluster sum of squares,
            the total between-cluster sum of squares, and the ratio of between-cluster to total
            sum of squares. In our initial example, the latter is 0.497467.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanssummary5"></span>
            <img src="pics7b/2_716_cluster_chars_5.png" alt="K Means cluster characteristics (k=5)" width="80%" />
            <p class="caption">
              Figure 6: K Means cluster characteristics (k=5)
            </p>
          </div>
          <p>The cluster labels (and colors in the map) are arbitrary and can be changed in the cluster
            map, using the same technique we saw earlier for unique value maps (in fact, the cluster
            maps are a special case of unique value maps). For example, if we wanted to switch
            category 4 with 3 and the corresponding colors, we would move the light green rectangle
            in the legend with label 4 up to the third spot in the legend, as shown in
            Figure <a href="#fig:changelabels">7</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:changelabels"></span>
            <img src="pics7b/2_722_change_labels1.png" alt="Change cluster labels" width="25%" />
            <p class="caption">
              Figure 7: Change cluster labels
            </p>
          </div>
          <p>Once we release the cursor, an updated cluster map is produced, with the categories (and colors)
            for 3 and 4 switched, as in Figure <a href="#fig:kmeanschangelabels">8</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanschangelabels"></span>
            <img src="pics7b/2_723_changed_labels.png" alt="Relabeled cluster map" width="80%" />
            <p class="caption">
              Figure 8: Relabeled cluster map
            </p>
          </div>
          <p>As the clusters are computed, a new categorical variable is added to the data table
            (the variable name is specified in the <strong>Save Cluster in Field</strong> option).
            It contains the assignment of each observation to one of the clusters as an
            integer variable. However, this
            is not automatically updated when we change the labels, as we did in the example above.</p>
          <p>In order to save the updated classification, we can still use the generic <strong>Save Categories</strong>
            option
            available in any map view (right click in the map).
            After specifying a variable name (e.g., <strong>cat_a</strong>), we can see both the original categorical
            variable and the new classification in the data table.</p>
          <p>In the table, shown in Figure <a href="#fig:kmeanscattable">9</a>, wherever <strong>CLa</strong> (the
            original
            classification) is <strong>3</strong>, the new classification (<strong>cat_a</strong>) is
            <strong>4</strong>.
            As always, the new variables do not become permanent additions until the table is saved.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanscattable"></span>
            <img src="pics7b/2_725_labels.png" alt="Cluster categories in table" width="25%" />
            <p class="caption">
              Figure 9: Cluster categories in table
            </p>
          </div>
        </div>
        <div id="saving-the-cluster-results" class="section level4 unnumbered">
          <h4>Saving the cluster results</h4>
          <p>The summary results listed in the <strong>Summary</strong> panel can be saved to a text file. Right
            clicking
            on the panel to brings up a dialog with a <strong>Save</strong> option,
            as in Figure <a href="#fig:kmeanssave">10</a>. Selecting this and specifying a file name
            for the results will provide a permanent record of the analysis.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeanssave"></span>
            <img src="pics7b/2_731_savesummary.png" alt="Saving summary results to a text file" width="60%" />
            <p class="caption">
              Figure 10: Saving summary results to a text file
            </p>
          </div>
        </div>
      </div>
      <div id="options-and-sensitivity-analysis" class="section level3 unnumbered">
        <h3>Options and sensitivity analysis</h3>
        <p>The k-means algorithm depends crucially on its initialization. In GeoDa, there are
          two methods to approach this. The default
          k-means++ algorithm usually picks a very good starting point. However, the number
          of initial runs may need to be increased to obtain the best solution.</p>
        <p>The second method is so-called random initialization. In this approach, k observations are
          randomly picked and used as seeds for an initial cluster assignment, for which the summary
          characteristics are then computed. This is repeated many times and the best solution is used
          as the starting point for the actual k-means algorithm.</p>
        <p>It is important to assess the sensitivity of the results to the starting point, and
          several combinations of settings should be compared.</p>
        <p>In addition to the starting options, it is also possible to <em>constrain</em> the k-means
          clustering by imposing a minimum value for a spatially extensive variable, such as a
          total population. This ensures that the clusters meet a minimum size for that variable.
          For example, we may want to create neighborhood types (clusters) based on a number
          of census variables, but we also want to make sure that each type has a minimum
          overall population size, to avoid creating clusters that are <em>too small</em> in terms
          of population. We will encounter this approach again when we discuss the max-p
          method for spatially constrained clustering.</p>
        <div id="changing-the-number-of-initial-runs-in-k-means" class="section level4 unnumbered">
          <h4>Changing the number of initial runs in k-means++</h4>
          <p>The default number of initial re-runs for the k-means++ algorithm is <strong>150</strong>. Sometimes,
            this is not sufficient to guarantee the best possible result (in terms of the ratio of between
            to total sum of squares). We can change this value in the <strong>Initialization Re-runs</strong> dialog, as
            illustrated in Figure <a href="#fig:kppinit">11</a> for a value of <strong>1000</strong> iterations.</p>
          <div class="figure" style="text-align: center"><span id="fig:kppinit"></span>
            <img src="pics7b/4_042_kppinitialize1000.png" alt="K-means++ initialization re-runs" width="40%" />
            <p class="caption">
              Figure 11: K-means++ initialization re-runs
            </p>
          </div>
          <p>The result is slightly different from what we obtained for the default setting.
            As shown in Figure <a href="#fig:kppinitmap">12</a>, the first category now has 23 elements, and
            the second 18. The other groupings remain the same.</p>
          <div class="figure" style="text-align: center"><span id="fig:kppinitmap"></span>
            <img src="pics7b/4_043_kpp1000map.png" alt="Cluster map for 1000 initial re-runs" width="80%" />
            <p class="caption">
              Figure 12: Cluster map for 1000 initial re-runs
            </p>
          </div>
          <p>The revised initialization results in a slight improvement of the sum of squares ratio,
            changing from 0.497467 to 0.497772, as shown in
            Figure <a href="#fig:kppinitresults">13</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:kppinitresults"></span>
            <img src="pics7b/4_044_kppinitresults.png" alt="Cluster characteristics for 1000 initial reruns"
              width="80%" />
            <p class="caption">
              Figure 13: Cluster characteristics for 1000 initial reruns
            </p>
          </div>
        </div>
        <div id="random-initialization" class="section level4 unnumbered">
          <h4>Random initialization</h4>
          <p>The alternative to the KMeans++ initialization is to select <strong>Random</strong> as the
            <strong>Initialization Method</strong>,
            as shown in Figure <a href="#fig:randominit">14</a>. We keep the number of initialization re-runs
            to the default value of 150 and save the result in the
            variable <strong>CLr</strong>.</p>
          <div class="figure" style="text-align: center"><span id="fig:randominit"></span>
            <img src="pics7b/3_732_randominit.png" alt="Random initialization" width="40%" />
            <p class="caption">
              Figure 14: Random initialization
            </p>
          </div>
          <p>The result is identical to what we obtained for K-means++ with 1000 initialization re-runs,
            with the cluster map as in Figure <a href="#fig:kppinitmap">12</a> and the cluster summary
            as in Figure <a href="#fig:kppinitresults">13</a>. However, if we had run the random initialization with
            much fewer runs (e.g., 10), the results would be inferior to what we obtained before.
            This highlights the effect of the starting values on the ultimate result.</p>
        </div>
        <div id="setting-a-minimum-bound" class="section level4 unnumbered">
          <h4>Setting a minimum bound</h4>
          <p>The minimum bound is set in the variable settings dialog by checking the
            box next to <strong>Minimum Bound</strong>, as in Figure <a href="#fig:minbound">15</a>. In our example, we
            select the variable <strong>Pop1831</strong> to set the constraint. Note that we specify a bound
            of <strong>15%</strong> (or <strong>4855</strong>) rather than the default <strong>10%</strong>. This is
            because the
            standard k-means solution satisfies the default constraint already, so that no actual bounding
            is carried out.</p>
          <div class="figure" style="text-align: center"><span id="fig:minbound"></span>
            <img src="pics7b/7_075_minbound.png" alt="Setting a minimum bound" width="40%" />
            <p class="caption">
              Figure 15: Setting a minimum bound
            </p>
          </div>
          <p>With the cluster size at 5 and all other options at their default value, we obtain
            the cluster result shown in the map in Figure <a href="#fig:minboundmap">16</a>. These results
            differ slightly from the unconstrained map in Figure <a href="#fig:kmeansmap5">5</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:minboundmap"></span>
            <img src="pics7b/7_076_minboundmap.png" alt="Cluster map with minimum bound constraint" width="80%" />
            <p class="caption">
              Figure 16: Cluster map with minimum bound constraint
            </p>
          </div>
          <p>The cluster characteristics show a slight deterioriation of our summary criterion,
            to a value of <strong>0.484033</strong> (compared to <strong>0.497772</strong>), as shown in
            Figure <a href="#fig:minboundresults">17</a>. This is the price to pay to
            satisfy the minimum population constraint.</p>
          <div class="figure" style="text-align: center"><span id="fig:minboundresults"></span>
            <img src="pics7b/7_077_minboundsummary.png" alt="Cluster characteristics with minimum bound" width="80%" />
            <p class="caption">
              Figure 17: Cluster characteristics with minimum bound
            </p>
          </div>
        </div>
        <div id="aggregation-by-cluster" class="section level4 unnumbered">
          <h4>Aggregation by cluster</h4>
          <p>With the clusters at hand, as defined for each observation by the category in the cluster
            field, we can now compute aggregate values for the new clusters. We illustrate this
            as a quick check on the population totals we imposed in the bounded cluster procedure.</p>
          <p>The aggregation is invoked from the <strong>Table</strong> as an option by right-clicking. This
            brings up the list of options, from which we select <strong>Aggregate</strong>, as in
            Figure <a href="#fig:tabagg">18</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:tabagg"></span>
            <img src="pics7b/7_002_table_aggregate.png" alt="Table Aggregate option" width="20%" />
            <p class="caption">
              Figure 18: Table Aggregate option
            </p>
          </div>
          <p>The following dialog, shown in Figure <a href="#fig:tabaggpop">19</a>, provides the specific
            aggregation method, i.e., count, average, max, min, or <strong>Sum</strong>, the <strong>key</strong> on
            which
            to aggregate and a selection of variable to aggregate. In our example, we use the
            cluster field key, e.g., <strong>CLb</strong> and select only the population variable
            <strong>Pop1831</strong>,
            which we will sum over the departments that make up each cluster. This can be readily
            extended to multiple variables, as well as to different summary measures, such
            as the average.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
          <div class="figure" style="text-align: center"><span id="fig:tabaggpop"></span>
            <img src="pics7b/7_079_aggregate_vars.png" alt="Aggregation of total population by cluster" width="40%" />
            <p class="caption">
              Figure 19: Aggregation of total population by cluster
            </p>
          </div>
          <p>Pressing the <strong>Aggregate</strong> key brings up a dialog to select the file in which the new
            results will be saved. For example, we can select a <strong>dbf</strong> format and specify the
            file name. The contents of the new file are given in Figure <a href="#fig:clusterpop">20</a>,
            with the total population for each cluster. Clearly, each cluster meets the minimum
            requirement of 4855 that was specified.</p>
          <div class="figure" style="text-align: center"><span id="fig:clusterpop"></span>
            <img src="pics7b/7_084_minbound_agg.png" alt="Total population by cluster" width="40%" />
            <p class="caption">
              Figure 20: Total population by cluster
            </p>
          </div>
          <p>The same procedure can be used to create new values for any variable, aggregated
            to the new cluster scale.</p>
        </div>
      </div>
    </div>
    <div id="hierarchical-clustering" class="section level2 unnumbered">
      <h2>Hierarchical Clustering</h2>
      <div id="principle-1" class="section level3 unnumbered">
        <h3>Principle</h3>
        <p>In contrast to a partioning method (like k-means), a hierarchical clustering approach
          builds the clusters step by step. This can be approached in a top-down fashion or in
          a bottom-up fashion.</p>
        <p>In a top-down approach, we start with the full data set as one cluster,
          and find the best break point to create two clusters. This process continues until each
          observation is its own cluster. The result of the successive divisions of the data is
          visualized in a so-called <em>dendrogram</em>, i.e., a representation as a tree.</p>
        <p>The bottom-up approach starts with each observation being assigned to its own cluster.
          Next, the two observations are found that are <em>closest</em> (using a given distance criterion),
          and they are combined into a cluster. This process repeats itself, by using a <em>representative
            point</em> for each grouping once multiple observations are combined. At the end of this process,
          there is a single cluster, containing all the observations. Again, the results of the
          sequential grouping of observations (and clusters) are visually represented by a dendrogram.</p>
        <p>The dendrogram is used to set a <em>cut</em> level for a specified k. By placing the cut point
          at different levels in the tree, clusters with varying dimensions are obtained.</p>
        <p>A key aspect of the hierarchical clustering process is how to compute the <em>distance</em> between
          two existing clusters in order to decide how to group the <em>closest</em> two together. There are
          several criteria in use, such as single linkage, complete linkage, average linkage, and
          Ward’s method (or centroid linkage).</p>
        <p>With <em>single linkage</em>, the distance between two clusters is defined by the distance between
          the <em>closest</em> (in attribute space) observations from each cluster. In contrast, for <em>complete
            linkage</em>, the distance
          is between the observations that are furthest apart. <em>Average linkage</em> uses the average of all the
          pairwise distances, whereas <em>Ward’s method</em> utilizes the distance between a central point in
          each cluster.</p>
        <p>A common default is to use Ward’s method, which tend to result in nicely balanced clusters.
          The complete linkage method yields similar clusters. In contrast, single linkage and average
          linkage tends to result in many singletons and a few very large clusters.</p>
      </div>
      <div id="implementation-1" class="section level3 unnumbered">
        <h3>Implementation</h3>
        <p>Hierarchical clustering is invoked in GeoDa from the same toolbar icon as k-means, shown
          in Figure <a href="#fig:kmeansicon">2</a>, by selecting the proper item from the drop down list. The desired
          clustering functionality can also be selected by using <strong>Clusters &gt; Hierarchical</strong> from the
          menu, as shown in Figure <a href="#fig:hierarchicalmenu">21</a>.</p>
        <div class="figure" style="text-align: center"><span id="fig:hierarchicalmenu"></span>
          <img src="pics7b/4_035_hierarchical.png" alt="Hierarchical clustering option" width="15%" />
          <p class="caption">
            Figure 21: Hierarchical clustering option
          </p>
        </div>
        <div id="variable-settings-panel-1" class="section level4 unnumbered">
          <h4>Variable Settings Panel</h4>
          <p>As before, the variables to be clustered are selected in the <strong>Variables Settings</strong> panel. We
            continue with the same six variables, shown in Figure <a href="#fig:hiervars">22</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:hiervars"></span>
            <img src="pics7b/5_048_hiervars.png" alt="Hierarchical clustering variable selection" width="40%" />
            <p class="caption">
              Figure 22: Hierarchical clustering variable selection
            </p>
          </div>
          <p>The panel also allows one to set several options, such as the <strong>Transformation</strong> (default
            value
            is a standardized z-value), the linkage <strong>Method</strong> (default is Ward’s method), and the
            <strong>Distance Function</strong> (default is Euclidean). In the same way as for k-means, the cluster
            classification is saved in the data table under the variable name specified in
            <strong>Save Cluster in Field</strong>.
          </p>
        </div>
      </div>
      <div id="cluster-results-1" class="section level3 unnumbered">
        <h3>Cluster results</h3>
        <p>The actual computation of the clusters proceeds in two steps. In the first step,
          after clicking on <strong>Run</strong>, a dendrogram is presented. The default cut point is set to 5,
          but this can be changed interactively (see below). Once a cut point is selected, clicking on
          <strong>Save/Show Map</strong> creates the cluster map, computes the summary characteristics, and saves the
          cluster classification in the data table.
        </p>
        <div id="dendrogram" class="section level4 unnumbered">
          <h4>Dendrogram</h4>
          <p>With all options set to the default, the resulting dendrogram is as in Figure <a
              href="#fig:dendrogram5">23</a>.
            The dashed red line corresponds to a cut point that yields five clusters (the default). The
            dendrogram shows how individual observations are combined into groups of two, and subsequently
            into larger and larger groups, by combining pairs of clusters. The colors on the right hand
            side match the colors of the observations in the cluster map (see next).</p>
          <div class="figure" style="text-align: center"><span id="fig:dendrogram5"></span>
            <img src="pics7b/5_049_dendrogram5.png" alt="Dendrogram (k=5)" width="60%" />
            <p class="caption">
              Figure 23: Dendrogram (k=5)
            </p>
          </div>
          <p>The dashed line (cut point) can be moved interactively. For example, in Figure <a
              href="#fig:dendrogram8">24</a>,
            we <em>grabbed</em> the line at the top (it can equally be grabbed at the bottom), and moved
            it to the right to yield eight clusters. The corresponding colors are shown on the
            right hand bar.</p>
          <div class="figure" style="text-align: center"><span id="fig:dendrogram8"></span>
            <img src="pics7b/5_050_dendrogram8.png" alt="Dendrogram (k=8)" width="60%" />
            <p class="caption">
              Figure 24: Dendrogram (k=8)
            </p>
          </div>
        </div>
        <div id="cluster-map" class="section level4 unnumbered">
          <h4>Cluster map</h4>
          <p>As mentioned before, once the dendrogram cut point is specified, clicking on <strong>Save/Show Map</strong>
            will generate the cluster map, shown in Figure <a href="#fig:hiermapW5">25</a>. Note how the colors for
            the map categories match the colors in the dendrogram. Also, the number of observations
            in each class also are the same between the groupings in the dendrogram and the cluster map.</p>
          <div class="figure" style="text-align: center"><span id="fig:hiermapW5"></span>
            <img src="pics7b/5_051_hierclusmapW5.png" alt="Hierarchical cluster map (Ward, k=5)" width="80%" />
            <p class="caption">
              Figure 25: Hierarchical cluster map (Ward, k=5)
            </p>
          </div>
        </div>
        <div id="cluster-summary" class="section level4 unnumbered">
          <h4>Cluster summary</h4>
          <p>Similarly, once <strong>Save/Show Map</strong> has been selected, the cluster descriptive statistics
            become available from the <strong>Summary</strong> button in the dialog. The same characteristics are
            reported
            as for k-means. In comparison to our k-means solution, this set of clusters is slightly
            inferior in terms of the ratio of between to total sum of squares, achieving 0.482044. However,
            setting the number of clusters at five is by no means necessarily the best solution. In a
            real application, one would experiment with different cut points and evaluate the solutions
            relative to the k-means solution.</p>
          <div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
            <img src="pics7b/5_052_hierW5summary.png" alt="Hierarchical cluster characteristics (Ward, k=5"
              width="80%" />
            <p class="caption">
              Figure 26: Hierarchical cluster characteristics (Ward, k=5
            </p>
          </div>
          <p>The two k-means and hierarchical clustering approaches can also be used in conjunction with each other. For
            example, one could
            explore the dendrogram to find a good cut-point, and then use this value for k in a k-means
            or other partitioning method.</p>
        </div>
      </div>
      <div id="options-and-sensitivity-analysis-1" class="section level3 unnumbered">
        <h3>Options and sensitivity analysis</h3>
        <p>The main option of interest in hierarchical clustering is the linkage <strong>Method</strong>. In
          addition, we can alter the <strong>Distance Function</strong> and the <strong>Transformation</strong>. The
          latter operates in the
          same fashion as for k-means.</p>
        <p>So far, we have used the default setting for <strong>Ward’s-linkage</strong>. We now consider each of the
          other linkage options in turn and illustrate the associated dendrogram, cluster map and cluster
          characteristics.</p>
        <div id="single-linkage" class="section level4 unnumbered">
          <h4>Single linkage</h4>
          <p>The linkage options are chosen from the <strong>Method</strong> item in the dialog. For example,
            in Figure <a href="#fig:singlelinkagemethod">27</a>, we select <strong>Single-linkage</strong>. The other
            options are chosen in the same way.</p>
          <div class="figure" style="text-align: center"><span id="fig:singlelinkagemethod"></span>
            <img src="pics7b/5_053_single_linkage.png" alt="Single linkage" width="40%" />
            <p class="caption">
              Figure 27: Single linkage
            </p>
          </div>
          <p>The cluster results for single linkage are typically characterized by one or a few very
            large clusters and several singletons (one observation per cluster). In our example,
            this is illustrated by the dendrogram in Figure <a href="#fig:dendrosingle5">28</a>, and the
            corresponding cluster map in Figure <a href="#fig:single5map">29</a>. Four <em>clusters</em> consist of a
            single observation, with the main cluster collecting the 81 other observations. This
            situation is not remedied by moving the cut point such that more clusters result, since
            almost all of the additional clusters are singletons as well.</p>
          <div class="figure" style="text-align: center"><span id="fig:dendrosingle5"></span>
            <img src="pics7b/5_054_dendrogram_single.png" alt="Dendrogram single linkage (k=5)" width="60%" />
            <p class="caption">
              Figure 28: Dendrogram single linkage (k=5)
            </p>
          </div>
          <div class="figure" style="text-align: center"><span id="fig:single5map"></span>
            <img src="pics7b/5_055_single_map.png" alt="Hierarchical cluster map (single linkage, k=5)" width="80%" />
            <p class="caption">
              Figure 29: Hierarchical cluster map (single linkage, k=5)
            </p>
          </div>
          <p>The characteristics of the single linkage hierarchical cluster are similarly dismal. Since
            four <em>clusters</em> are singeltons, their within cluster sum of squares is <strong>0</strong>. Hence,
            the total within-cluster sum of squares equals the sum of squares for cluster 5.
            The resulting ratio of between to total sum of squares is only 0.214771.</p>
          <div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
            <img src="pics7b/5_056_single_summary5.png" alt="Hierarchical cluster characteristics (single linkage, k=5)"
              width="80%" />
            <p class="caption">
              Figure 30: Hierarchical cluster characteristics (single linkage, k=5)
            </p>
          </div>
          <p>In practice, in most situations, single linkage will not be a good choice, unless the
            objective is to identify a lot of singletons.</p>
        </div>
        <div id="complete-linkage" class="section level4 unnumbered">
          <h4>Complete linkage</h4>
          <p>The complete linkage method yields clusters that are similar in balance to Ward’s method.
            For example, in Figure <a href="#fig:dendrocomplete5">31</a>, the dendrogram is shown for our example,
            using a cut point with five clusters. The corresponding cluster map is given as
            Figure <a href="#fig:completemap">32</a>. The map is similar in structure to that obtained with Ward’s
            method (Figure <a href="#fig:hiermapW5">25</a>), but note that the largest category (at 39) is much larger
            than the largest for Ward (25).</p>
          <div class="figure" style="text-align: center"><span id="fig:dendrocomplete5"></span>
            <img src="pics7b/5_057_dendro_complete5.png" alt="Dendrogram complete linkage (k=5)" width="60%" />
            <p class="caption">
              Figure 31: Dendrogram complete linkage (k=5)
            </p>
          </div>
          <div class="figure" style="text-align: center"><span id="fig:completemap"></span>
            <img src="pics7b/5_058_complete_map.png" alt="Hierarchical cluster map (complete linkage, k=5)"
              width="80%" />
            <p class="caption">
              Figure 32: Hierarchical cluster map (complete linkage, k=5)
            </p>
          </div>
          <p>In terms of the cluster characteristics, shown in Figure <a href="#fig:completesummary">33</a>, we note
            a slight deterioration relative to Ward’s results, with the ratio of between to total sum
            of squares at 0.423101 (but much better than single linkage).</p>
          <div class="figure" style="text-align: center"><span id="fig:completesummary"></span>
            <img src="pics7b/5_059_complete_summary.png"
              alt="Hierarchical cluster characteristics (complete linkage, k=5)" width="80%" />
            <p class="caption">
              Figure 33: Hierarchical cluster characteristics (complete linkage, k=5)
            </p>
          </div>
        </div>
        <div id="average-linkage" class="section level4 unnumbered">
          <h4>Average linkage</h4>
          <p>Finally, the average linkage criterion suffers from some of the same problems as single
            linkage, although it yields slightly better results. The dendrogram and cluster map
            are shown in Figures <a href="#fig:dendroavg5">34</a> and <a href="#fig:avgmap">35</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:dendroavg5"></span>
            <img src="pics7b/5_060_dendro_average.png" alt="Dendrogram average linkage (k=5)" width="60%" />
            <p class="caption">
              Figure 34: Dendrogram average linkage (k=5)
            </p>
          </div>
          <div class="figure" style="text-align: center"><span id="fig:avgmap"></span>
            <img src="pics7b/5_061_average_map.png" alt="Hierarchical cluster map (average linkage, k=5)" width="80%" />
            <p class="caption">
              Figure 35: Hierarchical cluster map (average linkage, k=5)
            </p>
          </div>
          <p>As given in Figure <a href="#fig:avgsummary">36</a>, the summary characteristics are slighly better
            than in the single linkage case, with only two singletons. However, the overall ratio of
            between to total sum of squares is still much worse than for the other two methods,
            at 0.296838.</p>
          <div class="figure" style="text-align: center"><span id="fig:avgsummary"></span>
            <img src="pics7b/5_062_average_summary.png"
              alt="Hierarchical cluster characteristics (average linkage, k=5)" width="80%" />
            <p class="caption">
              Figure 36: Hierarchical cluster characteristics (average linkage, k=5)
            </p>
          </div>
        </div>
        <div id="distance-metric" class="section level4 unnumbered">
          <h4>Distance metric</h4>
          <p>The default metric to gauge the distance between the center of each cluster is the
            Euclidean distance. In some contexts, it may be preferable to use absolute or Manhattan
            block distance, which penalizes larger distances less. This option can be selected
            through the <strong>Distance Function</strong> item in the dialog, as in Figure <a
              href="#fig:kmeansdist">37</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:kmeansdist"></span>
            <img src="pics7b/6_065_hier_manhattan.png" alt="Manhattan distance metric" width="40%" />
            <p class="caption">
              Figure 37: Manhattan distance metric
            </p>
          </div>
          <p>We run this for the same variables using Ward’s linkage and a cut point yielding 5 clusters.
            The corresponding cluster map is shown in Figure <a href="#fig:mandistmap">38</a>, with the summary
            characteristics given in Figure <a href="#fig:mandistsummary">39</a>.</p>
          <div class="figure" style="text-align: center"><span id="fig:mandistmap"></span>
            <img src="pics7b/6_066_h_man_map.png" alt="Manhattan distance cluster map" width="80%" />
            <p class="caption">
              Figure 38: Manhattan distance cluster map
            </p>
          </div>
          <div class="figure" style="text-align: center"><span id="fig:mandistsummary"></span>
            <img src="pics7b/6_067_h_man_summary.png" alt="Manhattan distance cluster characteristics" width="80%" />
            <p class="caption">
              Figure 39: Manhattan distance cluster characteristics
            </p>
          </div>
          <p>Relative to the Euclidean distance results, the ratio of between to total sum of squares
            is somewhat worse, at 0.44549 (compared to 0.482044). However, this is not a totally fair comparison, since
            the
            criterion for grouping is not based on a variance measure, but on an absolute difference.</p>
          <p><br></p>
        </div>
      </div>
    </div>
    <div id="references" class="section level2 unnumbered">
      <h2>References</h2>
      <div id="refs" class="references">
        <div id="ref-ArthurVassilvitskii:07">
          <p>Arthur, David, and Sergei Vassilvitskii. 2007. “k-means++: The Advantages of Careful Seeding.” In <em>SODA
              07, Proceedings of the Eighteenth Annual Acm-Siam Symposium on Discrete Algorithms</em>, edited by Harold
            Gabow, 1027–35. Philadelphia, PA: Society for Industrial and Applied Mathematics.</p>
        </div>
        <div id="ref-Hastieetal:09">
          <p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning (2nd
              Edition)</em>. New York, NY: Springer.</p>
        </div>
        <div id="ref-deHoonetal:17">
          <p>Hoon, Michiel de, Seiya Imoto, and Satoru Miyano. 2017. “The C Clustering Library.” Tokyo, Japan: The
            University of Tokyo, Institute of Medical Science, Human Genome Center.</p>
        </div>
        <div id="ref-Jamesetal:13">
          <p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to
              Statistical Learning, with Applications in R</em>. New York, NY: Springer-Verlag.</p>
        </div>
      </div>
    </div>
    <div class="footnotes">
      <hr />
      <ol>
        <li id="fn1">
          <p>University of Chicago, Center for Spatial Data Science – <a
              href="mailto:anselin@uchicago.edu">anselin@uchicago.edu</a><a href="#fnref1" class="footnote-back">↩</a>
          </p>
        </li>
        <li id="fn2">
          <p>The drop-down list goes from 2 to 85,
            which may be insufficient in <em>big data</em> settings. Hence GeoDa now offers the option to enter
            a value directly.<a href="#fnref2" class="footnote-back">↩</a></p>
        </li>
        <li id="fn3">
          <p>The Z standardization subtracts the mean and divides by the
            standard deviation. An alternative standardization is to use the mean absolute
            deviation, MAD.<a href="#fnref3" class="footnote-back">↩</a></p>
        </li>
        <li id="fn4">
          <p>In the current implementation, the same summary method needs to
            be applied to all the variables.<a href="#fnref4" class="footnote-back">↩</a></p>
        </li>
      </ol>
    </div>

    <footer class="site-footer">
      <span class="site-footer-owner"><a href="https://github.com/lixun910/geoda">GeoDa</a> is maintained by <a
          href="#">lixun910</a>.</span>
      <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>
        using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a
          href="https://twitter.com/jasonlong">Jason Long</a>.</span>
    </footer>

  </section>


  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>

</html>